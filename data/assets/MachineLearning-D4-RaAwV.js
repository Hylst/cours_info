import{c as J,r as R,j as e,C as t,a as n,b as l,e as o,d as f,B as $,L as W,R as T}from"./index-jatdNhFh.js";import{C as Z}from"./ContentLayout-Ck_M1CCv.js";import{G as r}from"./glossary-term-yY4LMtks.js";import{E as u,a as h,Q as M}from"./educational-cards-DKc7pKQa.js";import{B as d}from"./badge-D3N1h19b.js";import{T as V,a as B,b as _,c as g}from"./tabs-DyU7YeQV.js";import{T as A}from"./target-Cgxv7nHm.js";import{C as x}from"./circle-check-big-DcUkgBGn.js";import{C as S}from"./code-B5o97cHw.js";import{U as G}from"./users-DpOE6_sU.js";import{L as v}from"./lightbulb-D5sQnwCc.js";import{T as y}from"./tree-pine-BGklAe31.js";import{E as ee}from"./eye-CW1j5p4w.js";import{A as j}from"./arrow-right-BSp3FkSU.js";import{C as se}from"./clock-CgUQtsvd.js";import{B as D}from"./book-open-BGQhZmI6.js";import{Z as C}from"./zap-Df3zai_1.js";import{N as K}from"./network-xMSqk6VX.js";import{G as H}from"./git-branch-C09JVIQv.js";import{R as P,C as w,X as L,Y as k,T as F,L as I,B as re,a as ie}from"./generateCategoricalChart-Cpewz25D.js";import{B as ae}from"./BarChart-Bb4TIFVA.js";import{a as te,L as O}from"./LineChart-BJ-ld847.js";import{C as ne}from"./copy-DxZK1Zd6.js";import{D as le}from"./Layout-CR8zGZt_.js";import{E as q}from"./external-link-CTEWBqNU.js";import{U as oe}from"./unified-hero-section-_j3EuOk5.js";import{u as de}from"./use-section-tracker-DAwaxdR8.js";import{u as ce}from"./use-smooth-scroll-Cdf-j_aP.js";import{B as X,G as me}from"./graduation-cap-BzjsS-_o.js";import{C as ue}from"./chart-column-D8DiT-jd.js";import{C as pe}from"./cpu-Btr-CDLZ.js";import"./input-B_RYv3vb.js";import"./separator-1VTeyrJY.js";import"./chevron-left-HiyNMpmN.js";import"./index-DMzQkvGM.js";import"./info-CijxG-2I.js";import"./circle-alert-Bf4zJKID.js";import"./trending-up-gTw2Lzr0.js";import"./brain-e1e4SMeD.js";/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const E=J("Bot",[["path",{d:"M12 8V4H8",key:"hb8ula"}],["rect",{width:"16",height:"12",x:"4",y:"8",rx:"2",key:"enze0r"}],["path",{d:"M2 14h2",key:"vft8re"}],["path",{d:"M20 14h2",key:"4cs60a"}],["path",{d:"M15 13v2",key:"1xurst"}],["path",{d:"M9 13v2",key:"rq6x2g"}]]),Q=R.createContext(void 0),xe=({children:s})=>{const[a,b]=R.useState("introduction");return e.jsx(Q.Provider,{value:{activeSection:a,setActiveSection:b},children:s})},he=()=>{const s=R.useContext(Q);if(s===void 0)throw new Error("useMachineLearning must be used within a MachineLearningContextProvider");return s},i={"machine-learning":{term:"Machine Learning",shortDefinition:"Branche de l'IA permettant aux systÃ¨mes d'apprendre Ã  partir de donnÃ©es sans Ãªtre explicitement programmÃ©s.",longDefinition:"Le Machine Learning est un domaine de l'intelligence artificielle qui se concentre sur le dÃ©veloppement d'algorithmes permettant aux ordinateurs d'apprendre Ã  partir de donnÃ©es et d'amÃ©liorer leur performance au fil du temps sans Ãªtre explicitement programmÃ©s pour effectuer des tÃ¢ches spÃ©cifiques.",examples:["Un systÃ¨me qui apprend Ã  reconnaÃ®tre des visages aprÃ¨s avoir Ã©tÃ© entraÃ®nÃ© sur des milliers d'images","Un programme qui prÃ©dit les prix immobiliers en analysant les ventes historiques"],relatedTerms:["Intelligence Artificielle","Deep Learning","Apprentissage supervisÃ©"],source:"Mitchell, Tom M. (1997). Machine Learning",sourceUrl:"https://www.cs.cmu.edu/~tom/mlbook.html",domain:"general",level:"beginner",synonyms:["Apprentissage automatique","Apprentissage machine"],englishTerm:"Machine Learning"},"apprentissage-supervise":{term:"Apprentissage supervisÃ©",shortDefinition:"Approche oÃ¹ l'algorithme apprend Ã  partir de donnÃ©es Ã©tiquetÃ©es pour prÃ©dire des valeurs sur de nouvelles donnÃ©es.",longDefinition:"L'apprentissage supervisÃ© est une technique de machine learning oÃ¹ l'algorithme est entraÃ®nÃ© sur un ensemble de donnÃ©es Ã©tiquetÃ©es, ce qui signifie que chaque exemple d'entraÃ®nement est associÃ© Ã  une Ã©tiquette ou une valeur cible. L'objectif est d'apprendre une fonction de mappage qui peut Ãªtre utilisÃ©e pour prÃ©dire les valeurs des nouvelles entrÃ©es non vues.",examples:["Classification d'emails comme spam ou non spam","PrÃ©diction de prix immobiliers basÃ©e sur les caractÃ©ristiques de la propriÃ©tÃ©"],relatedTerms:["Classification","RÃ©gression","Machine Learning","Ã‰tiquetage de donnÃ©es"],source:"Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning",domain:"machinelearning",level:"beginner",synonyms:["Apprentissage dirigÃ©"],englishTerm:"Supervised Learning"},"apprentissage-non-supervise":{term:"Apprentissage non supervisÃ©",shortDefinition:"Technique qui traite des donnÃ©es non Ã©tiquetÃ©es pour dÃ©couvrir des structures et patterns cachÃ©s.",longDefinition:"L'apprentissage non supervisÃ© est une mÃ©thode de machine learning oÃ¹ l'algorithme est entraÃ®nÃ© sur des donnÃ©es non Ã©tiquetÃ©es. L'objectif est de dÃ©couvrir des structures intrinsÃ¨ques, des patterns et des relations dans les donnÃ©es sans guidance externe ou connaissance prÃ©alable des rÃ©sultats attendus.",examples:["Segmentation de clients en groupes similaires","DÃ©tection d'anomalies dans les transactions bancaires"],relatedTerms:["Clustering","RÃ©duction de dimensionnalitÃ©","K-means","DBSCAN"],source:"Ghahramani, Z. (2004). Unsupervised Learning",domain:"machinelearning",level:"intermediate",synonyms:["Apprentissage sans supervision","Apprentissage autonome"],englishTerm:"Unsupervised Learning"},classification:{term:"Classification",shortDefinition:"PrÃ©diction d'une catÃ©gorie ou classe pour une instance de donnÃ©es parmi un ensemble fini de possibilitÃ©s.",longDefinition:"La classification est une forme d'apprentissage supervisÃ© oÃ¹ l'algorithme apprend Ã  attribuer une instance de donnÃ©es Ã  une catÃ©gorie ou classe prÃ©dÃ©finie. Le modÃ¨le est entraÃ®nÃ© sur des exemples Ã©tiquetÃ©s et apprend Ã  gÃ©nÃ©raliser pour classer correctement de nouvelles instances.",examples:["DÃ©tection de fraude (frauduleux/lÃ©gitime)","Diagnostic mÃ©dical (malade/sain)","Reconnaissance d'objets dans des images"],relatedTerms:["Apprentissage supervisÃ©","RÃ©gression logistique","Arbres de dÃ©cision","SVM"],source:"Bishop, C. M. (2006). Pattern Recognition and Machine Learning",domain:"machinelearning",level:"beginner",englishTerm:"Classification"},regression:{term:"RÃ©gression",shortDefinition:"PrÃ©diction d'une valeur numÃ©rique continue basÃ©e sur des variables d'entrÃ©e.",longDefinition:"La rÃ©gression est une technique d'apprentissage supervisÃ© visant Ã  prÃ©dire une valeur numÃ©rique continue plutÃ´t qu'une classe discrÃ¨te. Elle modÃ©lise la relation entre une variable dÃ©pendante (cible) et une ou plusieurs variables indÃ©pendantes (prÃ©dicteurs).",examples:["PrÃ©diction des prix immobiliers","Estimation de la consommation d'Ã©nergie","PrÃ©vision des ventes"],relatedTerms:["Apprentissage supervisÃ©","RÃ©gression linÃ©aire","RÃ©gression polynomiale","MSE"],source:"James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning",domain:"machinelearning",level:"beginner",englishTerm:"Regression"},clustering:{term:"Clustering",shortDefinition:"Regroupement d'instances similaires en groupes (clusters) sans Ã©tiquettes prÃ©dÃ©finies.",longDefinition:"Le clustering est une technique d'apprentissage non supervisÃ© qui consiste Ã  regrouper des donnÃ©es en sous-ensembles (clusters) de maniÃ¨re Ã  ce que les objets au sein d'un mÃªme groupe soient plus similaires entre eux qu'avec ceux des autres groupes, selon certaines mesures de similaritÃ©.",examples:["Segmentation client pour le marketing ciblÃ©","Regroupement d'articles de presse par thÃ©matique","Identification de zones gÃ©ographiques avec des caractÃ©ristiques similaires"],relatedTerms:["Apprentissage non supervisÃ©","K-means","DBSCAN","Clustering hiÃ©rarchique"],source:"Jain, A. K. (2010). Data clustering: 50 years beyond K-means",domain:"machinelearning",level:"intermediate",synonyms:["Partitionnement de donnÃ©es","Segmentation"],englishTerm:"Clustering"},"reduction-dimensionnalite":{term:"RÃ©duction de dimensionnalitÃ©",shortDefinition:"Techniques rÃ©duisant le nombre de variables tout en prÃ©servant l'information essentielle.",longDefinition:"La rÃ©duction de dimensionnalitÃ© est un ensemble de techniques visant Ã  rÃ©duire le nombre de variables alÃ©atoires Ã  considÃ©rer dans un jeu de donnÃ©es, tout en prÃ©servant au maximum l'information pertinente. Ces mÃ©thodes permettent de lutter contre le flÃ©au de la dimensionnalitÃ© et facilitent la visualisation des donnÃ©es.",examples:["Visualisation de donnÃ©es complexes en 2D ou 3D","Compression d'images","PrÃ©traitement pour d'autres algorithmes de ML"],relatedTerms:["PCA","t-SNE","UMAP","Apprentissage non supervisÃ©"],source:"Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE",domain:"machinelearning",level:"intermediate",synonyms:["Projection de donnÃ©es","Compression de caractÃ©ristiques"],englishTerm:"Dimensionality Reduction"},overfitting:{term:"Surapprentissage",shortDefinition:"PhÃ©nomÃ¨ne oÃ¹ un modÃ¨le apprend trop prÃ©cisÃ©ment les donnÃ©es d'entraÃ®nement au dÃ©triment de sa capacitÃ© Ã  gÃ©nÃ©raliser.",longDefinition:"Le surapprentissage (overfitting) se produit lorsqu'un modÃ¨le capture le bruit et les fluctuations alÃ©atoires des donnÃ©es d'entraÃ®nement comme Ã©tant des caractÃ©ristiques significatives. Le modÃ¨le performe alors trÃ¨s bien sur les donnÃ©es d'entraÃ®nement mais Ã©choue Ã  gÃ©nÃ©raliser correctement sur de nouvelles donnÃ©es.",examples:["Un arbre de dÃ©cision qui crÃ©e des branches jusqu'Ã  classer parfaitement chaque exemple d'entraÃ®nement","Un rÃ©seau de neurones qui continue Ã  s'entraÃ®ner bien aprÃ¨s que l'erreur de validation commence Ã  augmenter"],relatedTerms:["Sous-apprentissage","RÃ©gularisation","Validation croisÃ©e","Biais-variance"],source:"Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning",domain:"machinelearning",level:"intermediate",synonyms:["Surajustement","HyperspÃ©cialisation"],englishTerm:"Overfitting"},underfitting:{term:"Sous-apprentissage",shortDefinition:"Situation oÃ¹ un modÃ¨le est trop simple pour capturer la structure sous-jacente des donnÃ©es.",longDefinition:"Le sous-apprentissage (underfitting) se produit lorsqu'un modÃ¨le est trop simpliste pour saisir les relations importantes dans les donnÃ©es. Il performe mal Ã  la fois sur les donnÃ©es d'entraÃ®nement et de test car il n'a pas suffisamment de capacitÃ© pour modÃ©liser la complexitÃ© inhÃ©rente au problÃ¨me.",examples:["Utiliser une rÃ©gression linÃ©aire pour modÃ©liser une relation hautement non linÃ©aire","Un rÃ©seau de neurones avec trop peu de couches pour un problÃ¨me complexe"],relatedTerms:["Surapprentissage","ComplexitÃ© du modÃ¨le","Biais Ã©levÃ©"],source:"Domingos, P. (2012). A few useful things to know about machine learning",domain:"machinelearning",level:"intermediate",synonyms:["Sous-ajustement","Sous-modÃ©lisation"],englishTerm:"Underfitting"},"deep-learning":{term:"Deep Learning",shortDefinition:"Sous-ensemble du ML utilisant des rÃ©seaux de neurones Ã  multiples couches pour apprendre des reprÃ©sentations hiÃ©rarchiques.",longDefinition:"Le Deep Learning est une forme avancÃ©e de machine learning utilisant des rÃ©seaux de neurones artificiels Ã  plusieurs couches (rÃ©seaux profonds) pour modÃ©liser des abstractions de haut niveau dans les donnÃ©es. Cette approche permet au systÃ¨me d'apprendre des reprÃ©sentations hiÃ©rarchiques des donnÃ©es, chaque couche transformant sa reprÃ©sentation d'entrÃ©e en une reprÃ©sentation plus abstraite.",examples:["Reconnaissance d'images avec des rÃ©seaux CNN","Traduction automatique avec des transformers","GÃ©nÃ©ration d'images avec des GANs"],relatedTerms:["RÃ©seaux de neurones","CNN","RNN","Transformers","Apprentissage par transfert"],source:"LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature",sourceUrl:"https://www.nature.com/articles/nature14539",domain:"machinelearning",level:"advanced",synonyms:["Apprentissage profond","RÃ©seaux de neurones profonds"],englishTerm:"Deep Learning"},cnn:{term:"RÃ©seaux de neurones convolutifs",shortDefinition:"Architecture de deep learning optimisÃ©e pour traiter des donnÃ©es avec une topologie en grille, comme les images.",longDefinition:"Les rÃ©seaux de neurones convolutifs (CNN) sont une classe spÃ©cialisÃ©e de rÃ©seaux de neurones conÃ§us pour traiter efficacement des donnÃ©es structurÃ©es en grille, comme les images. Ils utilisent l'opÃ©ration de convolution pour extraire automatiquement des caractÃ©ristiques hiÃ©rarchiques Ã  partir des donnÃ©es d'entrÃ©e, en apprenant des filtres qui dÃ©tectent des motifs locaux.",examples:["Classification d'images","DÃ©tection d'objets","Segmentation sÃ©mantique","Reconnaissance faciale"],relatedTerms:["Deep Learning","Convolution","Pooling","Feature maps"],source:"Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks",domain:"machinelearning",level:"advanced",synonyms:["ConvNets","RÃ©seaux convolutifs"],englishTerm:"Convolutional Neural Networks (CNN)"},rnn:{term:"RÃ©seaux de neurones rÃ©currents",shortDefinition:"Architecture de deep learning conÃ§ue pour traiter des donnÃ©es sÃ©quentielles en utilisant des connexions rÃ©currentes.",longDefinition:"Les rÃ©seaux de neurones rÃ©currents (RNN) sont spÃ©cialisÃ©s dans le traitement de donnÃ©es sÃ©quentielles comme le texte ou les sÃ©ries temporelles. Contrairement aux rÃ©seaux feedforward, les RNN possÃ¨dent des connexions rÃ©currentes qui leur permettent de maintenir un Ã©tat interne (mÃ©moire) et de traiter des sÃ©quences de longueur variable en partageant les paramÃ¨tres Ã  travers les Ã©tapes temporelles.",examples:["Analyse de sentiment dans des textes","Traduction automatique","PrÃ©diction de sÃ©ries temporelles","GÃ©nÃ©ration de texte"],relatedTerms:["LSTM","GRU","Deep Learning","SÃ©quences","ProblÃ¨me de gradient qui s'Ã©vanouit"],source:"Elman, J. L. (1990). Finding structure in time",domain:"machinelearning",level:"advanced",synonyms:["RÃ©seaux rÃ©currents"],englishTerm:"Recurrent Neural Networks (RNN)"},transformers:{term:"Transformers",shortDefinition:"Architecture de deep learning basÃ©e sur le mÃ©canisme d'attention, qui excelle dans le traitement du langage naturel.",longDefinition:"Les Transformers sont une architecture de rÃ©seaux de neurones introduite en 2017 qui a rÃ©volutionnÃ© le traitement du langage naturel. Contrairement aux RNN, les Transformers traitent l'ensemble de la sÃ©quence simultanÃ©ment grÃ¢ce au mÃ©canisme d'auto-attention, ce qui leur permet de modÃ©liser les dÃ©pendances Ã  longue distance plus efficacement et de se prÃªter Ã  la parallÃ©lisation pendant l'entraÃ®nement.",examples:["ModÃ¨les BERT pour la comprÃ©hension du langage","GPT pour la gÃ©nÃ©ration de texte","T5 pour diverses tÃ¢ches de NLP","ViT pour la vision par ordinateur"],relatedTerms:["Attention","Auto-attention","BERT","GPT","Encodeur-DÃ©codeur"],source:"Vaswani, A., et al. (2017). Attention Is All You Need",sourceUrl:"https://arxiv.org/abs/1706.03762",domain:"machinelearning",level:"advanced",englishTerm:"Transformers"},"evaluation-modele":{term:"Ã‰valuation de modÃ¨le",shortDefinition:"Processus de mesure des performances d'un modÃ¨le de machine learning pour estimer sa qualitÃ© et son utilitÃ©.",longDefinition:"L'Ã©valuation de modÃ¨le est le processus systÃ©matique d'estimation de la qualitÃ© et de l'utilitÃ© d'un modÃ¨le de machine learning. Elle implique de mesurer diverses mÃ©triques de performance sur des donnÃ©es non vues pendant l'entraÃ®nement, afin d'Ã©valuer la capacitÃ© du modÃ¨le Ã  gÃ©nÃ©raliser et Ã  faire des prÃ©dictions prÃ©cises dans des contextes rÃ©els.",examples:["Utilisation de mÃ©triques comme la prÃ©cision, le rappel et le F1-score pour les modÃ¨les de classification","Calcul de l'erreur quadratique moyenne (MSE) pour les modÃ¨les de rÃ©gression","Analyse des courbes ROC et de l'aire sous la courbe (AUC)"],relatedTerms:["Validation croisÃ©e","Train-test split","MÃ©triques de performance","Matrice de confusion"],source:"Japkowicz, N., & Shah, M. (2011). Evaluating Learning Algorithms: A Classification Perspective",domain:"machinelearning",level:"intermediate",englishTerm:"Model Evaluation"},"matrice-confusion":{term:"Matrice de confusion",shortDefinition:"Tableau qui prÃ©sente les prÃ©dictions correctes et incorrectes d'un modÃ¨le de classification par classe.",longDefinition:"Une matrice de confusion est un outil d'Ã©valuation pour les problÃ¨mes de classification qui montre le nombre de prÃ©dictions correctes et incorrectes pour chaque classe. Elle permet de calculer diverses mÃ©triques de performance comme la prÃ©cision, le rappel, le F1-score et d'analyser les types d'erreurs commises par le modÃ¨le.",examples:["Dans un problÃ¨me de classification binaire, la matrice contient les vrais positifs (TP), faux positifs (FP), vrais nÃ©gatifs (TN) et faux nÃ©gatifs (FN)","Pour la dÃ©tection de fraude, on peut voir combien de transactions frauduleuses ont Ã©tÃ© correctement identifiÃ©es vs. manquÃ©es"],relatedTerms:["PrÃ©cision","Rappel","F1-score","Classification"],source:"Fawcett, T. (2006). An introduction to ROC analysis",domain:"machinelearning",level:"beginner",englishTerm:"Confusion Matrix"},precision:{term:"PrÃ©cision",shortDefinition:"Proportion des positifs identifiÃ©s qui sont rÃ©ellement positifs.",longDefinition:"La prÃ©cision est une mÃ©trique de performance pour les modÃ¨les de classification, qui mesure la proportion des cas identifiÃ©s comme positifs qui sont effectivement des positifs rÃ©els. Elle quantifie la capacitÃ© du modÃ¨le Ã  Ã©viter les faux positifs, c'est-Ã -dire Ã  ne pas Ã©tiqueter comme positifs des cas qui sont en rÃ©alitÃ© nÃ©gatifs.",examples:["Dans un systÃ¨me de dÃ©tection de spam, la prÃ©cision mesure combien d'emails identifiÃ©s comme spam sont rÃ©ellement des spams","Formule: PrÃ©cision = VP / (VP + FP), oÃ¹ VP = vrais positifs et FP = faux positifs"],relatedTerms:["Rappel","F1-score","Matrice de confusion","SpÃ©cificitÃ©"],source:"Powers, D. M. W. (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation",domain:"machinelearning",level:"beginner",englishTerm:"Precision"},rappel:{term:"Rappel",shortDefinition:"Proportion des positifs rÃ©els qui ont Ã©tÃ© correctement identifiÃ©s.",longDefinition:"Le rappel (ou sensibilitÃ©) est une mÃ©trique de performance pour les modÃ¨les de classification, qui mesure la proportion des cas positifs rÃ©els qui ont Ã©tÃ© correctement identifiÃ©s comme tels par le modÃ¨le. Il quantifie la capacitÃ© du modÃ¨le Ã  trouver tous les cas positifs, en Ã©vitant les faux nÃ©gatifs.",examples:["Dans un test mÃ©dical, le rappel mesure la capacitÃ© Ã  identifier correctement tous les patients malades","Formule: Rappel = VP / (VP + FN), oÃ¹ VP = vrais positifs et FN = faux nÃ©gatifs"],relatedTerms:["PrÃ©cision","F1-score","Matrice de confusion","SensibilitÃ©"],source:"Powers, D. M. W. (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation",domain:"machinelearning",level:"beginner",synonyms:["SensibilitÃ©","Taux de vrais positifs"],englishTerm:"Recall"},"f1-score":{term:"F1-Score",shortDefinition:"Moyenne harmonique de la prÃ©cision et du rappel, combinant ces deux mÃ©triques en une seule valeur.",longDefinition:"Le F1-Score est une mÃ©trique de performance pour les modÃ¨les de classification qui combine la prÃ©cision et le rappel en une seule valeur. Il reprÃ©sente la moyenne harmonique de ces deux mÃ©triques, donnant un Ã©quilibre entre la capacitÃ© du modÃ¨le Ã  Ãªtre prÃ©cis (minimiser les faux positifs) et complet (minimiser les faux nÃ©gatifs).",examples:["Utile dans les cas oÃ¹ un Ã©quilibre entre prÃ©cision et rappel est nÃ©cessaire","Formule: F1 = 2 * (PrÃ©cision * Rappel) / (PrÃ©cision + Rappel)"],relatedTerms:["PrÃ©cision","Rappel","Matrice de confusion","Mesure-F"],source:"Van Rijsbergen, C. J. (1979). Information Retrieval",domain:"machinelearning",level:"intermediate",englishTerm:"F1-Score"},"auc-roc":{term:"AUC-ROC",shortDefinition:"Aire sous la courbe ROC, mesurant la capacitÃ© d'un modÃ¨le Ã  distinguer entre les classes.",longDefinition:"L'AUC-ROC (Area Under the Receiver Operating Characteristic Curve) est une mÃ©trique de performance pour les modÃ¨les de classification, reprÃ©sentant la probabilitÃ© qu'un exemple positif alÃ©atoire soit classÃ© avec un score plus Ã©levÃ© qu'un exemple nÃ©gatif alÃ©atoire. Une valeur de 1 reprÃ©sente un classifieur parfait, tandis que 0.5 Ã©quivaut Ã  un classement alÃ©atoire.",examples:["UtilisÃ© pour Ã©valuer les modÃ¨les produisant des scores de probabilitÃ© plutÃ´t que des classifications binaires directes","Permet de comparer diffÃ©rents modÃ¨les indÃ©pendamment du seuil de classification"],relatedTerms:["Courbe ROC","SensibilitÃ©","SpÃ©cificitÃ©","Seuil de classification"],source:"Fawcett, T. (2006). An introduction to ROC analysis",domain:"machinelearning",level:"intermediate",englishTerm:"AUC-ROC (Area Under the ROC Curve)"},mse:{term:"MSE (Erreur quadratique moyenne)",shortDefinition:"Moyenne des carrÃ©s des erreurs entre les valeurs prÃ©dites et rÃ©elles.",longDefinition:"L'erreur quadratique moyenne (Mean Squared Error) est une mÃ©trique d'Ã©valuation pour les problÃ¨mes de rÃ©gression, calculÃ©e comme la moyenne des carrÃ©s des diffÃ©rences entre les valeurs prÃ©dites et les valeurs rÃ©elles. Elle donne plus de poids aux grandes erreurs en raison de l'Ã©lÃ©vation au carrÃ©, ce qui la rend particuliÃ¨rement sensible aux valeurs aberrantes.",examples:["UtilisÃ©e pour Ã©valuer des modÃ¨les de prÃ©diction de prix immobiliers ou de consommation d'Ã©nergie","Formule: MSE = (1/n) * Î£(y_i - Å·_i)Â²"],relatedTerms:["RMSE","MAE","RÃ©gression","Fonction de perte"],source:"Chai, T., & Draxler, R. R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)?",domain:"machinelearning",level:"beginner",englishTerm:"MSE (Mean Squared Error)"},rmse:{term:"RMSE (Racine de l'erreur quadratique moyenne)",shortDefinition:"Racine carrÃ©e de la MSE, exprimant l'erreur dans la mÃªme unitÃ© que la variable cible.",longDefinition:"La racine de l'erreur quadratique moyenne (Root Mean Squared Error) est l'une des mÃ©triques les plus couramment utilisÃ©es pour Ã©valuer les modÃ¨les de rÃ©gression. Elle est calculÃ©e comme la racine carrÃ©e de la MSE, ce qui permet d'exprimer l'erreur dans la mÃªme unitÃ© que la variable cible, facilitant l'interprÃ©tation.",examples:["Un RMSE de 5000â‚¬ dans un modÃ¨le de prÃ©diction immobiliÃ¨re signifie que l'erreur moyenne est d'environ 5000â‚¬","Formule: RMSE = âˆš((1/n) * Î£(y_i - Å·_i)Â²)"],relatedTerms:["MSE","MAE","RÃ©gression","Fonction de perte"],source:"Chai, T., & Draxler, R. R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)?",domain:"machinelearning",level:"beginner",englishTerm:"RMSE (Root Mean Squared Error)"},"r-carre":{term:"RÂ² (Coefficient de dÃ©termination)",shortDefinition:"Proportion de la variance de la variable dÃ©pendante qui est prÃ©dictible Ã  partir des variables indÃ©pendantes.",longDefinition:"Le coefficient de dÃ©termination (RÂ²) est une mesure statistique qui reprÃ©sente la proportion de la variance dans la variable dÃ©pendante qui est prÃ©dictible Ã  partir des variables indÃ©pendantes. Il varie entre 0 et 1, oÃ¹ 1 indique que le modÃ¨le explique parfaitement toute la variabilitÃ© des donnÃ©es, et 0 indique que le modÃ¨le n'explique aucune variabilitÃ©.",examples:["Un RÂ² de 0.75 signifie que 75% de la variance de la variable cible est expliquÃ©e par le modÃ¨le","Formule: RÂ² = 1 - (Î£(y_i - Å·_i)Â² / Î£(y_i - È³)Â²)"],relatedTerms:["MSE","RÃ©gression","Variance expliquÃ©e","RÂ² ajustÃ©"],source:"Draper, N. R., & Smith, H. (1998). Applied Regression Analysis",domain:"machinelearning",level:"intermediate",englishTerm:"RÂ² (Coefficient of Determination)"},"k-means":{term:"K-means",shortDefinition:"Algorithme de clustering qui partitionne les donnÃ©es en K groupes en minimisant la variance intra-cluster.",longDefinition:"K-means est l'un des algorithmes de clustering les plus populaires et simples. Il vise Ã  partitionner n observations en k clusters, oÃ¹ chaque observation appartient au cluster avec la moyenne la plus proche. L'algorithme procÃ¨de par itÃ©rations, en alternant entre l'affectation des points aux centroÃ¯des les plus proches et la mise Ã  jour des centroÃ¯des.",examples:["Segmentation de clients pour des campagnes marketing ciblÃ©es","Compression d'images en rÃ©duisant le nombre de couleurs","Regroupement de documents par thÃ¨me"],relatedTerms:["Clustering","CentroÃ¯de","MÃ©thode du coude","Distance euclidienne"],source:"MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations",domain:"machinelearning",level:"intermediate",englishTerm:"K-means"},dbscan:{term:"DBSCAN",shortDefinition:"Algorithme de clustering basÃ© sur la densitÃ©, capable de dÃ©couvrir des clusters de forme arbitraire.",longDefinition:"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering basÃ© sur la densitÃ© qui groupe les points densÃ©ment regroupÃ©s et marque comme outliers les points dans des rÃ©gions Ã  faible densitÃ©. Contrairement Ã  K-means, DBSCAN ne nÃ©cessite pas de spÃ©cifier Ã  l'avance le nombre de clusters et peut dÃ©couvrir des clusters de forme arbitraire.",examples:["DÃ©tection d'anomalies dans les donnÃ©es de transaction","Identification de zones urbaines Ã  partir de donnÃ©es gÃ©ospatiales","Segmentation d'images basÃ©e sur la densitÃ© des pixels"],relatedTerms:["Clustering","DensitÃ©","Epsilon-voisinage","Points de bruit"],source:"Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise",domain:"machinelearning",level:"advanced",englishTerm:"DBSCAN"},pca:{term:"PCA (Analyse en Composantes Principales)",shortDefinition:"Technique de rÃ©duction de dimensionnalitÃ© qui transforme les donnÃ©es en un nouvel ensemble de variables dÃ©corrÃ©lÃ©es.",longDefinition:"L'Analyse en Composantes Principales (PCA) est une technique statistique qui transforme un ensemble de variables possiblement corrÃ©lÃ©es en un ensemble de variables linÃ©airement dÃ©corrÃ©lÃ©es appelÃ©es composantes principales. La transformation est dÃ©finie de telle sorte que la premiÃ¨re composante principale capture la plus grande variance possible, et chaque composante suivante capture la variance maximale sous contrainte d'Ãªtre orthogonale aux composantes prÃ©cÃ©dentes.",examples:["RÃ©duction de la dimensionnalitÃ© d'un dataset pour la visualisation","PrÃ©traitement des donnÃ©es avant l'application d'autres algorithmes de ML","Compression d'images en prÃ©servant les caractÃ©ristiques principales"],relatedTerms:["RÃ©duction de dimensionnalitÃ©","Valeurs propres","Vecteurs propres","Variance expliquÃ©e"],source:"Pearson, K. (1901). On lines and planes of closest fit to systems of points in space",domain:"machinelearning",level:"intermediate",englishTerm:"PCA (Principal Component Analysis)"},"t-sne":{term:"t-SNE",shortDefinition:"Algorithme de rÃ©duction de dimensionnalitÃ© non linÃ©aire adaptÃ© Ã  la visualisation de donnÃ©es de haute dimension.",longDefinition:"t-SNE (t-distributed Stochastic Neighbor Embedding) est une technique de rÃ©duction de dimensionnalitÃ© non linÃ©aire particuliÃ¨rement adaptÃ©e Ã  la visualisation de donnÃ©es de haute dimension. Elle modÃ©lise chaque objet de haute dimension par un point dans un espace de faible dimension, de maniÃ¨re Ã  ce que les objets similaires soient modÃ©lisÃ©s par des points proches et les objets dissimilaires par des points Ã©loignÃ©s.",examples:["Visualisation de clusters dans des datasets complexes comme MNIST","Exploration de donnÃ©es gÃ©nomiques","Analyse de donnÃ©es de sÃ©quenÃ§age d'ARN"],relatedTerms:["RÃ©duction de dimensionnalitÃ©","Embedding","PerplexitÃ©","Visualisation de donnÃ©es"],source:"Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE",sourceUrl:"https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",domain:"machinelearning",level:"advanced",englishTerm:"t-SNE (t-distributed Stochastic Neighbor Embedding)"},lstm:{term:"LSTM (Long Short-Term Memory)",shortDefinition:"Type de rÃ©seau de neurones rÃ©current conÃ§u pour apprendre les dÃ©pendances Ã  long terme dans les donnÃ©es sÃ©quentielles.",longDefinition:"Les LSTM (Long Short-Term Memory) sont un type spÃ©cial de rÃ©seaux de neurones rÃ©currents capables d'apprendre les dÃ©pendances Ã  long terme dans les donnÃ©es sÃ©quentielles. Ils sont conÃ§us pour Ã©viter le problÃ¨me du gradient qui s'Ã©vanouit prÃ©sent dans les RNN classiques grÃ¢ce Ã  une architecture Ã  base de cellules mÃ©moire avec des portes d'entrÃ©e, de sortie et d'oubli qui contrÃ´lent le flux d'information.",examples:["Traduction automatique","Reconnaissance vocale","GÃ©nÃ©ration de texte","PrÃ©diction de sÃ©ries temporelles financiÃ¨res"],relatedTerms:["RNN","GRU","Deep Learning","ProblÃ¨me du gradient qui s'Ã©vanouit"],source:"Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory",sourceUrl:"https://www.bioinf.jku.at/publications/older/2604.pdf",domain:"machinelearning",level:"advanced",englishTerm:"LSTM (Long Short-Term Memory)"},modÃ¨le:{term:"ModÃ¨le",shortDefinition:"ReprÃ©sentation mathÃ©matique d'un processus rÃ©el utilisÃ©e pour faire des prÃ©dictions ou des classifications.",longDefinition:"Un modÃ¨le de machine learning est une reprÃ©sentation mathÃ©matique qui capture les patterns et relations dans les donnÃ©es d'entraÃ®nement. Il s'agit d'un ensemble de rÃ¨gles, paramÃ¨tres et algorithmes qui permettent de transformer des donnÃ©es d'entrÃ©e en prÃ©dictions ou classifications. Le modÃ¨le est crÃ©Ã© pendant la phase d'entraÃ®nement et peut ensuite Ãªtre utilisÃ© pour faire des infÃ©rences sur de nouvelles donnÃ©es.",examples:["Un modÃ¨le de rÃ©gression linÃ©aire pour prÃ©dire les prix immobiliers","Un rÃ©seau de neurones pour la reconnaissance d'images","Un arbre de dÃ©cision pour la classification de clients"],relatedTerms:["Algorithme","EntraÃ®nement","PrÃ©diction","ParamÃ¨tres"],source:"Fundamentals of Machine Learning",domain:"machinelearning",level:"beginner",synonyms:["ModÃ¨le prÃ©dictif","ModÃ¨le statistique"],englishTerm:"Model"}},_e=()=>{const[s,a]=R.useState("models-work");return e.jsxs("section",{id:"introduction",className:"space-y-16",children:[e.jsxs("div",{className:"text-center mb-12",children:[e.jsx("h2",{className:"text-5xl font-bold mb-6 bg-gradient-to-r from-primary via-accent to-secondary bg-clip-text text-transparent",children:"Introduction au Machine Learning"}),e.jsx("p",{className:"text-xl text-muted-foreground max-w-4xl mx-auto leading-relaxed",children:"Cours pratique inspirÃ© de Kaggle Learn - MaÃ®trisez les fondamentaux du ML avec des exercices concrets et des visualisations interactives"}),e.jsxs("div",{className:"flex flex-wrap justify-center gap-4 mt-8",children:[e.jsx(d,{variant:"secondary",className:"px-4 py-2",children:"ðŸ“š Cours Kaggle"}),e.jsx(d,{variant:"secondary",className:"px-4 py-2",children:"ðŸ’» Exercices Pratiques"}),e.jsx(d,{variant:"secondary",className:"px-4 py-2",children:"ðŸ Python & Pandas"}),e.jsx(d,{variant:"secondary",className:"px-4 py-2",children:"ðŸ“Š Visualisations"})]})]}),e.jsx("div",{className:"flex justify-center mb-12",children:e.jsxs("div",{className:"relative group",children:[e.jsx("img",{src:"/img/machine_learning.jpg",alt:"Introduction au Machine Learning",className:"w-[80%] max-w-4xl rounded-2xl shadow-2xl hover:shadow-3xl transition-all duration-500 hover:scale-105"}),e.jsx("div",{className:"absolute inset-0 bg-gradient-to-t from-background/20 to-transparent rounded-2xl opacity-0 group-hover:opacity-100 transition-opacity duration-300"})]})}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6 mb-12",children:[e.jsxs(t,{className:"border-primary/20 hover:border-primary/40 transition-colors",children:[e.jsxs(n,{className:"text-center",children:[e.jsx(A,{className:"h-8 w-8 text-primary mx-auto mb-2"}),e.jsx(l,{className:"text-lg",children:"Objectifs"})]}),e.jsx(o,{children:e.jsxs("ul",{className:"space-y-2 text-sm",children:[e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(x,{className:"h-4 w-4 text-green-500"}),"Comprendre les modÃ¨les ML"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(x,{className:"h-4 w-4 text-green-500"}),"Explorer des donnÃ©es"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(x,{className:"h-4 w-4 text-green-500"}),"CrÃ©er votre premier modÃ¨le"]})]})})]}),e.jsxs(t,{className:"border-secondary/20 hover:border-secondary/40 transition-colors",children:[e.jsxs(n,{className:"text-center",children:[e.jsx(S,{className:"h-8 w-8 text-secondary mx-auto mb-2"}),e.jsx(l,{className:"text-lg",children:"Outils"})]}),e.jsx(o,{children:e.jsxs("ul",{className:"space-y-2 text-sm",children:[e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(E,{className:"h-4 w-4 text-blue-500"}),"Pandas pour l'analyse"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(E,{className:"h-4 w-4 text-blue-500"}),"Scikit-Learn pour le ML"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(E,{className:"h-4 w-4 text-blue-500"}),"Jupyter Notebooks"]})]})})]}),e.jsxs(t,{className:"border-accent/20 hover:border-accent/40 transition-colors",children:[e.jsxs(n,{className:"text-center",children:[e.jsx(G,{className:"h-8 w-8 text-accent mx-auto mb-2"}),e.jsx(l,{className:"text-lg",children:"PrÃ©requis"})]}),e.jsx(o,{children:e.jsxs("ul",{className:"space-y-2 text-sm",children:[e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(v,{className:"h-4 w-4 text-yellow-500"}),"Python de base"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(v,{className:"h-4 w-4 text-yellow-500"}),"Logique mathÃ©matique"]}),e.jsxs("li",{className:"flex items-center gap-2",children:[e.jsx(v,{className:"h-4 w-4 text-yellow-500"}),"CuriositÃ© d'apprendre"]})]})})]})]}),e.jsxs(V,{value:s,onValueChange:a,className:"w-full",children:[e.jsxs(B,{className:"grid w-full grid-cols-2 md:grid-cols-3 lg:grid-cols-6 h-auto p-1",children:[e.jsxs(_,{value:"models-work",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"ðŸŒ±"}),e.jsx("span",{children:"Comment Fonctionnent les ModÃ¨les"})]}),e.jsxs(_,{value:"data-exploration",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"ðŸ”"}),e.jsx("span",{children:"Exploration des DonnÃ©es"})]}),e.jsxs(_,{value:"first-model",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"ðŸ§ª"}),e.jsx("span",{children:"Premier ModÃ¨le"})]}),e.jsxs(_,{value:"model-validation",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"ðŸ“Š"}),e.jsx("span",{children:"Validation"})]}),e.jsxs(_,{value:"overfitting",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"âš–ï¸"}),e.jsx("span",{children:"Sur/Sous-ajustement"})]}),e.jsxs(_,{value:"random-forests",className:"text-xs p-3 h-auto flex flex-col gap-1",children:[e.jsx("span",{className:"text-lg",children:"ðŸŒ²"}),e.jsx("span",{children:"ForÃªts AlÃ©atoires"})]})]}),e.jsxs(g,{value:"models-work",className:"space-y-8",children:[e.jsxs("div",{className:"text-center mb-8",children:[e.jsx("h3",{className:"text-4xl font-bold mb-4 bg-gradient-to-r from-green-500 to-emerald-600 bg-clip-text text-transparent",children:"ðŸŒ± Comment Fonctionnent les ModÃ¨les"}),e.jsx("p",{className:"text-xl text-muted-foreground",children:"Comprendre les fondements des modÃ¨les prÃ©dictifs avec des arbres de dÃ©cision"})]}),e.jsx(u,{title:"Qu'est-ce qu'un modÃ¨le de Machine Learning ?",type:"concept",className:"mb-6",children:e.jsxs("div",{className:"space-y-6",children:[e.jsxs("div",{className:"flex items-start gap-4 p-4 bg-gradient-to-r from-blue-50 to-indigo-50 rounded-lg",children:[e.jsx(v,{className:"h-8 w-8 text-blue-500 mt-1 flex-shrink-0"}),e.jsx("div",{children:e.jsxs("p",{className:"text-lg leading-relaxed",children:["Un ",e.jsx(r,{definition:i.modÃ¨le,children:"modÃ¨le"})," de machine learning est un ",e.jsx("strong",{children:"ensemble de rÃ¨gles informatiques"}),' qui permet de faire des prÃ©dictions Ã  partir de donnÃ©es. Imaginez-le comme une "recette intelligente" qui transforme des informations en rÃ©ponses prÃ©cises.']})})]}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-green-50 to-emerald-50 p-6 rounded-lg border-l-4 border-green-400",children:[e.jsxs("h4",{className:"font-semibold text-green-800 mb-3 flex items-center gap-2",children:[e.jsx(y,{className:"h-5 w-5"}),"Arbre de DÃ©cision : Votre Premier ModÃ¨le"]}),e.jsx("p",{className:"text-green-700 mb-4",children:"Un arbre de dÃ©cision fonctionne comme un questionnaire intelligent. Il pose des questions successives sur vos donnÃ©es pour arriver Ã  une prÃ©diction."}),e.jsx("div",{className:"bg-white p-4 rounded border-2 border-dashed border-green-300",children:e.jsxs("p",{className:"text-sm font-mono text-green-600",children:["Taille > 150mÂ² ?",e.jsx("br",{}),"â”œâ”€ OUI â†’ Chambres > 3 ?",e.jsx("br",{}),"â”‚   â”œâ”€ OUI â†’ Prix: Ã‰levÃ©",e.jsx("br",{}),"â”‚   â””â”€ NON â†’ Prix: Moyen",e.jsx("br",{}),"â””â”€ NON â†’ Prix: Bas"]})})]}),e.jsxs("div",{className:"bg-gradient-to-br from-blue-50 to-cyan-50 p-6 rounded-lg border-l-4 border-blue-400",children:[e.jsxs("h4",{className:"font-semibold text-blue-800 mb-3 flex items-center gap-2",children:[e.jsx(A,{className:"h-5 w-5"}),"Exemple Concret : Prix d'une Maison"]}),e.jsx("p",{className:"text-blue-700 mb-4",children:"Pour prÃ©dire le prix d'une maison, notre modÃ¨le examine des caractÃ©ristiques comme :"}),e.jsxs("ul",{className:"space-y-2",children:[e.jsxs("li",{className:"flex items-center gap-2 text-blue-600",children:[e.jsx(x,{className:"h-4 w-4"}),"Surface habitable (mÂ²)"]}),e.jsxs("li",{className:"flex items-center gap-2 text-blue-600",children:[e.jsx(x,{className:"h-4 w-4"}),"Nombre de chambres"]}),e.jsxs("li",{className:"flex items-center gap-2 text-blue-600",children:[e.jsx(x,{className:"h-4 w-4"}),"Localisation (quartier)"]}),e.jsxs("li",{className:"flex items-center gap-2 text-blue-600",children:[e.jsx(x,{className:"h-4 w-4"}),"AnnÃ©e de construction"]})]})]})]})]})}),e.jsxs(t,{className:"bg-gradient-to-br from-purple-50 to-pink-50 border-purple-200",children:[e.jsxs(n,{children:[e.jsxs(l,{className:"flex items-center gap-2 text-purple-700",children:[e.jsx(ee,{className:"h-5 w-5"}),'Visualisation : Comment un Arbre "Voit" les DonnÃ©es']}),e.jsx(f,{children:"Un arbre de dÃ©cision divise l'espace des donnÃ©es en rÃ©gions homogÃ¨nes"})]}),e.jsx(o,{children:e.jsxs("div",{className:"bg-white p-6 rounded-lg border-2 border-dashed border-purple-300",children:[e.jsx("div",{className:"text-center mb-4",children:e.jsx("p",{className:"font-semibold text-purple-700",children:"Segmentation des DonnÃ©es"})}),e.jsxs("div",{className:"grid grid-cols-2 gap-4 text-sm",children:[e.jsxs("div",{className:"bg-green-100 p-3 rounded border-l-4 border-green-500",children:[e.jsx("strong",{className:"text-green-700",children:"RÃ©gion 1 :"}),e.jsx("br",{}),"Surface > 150mÂ² + Chambres > 3",e.jsx("br",{}),e.jsx("span",{className:"text-green-600",children:"â†’ Prix Ã‰levÃ© (â‚¬300k+)"})]}),e.jsxs("div",{className:"bg-yellow-100 p-3 rounded border-l-4 border-yellow-500",children:[e.jsx("strong",{className:"text-yellow-700",children:"RÃ©gion 2 :"}),e.jsx("br",{}),"Surface > 150mÂ² + Chambres â‰¤ 3",e.jsx("br",{}),e.jsx("span",{className:"text-yellow-600",children:"â†’ Prix Moyen (â‚¬200-300k)"})]}),e.jsxs("div",{className:"bg-orange-100 p-3 rounded border-l-4 border-orange-500",children:[e.jsx("strong",{className:"text-orange-700",children:"RÃ©gion 3 :"}),e.jsx("br",{}),"Surface â‰¤ 150mÂ² + Garage",e.jsx("br",{}),e.jsx("span",{className:"text-orange-600",children:"â†’ Prix Moyen-Bas (â‚¬150-200k)"})]}),e.jsxs("div",{className:"bg-red-100 p-3 rounded border-l-4 border-red-500",children:[e.jsx("strong",{className:"text-red-700",children:"RÃ©gion 4 :"}),e.jsx("br",{}),"Surface â‰¤ 150mÂ² + Pas de garage",e.jsx("br",{}),e.jsx("span",{className:"text-red-600",children:"â†’ Prix Bas (<â‚¬150k)"})]})]})]})})]}),e.jsx(h,{title:"ðŸŽ¯ Exercice Pratique : Construisez Votre Arbre de DÃ©cision",problem:"Vous Ãªtes consultant pour une concession automobile. CrÃ©ez un arbre de dÃ©cision pour prÃ©dire si un client achÃ¨tera une voiture Ã©lectrique. Vous devez poser exactement 3 questions pour segmenter vos clients.",solution:`
**Solution Optimale :**

1. **Budget disponible â‰¥ 35kâ‚¬ ?**
   - NON â†’ Proposer voiture hybride (pas Ã©lectrique)
   - OUI â†’ Question 2

2. **Trajet quotidien < 100km ?**
   - NON â†’ Ã‰valuer infrastructure (Question 3)  
   - OUI â†’ Question 3

3. **AccÃ¨s Ã  borne de recharge Ã  domicile/travail ?**
   - NON â†’ Voiture hybride recommandÃ©e
   - OUI â†’ **Voiture Ã©lectrique recommandÃ©e** âœ…

**Logique :** Le budget Ã©limine d'abord les non-Ã©ligibles, puis l'autonomie et l'infrastructure dÃ©terminent la faisabilitÃ© pratique.
            `,hints:["ðŸ’° Commencez par le critÃ¨re financier (budget)","ðŸš— Pensez aux contraintes d'usage quotidien (distance)","ðŸ”Œ L'infrastructure de recharge est cruciale","ðŸ“Š Chaque question doit diviser les clients en groupes distincts"],difficulty:"dÃ©butant",estimatedTime:"10 min"}),e.jsxs("div",{className:"bg-gradient-to-r from-green-500 to-emerald-600 text-white p-6 rounded-lg",children:[e.jsxs("h4",{className:"font-bold text-xl mb-3 flex items-center gap-2",children:[e.jsx(x,{className:"h-6 w-6"}),"Points ClÃ©s Ã  Retenir"]}),e.jsxs("ul",{className:"space-y-2",children:[e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx(j,{className:"h-5 w-5 mt-0.5 flex-shrink-0"}),"Un modÃ¨le ML = ensemble de rÃ¨gles pour faire des prÃ©dictions"]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx(j,{className:"h-5 w-5 mt-0.5 flex-shrink-0"}),"Les arbres de dÃ©cision sont intuitifs et visuels"]}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx(j,{className:"h-5 w-5 mt-0.5 flex-shrink-0"}),'Chaque "split" divise les donnÃ©es en groupes plus homogÃ¨nes']}),e.jsxs("li",{className:"flex items-start gap-2",children:[e.jsx(j,{className:"h-5 w-5 mt-0.5 flex-shrink-0"}),"La qualitÃ© des questions dÃ©termine la prÃ©cision du modÃ¨le"]})]})]})]}),e.jsxs(g,{value:"data-exploration",className:"space-y-8",children:[e.jsx(u,{title:"ðŸ” 2. Basic Data Exploration",type:"concept",children:e.jsx("div",{className:"space-y-6",children:e.jsxs("div",{className:"bg-gradient-to-r from-blue-50 to-purple-50 p-6 rounded-xl border-l-4 border-blue-500",children:[e.jsx("h4",{className:"font-bold text-blue-800 mb-3",children:"ðŸŽ¯ Objectif : Apprendre Ã  explorer un dataset avec Pandas"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-2",children:"ðŸ“¥ 1. Chargement des donnÃ©es"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono",children:["import pandas as pd",e.jsx("br",{}),"data = pd.read_csv('melbourne_housing.csv')"]}),e.jsx("p",{className:"text-sm mt-2",children:"PremiÃ¨re Ã©tape cruciale : charger vos donnÃ©es depuis un fichier CSV."})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-purple-500",children:[e.jsx("h5",{className:"font-semibold text-purple-800 mb-2",children:"ðŸ” 2. Analyse descriptive"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-3",children:[e.jsxs("div",{className:"bg-purple-50 p-3 rounded",children:[e.jsx("div",{className:"font-mono text-sm mb-1",children:"data.describe()"}),e.jsx("p",{className:"text-xs",children:"Statistiques de base (moyenne, mÃ©diane, etc.)"})]}),e.jsxs("div",{className:"bg-purple-50 p-3 rounded",children:[e.jsx("div",{className:"font-mono text-sm mb-1",children:"data.columns"}),e.jsx("p",{className:"text-xs",children:"Liste toutes les colonnes disponibles"})]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-yellow-500",children:[e.jsx("h5",{className:"font-semibold text-yellow-800 mb-2",children:"ðŸŽ¯ 3. Identification des variables"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-3",children:[e.jsxs("div",{className:"bg-yellow-50 p-3 rounded",children:[e.jsx("strong",{children:"Features (caractÃ©ristiques) :"}),e.jsx("p",{className:"text-sm",children:"Rooms, Bathroom, Landsize, BuildingArea..."})]}),e.jsxs("div",{className:"bg-orange-50 p-3 rounded",children:[e.jsx("strong",{children:"Target (variable cible) :"}),e.jsx("p",{className:"text-sm",children:"Price (ce qu'on veut prÃ©dire)"})]})]})]})]})]})})}),e.jsx(h,{title:"Exercice : Exploration du Dataset Melbourne House Prices",problem:"Explorez le dataset des prix immobiliers de Melbourne et extrayez les statistiques clÃ©s : prix mÃ©dian, nombre moyen de chambres, taille moyenne du terrain.",solution:"Prix mÃ©dian : 1,035,000 AUD | Chambres moyennes : 2.9 | Terrain moyen : 558 mÂ²",hints:["Utilisez data['Price'].median() pour le prix mÃ©dian","data['Rooms'].mean() pour la moyenne des chambres","N'oubliez pas de gÃ©rer les valeurs manquantes avec dropna()"],difficulty:"dÃ©butant",estimatedTime:"10 min"})]}),e.jsxs(g,{value:"first-model",className:"space-y-8",children:[e.jsx(u,{title:"ðŸ§ª 3. Your First Machine Learning Model",type:"concept",children:e.jsx("div",{className:"space-y-6",children:e.jsxs("div",{className:"bg-gradient-to-r from-purple-50 to-pink-50 p-6 rounded-xl border-l-4 border-purple-500",children:[e.jsx("h4",{className:"font-bold text-purple-800 mb-3",children:"ðŸŽ¯ Objectif : Construire un modÃ¨le prÃ©dictif simple"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-blue-500",children:[e.jsx("h5",{className:"font-semibold text-blue-800 mb-2",children:"1ï¸âƒ£ SÃ©lection des features et de la target"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea']",e.jsx("br",{}),"X = data[features]",e.jsx("br",{}),"y = data['Price']"]}),e.jsx("p",{className:"text-sm",children:"Choisissez soigneusement vos variables d'entrÃ©e (X) et votre cible (y)."})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-2",children:"2ï¸âƒ£ Utilisation de Scikit-Learn"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["from sklearn.tree import DecisionTreeRegressor",e.jsx("br",{}),"model = DecisionTreeRegressor(random_state=1)",e.jsx("br",{}),"model.fit(X, y)  # EntraÃ®nement",e.jsx("br",{}),"predictions = model.predict(X)  # PrÃ©dictions"]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-2 mt-3",children:[e.jsxs("div",{className:"bg-green-50 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"DÃ©finir"}),e.jsx("p",{className:"text-xs",children:"DecisionTreeRegressor"})]}),e.jsxs("div",{className:"bg-blue-50 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"EntraÃ®ner"}),e.jsx("p",{className:"text-xs",children:"model.fit(X, y)"})]}),e.jsxs("div",{className:"bg-purple-50 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"PrÃ©dire"}),e.jsx("p",{className:"text-xs",children:"model.predict()"})]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-red-500",children:[e.jsx("h5",{className:"font-semibold text-red-800 mb-2",children:"3ï¸âƒ£ Ã‰valuation avec MAE"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["from sklearn.metrics import mean_absolute_error",e.jsx("br",{}),"mae = mean_absolute_error(y, predictions)"]}),e.jsx("div",{className:"bg-red-50 p-3 rounded",children:e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"MAE (Mean Absolute Error) :"})," Moyenne des erreurs en valeur absolue. Plus c'est bas, mieux c'est !"]})})]})]})]})})}),e.jsx(h,{title:"Exercice : Construisez votre premier modÃ¨le",problem:"CrÃ©ez un modÃ¨le de prÃ©diction de prix avec les features : Rooms, Bathroom, Landsize. Calculez la MAE sur vos prÃ©dictions.",solution:"MAE â‰ˆ 434 (peut varier selon les donnÃ©es). Le modÃ¨le prÃ©dit avec une erreur moyenne de 434 unitÃ©s monÃ©taires.",hints:["N'oubliez pas d'importer DecisionTreeRegressor","Utilisez random_state=1 pour des rÃ©sultats reproductibles","La MAE vous donne l'erreur moyenne en valeur absolue"],difficulty:"intermÃ©diaire",estimatedTime:"15 min"})]}),e.jsxs(g,{value:"model-validation",className:"space-y-8",children:[e.jsx(u,{title:"ðŸ“Š 4. Model Validation",type:"concept",children:e.jsx("div",{className:"space-y-6",children:e.jsxs("div",{className:"bg-gradient-to-r from-orange-50 to-red-50 p-6 rounded-xl border-l-4 border-orange-500",children:[e.jsx("h4",{className:"font-bold text-orange-800 mb-3",children:"ðŸŽ¯ Objectif : Valider la fiabilitÃ© d'un modÃ¨le"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-red-500",children:[e.jsx("h5",{className:"font-semibold text-red-800 mb-2",children:`âš ï¸ ProblÃ¨me de l'"In-Sample Score"`}),e.jsxs("p",{className:"text-sm mb-3",children:["Un modÃ¨le performant sur les donnÃ©es d'entraÃ®nement peut ",e.jsx("strong",{children:"Ã©chouer sur de nouvelles donnÃ©es"}),". C'est comme rÃ©viser avec les rÃ©ponses du test - on ne teste pas vraiment ses connaissances !"]}),e.jsx("div",{className:"bg-red-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Analogie :"})," Un Ã©tudiant qui mÃ©morise par cÅ“ur aura 20/20 sur les mÃªmes questions, mais 5/20 sur un nouveau test !"]})})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-2",children:"âœ… Solution : Train-Test Split"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["from sklearn.model_selection import train_test_split",e.jsx("br",{}),"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-3 mt-3",children:[e.jsxs("div",{className:"bg-green-50 p-3 rounded",children:[e.jsx("strong",{className:"text-sm",children:"80% EntraÃ®nement"}),e.jsx("p",{className:"text-xs",children:"Le modÃ¨le apprend sur ces donnÃ©es"})]}),e.jsxs("div",{className:"bg-blue-50 p-3 rounded",children:[e.jsx("strong",{className:"text-sm",children:"20% Validation"}),e.jsx("p",{className:"text-xs",children:'Test sur donnÃ©es "inÃ©dites"'})]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-purple-500",children:[e.jsx("h5",{className:"font-semibold text-purple-800 mb-2",children:"ðŸ“ Calcul de la MAE sur la validation"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["model.fit(X_train, y_train)",e.jsx("br",{}),"val_predictions = model.predict(X_val)",e.jsx("br",{}),"val_mae = mean_absolute_error(y_val, val_predictions)"]}),e.jsx("p",{className:"text-sm",children:"Cette MAE reflÃ¨te mieux la performance rÃ©elle du modÃ¨le !"})]})]})]})})}),e.jsx(h,{title:"Exercice : ImplÃ©mentation du Train-Test Split",problem:"SÃ©parez vos donnÃ©es en 80% entraÃ®nement / 20% validation. Comparez la MAE sur l'entraÃ®nement vs validation.",solution:"MAE train : ~434 | MAE validation : ~672 (la diffÃ©rence indique un possible overfitting)",hints:["Utilisez test_size=0.2 pour 80%/20%","Fixez random_state pour des rÃ©sultats reproductibles","La MAE de validation est gÃ©nÃ©ralement plus Ã©levÃ©e"],difficulty:"intermÃ©diaire",estimatedTime:"10 min"})]}),e.jsxs(g,{value:"overfitting",className:"space-y-8",children:[e.jsx(u,{title:"âš–ï¸ 5. Underfitting and Overfitting",type:"concept",children:e.jsx("div",{className:"space-y-6",children:e.jsxs("div",{className:"bg-gradient-to-r from-yellow-50 to-orange-50 p-6 rounded-xl border-l-4 border-yellow-500",children:[e.jsx("h4",{className:"font-bold text-yellow-800 mb-3",children:"ðŸŽ¯ Objectif : Diagnostiquer les problÃ¨mes de complexitÃ© du modÃ¨le"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-blue-500",children:[e.jsx("h5",{className:"font-semibold text-blue-800 mb-2",children:"ðŸ“‰ Sous-ajustement (Underfitting)"}),e.jsxs("div",{className:"space-y-2 text-sm",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"DÃ©finition :"})," ModÃ¨le trop simple"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"SymptÃ´me :"})," Arbre avec peu de feuilles"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"ProblÃ¨me :"})," N'arrive pas Ã  capturer les tendances"]}),e.jsx("div",{className:"bg-blue-50 p-2 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Analogie :"})," Ã‰tudiant qui n'a pas assez rÃ©visÃ©"]})})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-red-500",children:[e.jsx("h5",{className:"font-semibold text-red-800 mb-2",children:"ðŸ“ˆ Sur-ajustement (Overfitting)"}),e.jsxs("div",{className:"space-y-2 text-sm",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"DÃ©finition :"})," ModÃ¨le trop complexe"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"SymptÃ´me :"})," Arbre avec trop de feuilles"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"ProblÃ¨me :"})," MÃ©morise le bruit des donnÃ©es"]}),e.jsx("div",{className:"bg-red-50 p-2 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Analogie :"})," Ã‰tudiant qui connaÃ®t par cÅ“ur mais ne comprend pas"]})})]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-2",children:"ðŸŽ›ï¸ ContrÃ´le via max_leaf_nodes"}),e.jsx("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:"model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-2 mt-3",children:[e.jsxs("div",{className:"bg-red-100 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"Peu de feuilles"}),e.jsx("p",{className:"text-xs",children:"Underfitting"})]}),e.jsxs("div",{className:"bg-green-100 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"Juste ce qu'il faut"}),e.jsx("p",{className:"text-xs",children:"Sweet spot!"})]}),e.jsxs("div",{className:"bg-red-100 p-2 rounded text-center",children:[e.jsx("strong",{className:"text-xs",children:"Trop de feuilles"}),e.jsx("p",{className:"text-xs",children:"Overfitting"})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-r from-purple-100 to-blue-100 p-4 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-purple-800 mb-2",children:"ðŸŽ¯ Trouver le Sweet Spot"}),e.jsx("p",{className:"text-sm mb-2",children:"Testez diffÃ©rentes valeurs de max_leaf_nodes et choisissez celle qui minimise la MAE de validation :"}),e.jsxs("div",{className:"grid grid-cols-2 md:grid-cols-4 gap-2 text-xs",children:[e.jsx("div",{className:"bg-white p-2 rounded",children:"50 feuilles â†’ MAE: 652"}),e.jsx("div",{className:"bg-white p-2 rounded",children:"100 feuilles â†’ MAE: 634"}),e.jsx("div",{className:"bg-green-200 p-2 rounded",children:e.jsx("strong",{children:"250 feuilles â†’ MAE: 628"})}),e.jsx("div",{className:"bg-white p-2 rounded",children:"500 feuilles â†’ MAE: 645"})]})]})]})]})})}),e.jsx(h,{title:"Exercice : ExpÃ©rimentation avec max_leaf_nodes",problem:"Testez diffÃ©rentes valeurs de max_leaf_nodes (50, 100, 250, 500) et trouvez celle qui donne la meilleure MAE de validation.",solution:"La valeur optimale est gÃ©nÃ©ralement autour de 250 feuilles (peut varier selon les donnÃ©es)",hints:["CrÃ©ez une boucle pour tester diffÃ©rentes valeurs","Stockez les MAE de validation pour chaque test","La meilleure valeur minimise la MAE de validation"],difficulty:"intermÃ©diaire",estimatedTime:"15 min"})]}),e.jsxs(g,{value:"random-forests",className:"space-y-8",children:[e.jsx(u,{title:"ðŸŒ² 6. Random Forests",type:"concept",children:e.jsx("div",{className:"space-y-6",children:e.jsxs("div",{className:"bg-gradient-to-r from-green-50 to-teal-50 p-6 rounded-xl border-l-4 border-green-500",children:[e.jsx("h4",{className:"font-bold text-green-800 mb-3",children:"ðŸŽ¯ Objectif : AmÃ©liorer les prÃ©dictions avec des ensembles d'arbres"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-orange-500",children:[e.jsx("h5",{className:"font-semibold text-orange-800 mb-2",children:"âš ï¸ Limites des arbres de dÃ©cision uniques"}),e.jsxs("p",{className:"text-sm mb-3",children:["Les arbres de dÃ©cision sont ",e.jsx("strong",{children:"sensibles aux petites variations"})," des donnÃ©es. Changer quelques points peut crÃ©er un arbre complÃ¨tement diffÃ©rent !"]}),e.jsx("div",{className:"bg-orange-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Analogie :"})," Comme demander l'avis Ã  une seule personne - c'est risquÃ© ! Mieux vaut consulter plusieurs experts."]})})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-2",children:"ðŸŒ³ Principe des ForÃªts AlÃ©atoires"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("p",{className:"text-sm",children:["Les Random Forests combinent ",e.jsx("strong",{children:"plusieurs arbres indÃ©pendants"})," :"]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-3",children:[e.jsxs("div",{className:"bg-green-50 p-3 rounded text-center",children:[e.jsx(y,{className:"h-6 w-6 mx-auto mb-2 text-green-600"}),e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Arbre 1"}),e.jsx("br",{}),"DonnÃ©es Ã©chantillon A"]})]}),e.jsxs("div",{className:"bg-green-50 p-3 rounded text-center",children:[e.jsx(y,{className:"h-6 w-6 mx-auto mb-2 text-green-600"}),e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Arbre 2"}),e.jsx("br",{}),"DonnÃ©es Ã©chantillon B"]})]}),e.jsxs("div",{className:"bg-green-50 p-3 rounded text-center",children:[e.jsx(y,{className:"h-6 w-6 mx-auto mb-2 text-green-600"}),e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Arbre N"}),e.jsx("br",{}),"DonnÃ©es Ã©chantillon N"]})]})]}),e.jsx("div",{className:"text-center",children:e.jsx("div",{className:"bg-blue-100 p-3 rounded",children:e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"PrÃ©diction finale ="})," Moyenne des prÃ©dictions de tous les arbres"]})})})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-blue-500",children:[e.jsx("h5",{className:"font-semibold text-blue-800 mb-2",children:"ðŸ“Š Avantages des Random Forests"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-3",children:[e.jsxs("div",{className:"space-y-2",children:[e.jsxs("div",{className:"bg-blue-50 p-2 rounded text-sm",children:[e.jsx("strong",{children:"ðŸ›¡ï¸ RÃ©duction du sur-ajustement"}),e.jsx("br",{}),e.jsx("span",{className:"text-xs",children:"La moyenne lisse les erreurs individuelles"})]}),e.jsxs("div",{className:"bg-blue-50 p-2 rounded text-sm",children:[e.jsx("strong",{children:"ðŸŽ¯ Meilleure gÃ©nÃ©ralisation"}),e.jsx("br",{}),e.jsx("span",{className:"text-xs",children:"Plus robuste sur nouvelles donnÃ©es"})]})]}),e.jsxs("div",{className:"space-y-2",children:[e.jsxs("div",{className:"bg-blue-50 p-2 rounded text-sm",children:[e.jsx("strong",{children:"âš¡ Relativement rapide"}),e.jsx("br",{}),e.jsx("span",{className:"text-xs",children:"Arbres entraÃ®nÃ©s en parallÃ¨le"})]}),e.jsxs("div",{className:"bg-blue-50 p-2 rounded text-sm",children:[e.jsx("strong",{children:"ðŸ”§ Peu de paramÃ©trage"}),e.jsx("br",{}),e.jsx("span",{className:"text-xs",children:'Fonctionne bien "out of the box"'})]})]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-l-4 border-purple-500",children:[e.jsx("h5",{className:"font-semibold text-purple-800 mb-2",children:"ðŸ’» Utilisation avec Scikit-Learn"}),e.jsxs("div",{className:"bg-gray-100 p-3 rounded text-sm font-mono mb-2",children:["from sklearn.ensemble import RandomForestRegressor",e.jsx("br",{}),"rf_model = RandomForestRegressor(n_estimators=100, random_state=1)",e.jsx("br",{}),"rf_model.fit(X_train, y_train)",e.jsx("br",{}),"rf_predictions = rf_model.predict(X_val)"]}),e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"n_estimators=100"})," signifie qu'on utilise 100 arbres dans la forÃªt."]})]})]})]})})}),e.jsx(h,{title:"Exercice : Remplacement par une Random Forest",problem:"Remplacez votre arbre de dÃ©cision par une Random Forest avec 100 arbres. Comparez les performances MAE.",solution:"Random Forest MAE : ~602 (amÃ©lioration vs arbre unique : ~628)",hints:["Importez RandomForestRegressor depuis sklearn.ensemble","Utilisez n_estimators=100 pour 100 arbres","Comparez la MAE avec votre modÃ¨le d'arbre unique"],difficulty:"dÃ©butant",estimatedTime:"10 min"}),e.jsxs("div",{className:"bg-gradient-to-r from-emerald-50 to-teal-50 p-6 rounded-xl border-l-4 border-emerald-500",children:[e.jsx("h4",{className:"font-bold text-emerald-800 mb-4 text-center",children:"âœ… Points clÃ©s du cours"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-blue-800 mb-1",children:"ðŸŽ¯ Public visÃ©"}),e.jsx("p",{className:"text-sm",children:"DÃ©butants en Python, sans prÃ©requis en ML"})]}),e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-green-800 mb-1",children:"ðŸ› ï¸ Outils"}),e.jsx("p",{className:"text-sm",children:"Pandas pour l'exploration, Scikit-Learn pour la modÃ©lisation"})]}),e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-purple-800 mb-1",children:"ðŸ“š PÃ©dagogie"}),e.jsx("p",{className:"text-sm",children:"Approche pratique avec exercices intÃ©grÃ©s"})]})]}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-orange-800 mb-1",children:"ðŸ’» Environnement"}),e.jsx("p",{className:"text-sm",children:"Notebooks Jupyter exÃ©cutÃ©s directement"})]}),e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-red-800 mb-1",children:"âš¡ Installation"}),e.jsx("p",{className:"text-sm",children:"Aucune installation requise"})]}),e.jsxs("div",{className:"bg-white p-3 rounded-lg",children:[e.jsx("h5",{className:"font-semibold text-teal-800 mb-1",children:"ðŸŽ® InteractivitÃ©"}),e.jsx("p",{className:"text-sm",children:"SchÃ©mas et exercices interactifs"})]})]})]})]})]})]})]})},ge=()=>{const s=[{title:"Apprentissage SupervisÃ©",description:"MaÃ®trisez la classification et la rÃ©gression avec des algorithmes comme Random Forest, SVM et les rÃ©seaux de neurones. Cours complet avec exemples pratiques et exercices interactifs.",href:"/machine-learning/supervised",icon:e.jsx(K,{className:"h-8 w-8"}),color:"bg-blue-500",duration:"4-6 heures",level:"IntermÃ©diaire",modules:"8 modules",highlights:["Classification binaire et multi-classe","Algorithmes de rÃ©gression avancÃ©s","Ã‰valuation et optimisation des modÃ¨les","Projets pratiques complets"]},{title:"Apprentissage Non SupervisÃ©",description:"DÃ©couvrez le clustering, la rÃ©duction de dimensionnalitÃ© et la dÃ©tection d'anomalies. Explorez K-means, PCA, t-SNE et leurs applications concrÃ¨tes.",href:"/machine-learning/unsupervised",icon:e.jsx(H,{className:"h-8 w-8"}),color:"bg-purple-500",duration:"3-4 heures",level:"IntermÃ©diaire",modules:"6 modules",highlights:["Algorithmes de clustering avancÃ©s","Techniques de rÃ©duction de dimensionnalitÃ©","DÃ©tection d'anomalies et d'outliers","Visualisation de donnÃ©es complexes"]},{title:"Apprentissage par Renforcement",description:"Plongez dans l'IA qui apprend par l'interaction : Q-learning, Deep Q-Networks, et applications en robotique et jeux. Le futur de l'intelligence artificielle.",href:"/machine-learning/reinforcement",icon:e.jsx(C,{className:"h-8 w-8"}),color:"bg-orange-500",duration:"5-7 heures",level:"AvancÃ©",modules:"10 modules",highlights:["Processus de dÃ©cision markoviens","Algorithmes Q-learning et SARSA","Deep Reinforcement Learning","Applications en robotique et jeux"]}];return e.jsxs("section",{id:"advanced-courses",className:"space-y-8",children:[e.jsxs("div",{className:"text-center space-y-4",children:[e.jsx("h2",{className:"text-3xl font-bold",children:"Cours Approfondis"}),e.jsx("p",{className:"text-lg text-muted-foreground max-w-3xl mx-auto",children:"Explorez en dÃ©tail les trois piliers du Machine Learning avec nos cours interactifs complets, conÃ§us pour vous transformer en expert de l'apprentissage automatique."})]}),e.jsx("div",{className:"grid grid-cols-1 lg:grid-cols-3 gap-6",children:s.map((a,b)=>e.jsxs(t,{className:"group hover:shadow-xl transition-all duration-300 border-0 shadow-lg relative overflow-hidden",children:[e.jsx("div",{className:`absolute top-0 left-0 right-0 h-1 ${a.color}`}),e.jsxs(n,{className:"space-y-4",children:[e.jsxs("div",{className:"flex items-center justify-between",children:[e.jsx("div",{className:`p-3 rounded-xl ${a.color} text-white`,children:a.icon}),e.jsx(d,{variant:"outline",className:"text-xs",children:a.level})]}),e.jsxs("div",{children:[e.jsx(l,{className:"text-xl group-hover:text-primary transition-colors",children:a.title}),e.jsx(f,{className:"text-sm mt-2 line-clamp-3",children:a.description})]})]}),e.jsxs(o,{className:"space-y-6",children:[e.jsxs("div",{className:"flex items-center justify-between text-sm text-muted-foreground",children:[e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx(se,{className:"h-4 w-4"}),e.jsx("span",{children:a.duration})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx(D,{className:"h-4 w-4"}),e.jsx("span",{children:a.modules})]})]}),e.jsxs("div",{className:"space-y-3",children:[e.jsx("h4",{className:"font-semibold text-sm",children:"Ce que vous apprendrez :"}),e.jsx("ul",{className:"space-y-1",children:a.highlights.slice(0,3).map((N,c)=>e.jsxs("li",{className:"text-xs text-muted-foreground flex items-start gap-2",children:[e.jsx("span",{className:"text-primary mt-1",children:"â€¢"}),e.jsx("span",{children:N})]},c))})]}),e.jsx($,{asChild:!0,className:"w-full group-hover:scale-105 transition-transform",children:e.jsxs(W,{to:a.href,className:"flex items-center justify-center gap-2",children:["Commencer le cours",e.jsx(j,{className:"h-4 w-4 group-hover:translate-x-1 transition-transform"})]})})]})]},b))}),e.jsxs("div",{className:"bg-gradient-to-r from-blue-50 to-purple-50 p-8 rounded-xl border text-center space-y-4",children:[e.jsx("h3",{className:"text-2xl font-bold",children:"PrÃªt Ã  devenir un expert ?"}),e.jsx("p",{className:"text-muted-foreground max-w-2xl mx-auto",children:"Nos cours approfondis combinent thÃ©orie rigoureuse et pratique intensive. Chaque cours inclut des exercices interactifs, des projets concrets et une communautÃ© d'apprentissage."}),e.jsxs("div",{className:"flex items-center justify-center gap-4 text-sm text-muted-foreground",children:[e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx(G,{className:"h-4 w-4"}),e.jsx("span",{children:"CommunautÃ© active"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx(D,{className:"h-4 w-4"}),e.jsx("span",{children:"Contenu mis Ã  jour"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx(C,{className:"h-4 w-4"}),e.jsx("span",{children:"Projets pratiques"})]})]})]})]})},fe=()=>e.jsxs("section",{id:"supervised",className:"space-y-8",children:[e.jsx("h2",{className:"text-3xl font-bold mb-6",children:"Apprentissage SupervisÃ©"}),e.jsxs("p",{className:"text-lg mb-6",children:["L'",e.jsx(r,{definition:i["apprentissage-supervise"],children:"apprentissage supervisÃ©"})," est une approche oÃ¹ l'algorithme apprend Ã  partir de donnÃ©es Ã©tiquetÃ©es. L'objectif est de prÃ©dire des valeurs pour de nouvelles donnÃ©es non Ã©tiquetÃ©es."]}),e.jsxs(V,{defaultValue:"classification",className:"w-full mb-8",children:[e.jsxs(B,{className:"grid w-full grid-cols-1 md:grid-cols-2 mb-6",children:[e.jsx(_,{value:"classification",children:"Classification"}),e.jsx(_,{value:"regression",children:"RÃ©gression"})]}),e.jsx(g,{value:"classification",className:"space-y-6",children:e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-8",children:[e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:e.jsx(r,{definition:i.classification,children:"Classification"})}),e.jsx("p",{className:"mb-4",children:"La classification consiste Ã  prÃ©dire une catÃ©gorie ou une classe pour une instance de donnÃ©es. La sortie est discrÃ¨te (ex: spam/non-spam, malade/sain)."}),e.jsx("h4",{className:"font-medium mt-4 mb-2",children:"Algorithmes populaires :"}),e.jsxs("ul",{className:"list-disc pl-6 space-y-1",children:[e.jsx("li",{children:"RÃ©gression logistique"}),e.jsx("li",{children:"Arbres de dÃ©cision"}),e.jsx("li",{children:"ForÃªts alÃ©atoires"}),e.jsx("li",{children:"Machines Ã  vecteurs de support (SVM)"}),e.jsx("li",{children:"k-plus proches voisins (k-NN)"}),e.jsx("li",{children:"RÃ©seaux de neurones"})]})]}),e.jsxs(t,{children:[e.jsxs(n,{children:[e.jsx(l,{children:"Exemple de code : Classification"}),e.jsx(f,{children:"Utilisation de Scikit-learn pour un classifieur"})]}),e.jsx(o,{children:e.jsx("pre",{className:"bg-slate-950 text-slate-50 p-4 rounded-md overflow-x-auto text-sm",children:e.jsx("code",{children:`from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Diviser les donnÃ©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# CrÃ©er et entraÃ®ner le modÃ¨le
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# PrÃ©dictions
y_pred = model.predict(X_test)

# Ã‰valuation
accuracy = accuracy_score(y_test, y_pred)
print(f"PrÃ©cision: {accuracy:.2f}")`})})})]})]})}),e.jsx(g,{value:"regression",className:"space-y-6",children:e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-8",children:[e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:e.jsx(r,{definition:i.regression,children:"RÃ©gression"})}),e.jsx("p",{className:"mb-4",children:"La rÃ©gression consiste Ã  prÃ©dire une valeur numÃ©rique continue, comme un prix, une tempÃ©rature ou un pourcentage."}),e.jsx("h4",{className:"font-medium mt-4 mb-2",children:"Algorithmes populaires :"}),e.jsxs("ul",{className:"list-disc pl-6 space-y-1",children:[e.jsx("li",{children:"RÃ©gression linÃ©aire"}),e.jsx("li",{children:"RÃ©gression polynomiale"}),e.jsx("li",{children:"RÃ©gression par arbres de dÃ©cision"}),e.jsx("li",{children:"ForÃªts alÃ©atoires pour la rÃ©gression"}),e.jsx("li",{children:"RÃ©seaux de neurones"}),e.jsx("li",{children:"Gradient Boosting (XGBoost, LightGBM)"})]})]}),e.jsxs(t,{children:[e.jsxs(n,{children:[e.jsx(l,{children:"Exemple de code : RÃ©gression"}),e.jsx(f,{children:"Utilisation de Scikit-learn pour un modÃ¨le de rÃ©gression"})]}),e.jsx(o,{children:e.jsx("pre",{className:"bg-slate-950 text-slate-50 p-4 rounded-md overflow-x-auto text-sm",children:e.jsx("code",{children:`from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Diviser les donnÃ©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# CrÃ©er et entraÃ®ner le modÃ¨le
model = LinearRegression()
model.fit(X_train, y_train)

# PrÃ©dictions
y_pred = model.predict(X_test)

# Ã‰valuation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.2f}")`})})})]})]})})]}),e.jsxs("div",{className:"mt-10 mb-6",children:[e.jsx("h3",{className:"text-xl font-semibold mb-6",children:"Techniques de division des donnÃ©es"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6",children:[e.jsxs(t,{children:[e.jsx(n,{children:e.jsx(l,{children:"Train-Test Split"})}),e.jsxs(o,{children:[e.jsx("p",{className:"text-sm",children:"Division simple des donnÃ©es en ensemble d'entraÃ®nement (gÃ©nÃ©ralement 70-80%) et ensemble de test (20-30%)."}),e.jsxs("div",{className:"mt-4 flex items-center gap-2",children:[e.jsx("div",{className:"bg-blue-500 h-4 w-8/12 rounded-sm"}),e.jsx("div",{className:"bg-green-500 h-4 w-4/12 rounded-sm"})]}),e.jsxs("div",{className:"flex justify-between text-xs mt-1",children:[e.jsx("span",{children:"EntraÃ®nement"}),e.jsx("span",{children:"Test"})]})]})]}),e.jsxs(t,{children:[e.jsx(n,{children:e.jsx(l,{children:"Validation croisÃ©e"})}),e.jsxs(o,{children:[e.jsx("p",{className:"text-sm",children:"Division des donnÃ©es en k sous-ensembles (folds), avec entraÃ®nement sur k-1 folds et test sur le fold restant, en alternant."}),e.jsxs("div",{className:"mt-4 space-y-2",children:[e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("div",{className:"bg-green-500 h-3 w-2/12 rounded-sm"}),e.jsx("div",{className:"bg-blue-500 h-3 w-10/12 rounded-sm"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("div",{className:"bg-blue-500 h-3 w-2/12 rounded-sm"}),e.jsx("div",{className:"bg-green-500 h-3 w-2/12 rounded-sm"}),e.jsx("div",{className:"bg-blue-500 h-3 w-8/12 rounded-sm"})]}),e.jsxs("div",{className:"flex items-center gap-1",children:[e.jsx("div",{className:"bg-blue-500 h-3 w-4/12 rounded-sm"}),e.jsx("div",{className:"bg-green-500 h-3 w-2/12 rounded-sm"}),e.jsx("div",{className:"bg-blue-500 h-3 w-6/12 rounded-sm"})]})]})]})]}),e.jsxs(t,{children:[e.jsx(n,{children:e.jsx(l,{children:"Train-Val-Test"})}),e.jsxs(o,{children:[e.jsx("p",{className:"text-sm",children:"Division en trois ensembles: entraÃ®nement, validation pour le rÃ©glage des hyperparamÃ¨tres, et test pour l'Ã©valuation finale."}),e.jsxs("div",{className:"mt-4 flex items-center gap-2",children:[e.jsx("div",{className:"bg-blue-500 h-4 w-6/12 rounded-sm"}),e.jsx("div",{className:"bg-purple-500 h-4 w-2/12 rounded-sm"}),e.jsx("div",{className:"bg-green-500 h-4 w-4/12 rounded-sm"})]}),e.jsxs("div",{className:"flex justify-between text-xs mt-1",children:[e.jsx("span",{children:"Train"}),e.jsx("span",{children:"Val"}),e.jsx("span",{children:"Test"})]})]})]})]})]})]}),be=()=>e.jsxs("section",{id:"unsupervised",className:"space-y-8",children:[e.jsx("h2",{className:"text-3xl font-bold mb-6",children:"Apprentissage Non SupervisÃ©"}),e.jsxs("p",{className:"text-lg mb-6",children:["L'",e.jsx(r,{definition:i["apprentissage-non-supervise"],children:"apprentissage non supervisÃ©"})," traite des donnÃ©es non Ã©tiquetÃ©es pour dÃ©couvrir des structures cachÃ©es, des patterns et des relations sans guidance externe."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-8 mb-10",children:[e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:e.jsx(r,{definition:i.clustering,children:"Clustering"})}),e.jsx("p",{className:"mb-4",children:"Le clustering regroupe des instances similaires en clusters, oÃ¹ les objets au sein d'un mÃªme groupe sont plus similaires entre eux qu'avec ceux des autres groupes."}),e.jsx("h4",{className:"font-medium mt-4 mb-2",children:"Algorithmes populaires :"}),e.jsxs("ul",{className:"list-disc pl-6 space-y-1",children:[e.jsx("li",{children:e.jsx(r,{definition:i["k-means"],children:"K-means"})}),e.jsx("li",{children:e.jsx(r,{definition:i.dbscan,children:"DBSCAN"})}),e.jsx("li",{children:"Hierarchical Clustering"}),e.jsx("li",{children:"Mean Shift"}),e.jsx("li",{children:"Expectation-Maximization (EM)"})]}),e.jsx("h3",{className:"text-xl font-semibold mt-6 mb-4",children:e.jsx(r,{definition:i["reduction-dimensionnalite"],children:"RÃ©duction de dimensionnalitÃ©"})}),e.jsx("p",{className:"mb-4",children:"Ces techniques rÃ©duisent le nombre de variables alÃ©atoires Ã  considÃ©rer, en trouvant un ensemble de variables principales qui prÃ©servent la plupart des informations."}),e.jsx("h4",{className:"font-medium mt-4 mb-2",children:"Algorithmes populaires :"}),e.jsxs("ul",{className:"list-disc pl-6 space-y-1",children:[e.jsx("li",{children:e.jsx(r,{definition:i.pca,children:"Analyse en Composantes Principales (PCA)"})}),e.jsx("li",{children:e.jsx(r,{definition:i["t-sne"],children:"t-SNE"})}),e.jsx("li",{children:"UMAP"}),e.jsx("li",{children:"Autoencodeurs"})]})]}),e.jsxs(t,{children:[e.jsxs(n,{children:[e.jsx(l,{children:"Exemple de code : K-means Clustering"}),e.jsx(f,{children:"ImplÃ©mentation avec Scikit-learn"})]}),e.jsx(o,{children:e.jsx("pre",{className:"bg-slate-950 text-slate-50 p-4 rounded-md overflow-x-auto text-sm",children:e.jsx("code",{children:`from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Standardisation des donnÃ©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DÃ©terminer le nombre optimal de clusters
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# MÃ©thode du coude pour choisir k
plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Nombre de clusters')
plt.ylabel('Inertie')
plt.title('MÃ©thode du coude')
plt.show()

# Application du K-means avec k optimal
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Visualisation des clusters
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=300, c='red', marker='X')
plt.title('K-means Clustering')
plt.show()`})})})]})]}),e.jsxs(t,{className:"mb-10",children:[e.jsxs(n,{children:[e.jsx(l,{children:"Applications de l'apprentissage non supervisÃ©"}),e.jsx(f,{children:"Cas d'utilisation courants en data science"})]}),e.jsx(o,{children:e.jsxs("div",{className:"grid grid-cols-1 sm:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"Segmentation client"}),e.jsx("p",{className:"text-sm",children:"Regroupement des clients en fonction de leur comportement d'achat, prÃ©fÃ©rences, et dÃ©mographie."})]}),e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"DÃ©tection d'anomalies"}),e.jsx("p",{className:"text-sm",children:"Identification des transactions frauduleuses, pannes d'Ã©quipement, ou comportements inhabituels."})]}),e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"SystÃ¨mes de recommandation"}),e.jsx("p",{className:"text-sm",children:"Recommandation de produits, films, ou contenus basÃ©e sur des patterns de similaritÃ©."})]}),e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"Compression d'image"}),e.jsx("p",{className:"text-sm",children:"RÃ©duction de la dimensionnalitÃ© des images tout en prÃ©servant l'information principale."})]}),e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"Analyse de marchÃ©"}),e.jsx("p",{className:"text-sm",children:"DÃ©couverte de segments de marchÃ© et positionnement des produits."})]}),e.jsxs("div",{className:"border p-4 rounded-md",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"Visualisation de donnÃ©es complexes"}),e.jsx("p",{className:"text-sm",children:"Projection de donnÃ©es de haute dimension en 2D ou 3D pour faciliter l'interprÃ©tation."})]})]})})]})]}),z=[{name:"True Positive",value:120},{name:"False Positive",value:15},{name:"False Negative",value:20},{name:"True Negative",value:140}],U=["#0088FE","#FF8042","#FFBB28","#00C49F"],ve=[{samples:10,trainingError:.8,validationError:.9},{samples:50,trainingError:.6,validationError:.7},{samples:100,trainingError:.4,validationError:.5},{samples:200,trainingError:.25,validationError:.4},{samples:500,trainingError:.15,validationError:.35},{samples:1e3,trainingError:.1,validationError:.3}],je=()=>e.jsxs("section",{id:"evaluation",className:"space-y-8",children:[e.jsx("h2",{className:"text-3xl font-bold mb-6",children:"Ã‰valuation des ModÃ¨les"}),e.jsxs("p",{className:"text-lg mb-8",children:["L'",e.jsx(r,{definition:i["evaluation-modele"],children:"Ã©valuation"})," permet de mesurer les performances d'un modÃ¨le de machine learning et de comparer diffÃ©rentes approches pour sÃ©lectionner la plus appropriÃ©e."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-8 mb-10",children:[e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:"MÃ©triques pour la classification"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"PrÃ©cision (Accuracy)"}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Proportion de prÃ©dictions correctes parmi toutes les prÃ©dictions"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"Accuracy = (TP + TN) / (TP + TN + FP + FN)"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.precision,children:"PrÃ©cision (Precision)"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Proportion des positifs identifiÃ©s qui sont rÃ©ellement positifs"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"Precision = TP / (TP + FP)"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.rappel,children:"Rappel (Recall)"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Proportion des positifs rÃ©els qui ont Ã©tÃ© correctement identifiÃ©s"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"Recall = TP / (TP + FN)"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i["f1-score"],children:"F1-Score"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Moyenne harmonique de la prÃ©cision et du rappel"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"F1 = 2 * (Precision * Recall) / (Precision + Recall)"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i["auc-roc"],children:"AUC-ROC"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Aire sous la courbe ROC, mesure la capacitÃ© du modÃ¨le Ã  distinguer les classes"})]})]})]}),e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:"MÃ©triques pour la rÃ©gression"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.mse,children:"MSE (Mean Squared Error)"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Moyenne des carrÃ©s des erreurs"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"MSE = (1/n) * Î£(y_i - Å·_i)Â²"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.rmse,children:"RMSE (Root Mean Squared Error)"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Racine carrÃ©e de MSE"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"RMSE = âˆšMSE"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"MAE (Mean Absolute Error)"}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Moyenne des valeurs absolues des erreurs"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"MAE = (1/n) * Î£|y_i - Å·_i|"})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i["r-carre"],children:"RÂ² (Coefficient de dÃ©termination)"})}),e.jsx("p",{className:"text-sm text-muted-foreground",children:"Proportion de la variance expliquÃ©e par le modÃ¨le"}),e.jsx("p",{className:"text-sm font-mono mt-1",children:"RÂ² = 1 - (Î£(y_i - Å·_i)Â² / Î£(y_i - È³)Â²)"})]})]}),e.jsxs("div",{className:"mt-8 mb-4",children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:e.jsx(r,{definition:i["matrice-confusion"],children:"Matrice de confusion"})}),e.jsx("p",{className:"mb-4 text-sm",children:"Tableau qui montre les prÃ©dictions correctes et incorrectes pour chaque classe."})]})]})]}),e.jsxs("div",{className:"mb-10 mt-10 p-6 bg-slate-50 dark:bg-slate-900 rounded-xl",children:[e.jsx("h3",{className:"text-xl font-semibold mb-6",children:"Visualisation : Matrice de confusion"}),e.jsx("div",{className:"flex justify-center mb-8 mt-2",children:e.jsx(P,{width:"100%",height:300,children:e.jsxs(ae,{data:z,children:[e.jsx(w,{strokeDasharray:"3 3"}),e.jsx(L,{dataKey:"name"}),e.jsx(k,{}),e.jsx(F,{}),e.jsx(I,{}),e.jsxs(re,{dataKey:"value",fill:"#8884d8",children:["// VÃ©rifier et garder seulement les imports utilisÃ©s de recharts",z.map((s,a)=>e.jsx(ie,{fill:U[a%U.length]},`cell-${a}`))]})]})})})]}),e.jsxs("div",{className:"mb-10 p-6 bg-slate-50 dark:bg-slate-900 rounded-xl",children:[e.jsx("h3",{className:"text-xl font-semibold mb-6",children:"Courbe d'apprentissage"}),e.jsx("p",{className:"mb-4",children:"Les courbes d'apprentissage montrent comment l'erreur d'entraÃ®nement et de validation Ã©voluent en fonction du nombre d'Ã©chantillons d'apprentissage."}),e.jsx("div",{className:"flex justify-center mt-6",children:e.jsx(P,{width:"100%",height:300,children:e.jsxs(te,{data:ve,children:[e.jsx(w,{strokeDasharray:"3 3"}),e.jsx(L,{dataKey:"samples",label:{value:"Nombre d'Ã©chantillons",position:"insideBottom",offset:-5}}),e.jsx(k,{label:{value:"Erreur",angle:-90,position:"insideLeft"}}),e.jsx(F,{}),e.jsx(I,{}),e.jsx(O,{type:"monotone",dataKey:"trainingError",name:"Erreur d'entraÃ®nement",stroke:"#8884d8",activeDot:{r:8}}),e.jsx(O,{type:"monotone",dataKey:"validationError",name:"Erreur de validation",stroke:"#82ca9d"})]})})}),e.jsxs("div",{className:"mt-4 text-sm",children:[e.jsx("p",{className:"font-medium",children:"InterprÃ©tation :"}),e.jsxs("ul",{className:"list-disc pl-6 mt-2",children:[e.jsxs("li",{children:[e.jsx("span",{className:"font-medium",children:e.jsx(r,{definition:i.underfitting,highlightStyle:"dotted",children:"High Bias (sous-apprentissage)"})})," : Erreurs d'entraÃ®nement et de validation Ã©levÃ©es"]}),e.jsxs("li",{children:[e.jsx("span",{className:"font-medium",children:e.jsx(r,{definition:i.overfitting,highlightStyle:"dotted",children:"High Variance (sur-apprentissage)"})})," : Erreur d'entraÃ®nement faible mais erreur de validation Ã©levÃ©e"]}),e.jsxs("li",{children:[e.jsx("span",{className:"font-medium",children:"Bon Ã©quilibre"})," : Erreurs d'entraÃ®nement et de validation proches et relativement faibles"]})]})]})]})]}),Ne=()=>e.jsxs("section",{id:"deep-learning",className:"space-y-8",children:[e.jsx("h2",{className:"text-3xl font-bold mb-6",children:"Deep Learning"}),e.jsxs("p",{className:"text-lg mb-6",children:["Le ",e.jsx(r,{definition:i["deep-learning"],children:"Deep Learning"})," est un sous-ensemble du Machine Learning qui utilise des rÃ©seaux de neurones artificiels Ã  plusieurs couches pour apprendre des reprÃ©sentations hiÃ©rarchiques des donnÃ©es."]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-8 mb-10",children:[e.jsxs("div",{children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:"Concepts fondamentaux"}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"RÃ©seaux de neurones"}),e.jsx("p",{className:"text-sm",children:"SystÃ¨mes inspirÃ©s du cerveau humain, composÃ©s de neurones artificiels organisÃ©s en couches qui transforment les donnÃ©es d'entrÃ©e en sorties prÃ©dictives."})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Fonctions d'activation"}),e.jsx("p",{className:"text-sm",children:"Fonctions non linÃ©aires (ReLU, Sigmoid, Tanh) qui dÃ©terminent si un neurone doit Ãªtre activÃ©."})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"RÃ©tropropagation"}),e.jsx("p",{className:"text-sm",children:"Algorithme permettant d'ajuster les poids du rÃ©seau en propageant l'erreur de la sortie vers l'entrÃ©e."})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Descente de gradient"}),e.jsx("p",{className:"text-sm",children:"MÃ©thode d'optimisation qui ajuste itÃ©rativement les paramÃ¨tres pour minimiser la fonction de coÃ»t."})]})]}),e.jsx("h3",{className:"text-xl font-semibold mt-6 mb-4",children:"Architectures de Deep Learning"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.cnn,children:"RÃ©seaux de neurones convolutifs (CNN)"})}),e.jsx("p",{className:"text-sm",children:"SpÃ©cialisÃ©s pour traiter les donnÃ©es avec une topologie en grille comme les images."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.rnn,children:"RÃ©seaux de neurones rÃ©currents (RNN)"})}),e.jsx("p",{className:"text-sm",children:"AdaptÃ©s aux sÃ©quences comme le texte ou les sÃ©ries temporelles."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.lstm,children:"RÃ©seaux de neurones Ã  mÃ©moire (LSTM, GRU)"})}),e.jsx("p",{className:"text-sm",children:"Types de RNN conÃ§us pour mÃ©moriser les dÃ©pendances Ã  long terme."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.transformers,children:"Transformers"})}),e.jsx("p",{className:"text-sm",children:"Architecture basÃ©e sur le mÃ©canisme d'attention, excellant dans les tÃ¢ches de NLP."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Autoencodeurs"}),e.jsx("p",{className:"text-sm",children:"RÃ©seaux non supervisÃ©s pour la rÃ©duction de dimension et la gÃ©nÃ©ration de donnÃ©es."})]})]})]}),e.jsxs("div",{children:[e.jsxs(t,{children:[e.jsxs(n,{children:[e.jsx(l,{children:"Exemple de code : RÃ©seau de neurones simple"}),e.jsx(f,{children:"ImplÃ©mentation avec TensorFlow/Keras"})]}),e.jsx(o,{children:e.jsx("pre",{className:"bg-slate-950 text-slate-50 p-4 rounded-md overflow-x-auto text-sm",children:e.jsx("code",{children:`import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# PrÃ©traitement des donnÃ©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Construction du modÃ¨le
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # pour classification binaire
])

# Compilation du modÃ¨le
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# EntraÃ®nement
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Ã‰valuation
loss, accuracy = model.evaluate(X_test, y_test)
print(f"PrÃ©cision du test: {accuracy:.4f}")

# PrÃ©dictions
y_pred = model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)`})})})]}),e.jsxs("div",{className:"mt-8",children:[e.jsx("h3",{className:"text-xl font-semibold mb-4",children:"Applications du Deep Learning"}),e.jsxs("div",{className:"grid grid-cols-2 gap-3",children:[e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Vision par ordinateur"}),e.jsx("p",{className:"text-sm",children:"Reconnaissance d'images, dÃ©tection d'objets, segmentation, etc."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Traitement du langage naturel"}),e.jsx("p",{className:"text-sm",children:"Traduction, analyse de sentiment, gÃ©nÃ©ration de texte, etc."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"SynthÃ¨se vocale et reconnaissance"}),e.jsx("p",{className:"text-sm",children:"Text-to-speech, speech-to-text, identification de locuteurs."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"SystÃ¨mes de recommandation"}),e.jsx("p",{className:"text-sm",children:"Recommandations personnalisÃ©es basÃ©es sur les comportements."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"GÃ©nÃ©ration d'images"}),e.jsx("p",{className:"text-sm",children:"GANs, diffusion, text-to-image comme DALL-E et Stable Diffusion."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Aide Ã  la dÃ©cision mÃ©dicale"}),e.jsx("p",{className:"text-sm",children:"Diagnostic basÃ© sur les images mÃ©dicales, prÃ©diction de maladies."})]})]})]})]})]}),e.jsxs(t,{children:[e.jsx(n,{children:e.jsx(l,{children:"DÃ©fis et considÃ©rations du Deep Learning"})}),e.jsx(o,{children:e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Besoins en donnÃ©es"}),e.jsx("p",{className:"text-sm",children:"Les modÃ¨les de Deep Learning nÃ©cessitent gÃ©nÃ©ralement de grandes quantitÃ©s de donnÃ©es pour bien performer."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Puissance de calcul"}),e.jsx("p",{className:"text-sm",children:"L'entraÃ®nement de modÃ¨les complexes nÃ©cessite des GPUs ou TPUs puissants et peut Ãªtre coÃ»teux."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"InterprÃ©tabilitÃ©"}),e.jsx("p",{className:"text-sm",children:'Les modÃ¨les de Deep Learning sont souvent des "boÃ®tes noires" difficiles Ã  interprÃ©ter.'})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"HyperparamÃ¨tres"}),e.jsx("p",{className:"text-sm",children:"Le rÃ©glage des hyperparamÃ¨tres peut Ãªtre complexe et nÃ©cessite souvent de l'expÃ©rience."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:e.jsx(r,{definition:i.overfitting,children:"Surapprentissage"})}),e.jsx("p",{className:"text-sm",children:"Les modÃ¨les profonds peuvent facilement mÃ©moriser les donnÃ©es d'entraÃ®nement sans gÃ©nÃ©raliser."})]}),e.jsxs("div",{className:"border p-3 rounded-md",children:[e.jsx("p",{className:"font-medium",children:"Ã‰thique et biais"}),e.jsx("p",{className:"text-sm",children:"Les modÃ¨les peuvent perpÃ©tuer ou amplifier les biais prÃ©sents dans les donnÃ©es d'entraÃ®nement."})]})]})})]})]}),ye=()=>{const[s,a]=T.useState(0),[b,N]=T.useState(null),c=[{title:"Classification avec Scikit-learn",description:"Exemple complet de classification d'iris avec analyse dÃ©taillÃ©e",difficulty:"DÃ©butant",estimatedTime:"20 min",code:`# Classification des fleurs d'Iris - Exemple complet et commentÃ©
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

# 1. ðŸ“Š CHARGEMENT ET EXPLORATION DES DONNÃ‰ES
print("ðŸŒ¸ Analyse du dataset Iris")
print("=" * 50)

iris = load_iris()
X, y = iris.data, iris.target

# CrÃ©ation d'un DataFrame pour faciliter l'analyse
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"ðŸ“ˆ Forme du dataset: {df.shape}")
print(f"ðŸŽ¯ Classes: {iris.target_names}")
print(f"ðŸ“Š RÃ©partition des classes:")
print(df['species'].value_counts())

# Statistiques descriptives
print("\\nðŸ“Š Statistiques descriptives:")
print(df.describe())

# 2. ðŸŽ¨ VISUALISATION EXPLORATOIRE
plt.figure(figsize=(15, 10))

# Distribution des features
plt.subplot(2, 3, 1)
df[iris.feature_names].boxplot()
plt.title('ðŸ“Š Distribution des caractÃ©ristiques')
plt.xticks(rotation=45)

# Matrice de corrÃ©lation
plt.subplot(2, 3, 2)
correlation_matrix = df[iris.feature_names].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('ðŸ”— Matrice de corrÃ©lation')

# Pairplot pour visualiser les relations
plt.subplot(2, 3, 3)
# Note: en pratique, utilisez sns.pairplot(df, hue='species')
plt.scatter(df['sepal length (cm)'], df['petal length (cm)'], c=df['target'], cmap='viridis')
plt.xlabel('Longueur sÃ©pale (cm)')
plt.ylabel('Longueur pÃ©tale (cm)')
plt.title('ðŸŒº Relation longueur sÃ©pale vs pÃ©tale')

plt.tight_layout()
plt.show()

# 3. âš™ï¸ PRÃ‰PARATION DES DONNÃ‰ES
print("\\nâš™ï¸ PrÃ©paration des donnÃ©es")
print("=" * 30)

# Division stratifiÃ©e pour garder la proportion des classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"ðŸ“ˆ Taille entraÃ®nement: {X_train.shape[0]} Ã©chantillons")
print(f"ðŸ“Š Taille test: {X_test.shape[0]} Ã©chantillons")

# Standardisation des donnÃ©es (importante pour SVM et Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. ðŸ¤– COMPARAISON DE PLUSIEURS MODÃˆLES
print("\\nðŸ¤– Comparaison des modÃ¨les")
print("=" * 35)

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\\nðŸ”§ EntraÃ®nement {name}...")
    
    # Utiliser les donnÃ©es standardisÃ©es pour Logistic Regression et SVM
    if name in ['Logistic Regression', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        # Validation croisÃ©e
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    
    # MÃ©triques
    accuracy = accuracy_score(y_test, y_pred)
    
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"âœ… PrÃ©cision test: {accuracy:.3f}")
    print(f"ðŸ“Š CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# 5. ðŸ† SÃ‰LECTION DU MEILLEUR MODÃˆLE
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']

print(f"\\nðŸ† Meilleur modÃ¨le: {best_model_name}")
print(f"ðŸŽ¯ PrÃ©cision: {results[best_model_name]['accuracy']:.3f}")

# 6. ðŸ“Š ANALYSE DÃ‰TAILLÃ‰E DU MEILLEUR MODÃˆLE
print(f"\\nðŸ“Š Analyse dÃ©taillÃ©e - {best_model_name}")
print("=" * 45)

# Rapport de classification dÃ©taillÃ©
y_pred_best = results[best_model_name]['predictions']
print("\\nðŸ“‹ Rapport de classification:")
print(classification_report(y_test, y_pred_best, target_names=iris.target_names))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred_best)
print("\\nðŸŽ¯ Matrice de confusion:")
print(cm)

# Visualisation de la matrice de confusion
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=iris.target_names, 
            yticklabels=iris.target_names)
plt.title(f'Matrice de confusion - {best_model_name}')
plt.ylabel('Vraie classe')
plt.xlabel('Classe prÃ©dite')
plt.show()

# 7. ðŸ”® PRÃ‰DICTIONS SUR DE NOUVEAUX EXEMPLES
print("\\nðŸ”® Test sur de nouveaux exemples")
print("=" * 40)

# Exemples de nouvelles fleurs (features: sepal_length, sepal_width, petal_length, petal_width)
nouvelles_fleurs = [
    [5.1, 3.5, 1.4, 0.2],  # Ressemble Ã  setosa
    [6.2, 2.9, 4.3, 1.3],  # Ressemble Ã  versicolor  
    [7.3, 2.9, 6.3, 1.8]   # Ressemble Ã  virginica
]

for i, fleur in enumerate(nouvelles_fleurs):
    if best_model_name in ['Logistic Regression', 'SVM']:
        fleur_scaled = scaler.transform([fleur])
        prediction = best_model.predict(fleur_scaled)[0]
        probabilities = best_model.predict_proba(fleur_scaled)[0] if hasattr(best_model, 'predict_proba') else None
    else:
        prediction = best_model.predict([fleur])[0]
        probabilities = best_model.predict_proba([fleur])[0]
    
    espece_predite = iris.target_names[prediction]
    
    print(f"\\nðŸŒ¸ Fleur {i+1}: {fleur}")
    print(f"ðŸŽ¯ PrÃ©diction: {espece_predite}")
    
    if probabilities is not None:
        print("ðŸ“Š ProbabilitÃ©s:")
        for j, (espece, prob) in enumerate(zip(iris.target_names, probabilities)):
            print(f"   {espece}: {prob:.3f} ({prob*100:.1f}%)")

# 8. ðŸ’¡ IMPORTANCE DES FEATURES (si Random Forest est le meilleur)
if best_model_name == 'Random Forest':
    print("\\nðŸ’¡ Importance des caractÃ©ristiques")
    print("=" * 40)
    
    feature_importance = pd.DataFrame({
        'feature': iris.feature_names,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance)
    
    # Visualisation
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.title('ðŸŽ¯ Importance des caractÃ©ristiques')
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.show()

print("\\nâœ… Analyse terminÃ©e ! Votre modÃ¨le est prÃªt Ã  classifier de nouvelles fleurs ðŸŒ¸")`,language:"python",outputs:["PrÃ©cision finale: 96.7%","Meilleur modÃ¨le: Random Forest","Features importantes: petal length > petal width > sepal length"]},{title:"RÃ©gression avec visualisation avancÃ©e",description:"PrÃ©diction de prix immobiliers avec analyse complÃ¨te et visualisations",difficulty:"IntermÃ©diaire",estimatedTime:"35 min",code:`# RÃ©gression pour prÃ©dire les prix immobiliers - Analyse complÃ¨te
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Configuration pour de beaux graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("ðŸ  PRÃ‰DICTION DE PRIX IMMOBILIERS")
print("=" * 50)

# 1. ðŸŽ² GÃ‰NÃ‰RATION DE DONNÃ‰ES RÃ‰ALISTES
print("\\nðŸ“Š GÃ©nÃ©ration du dataset...")
np.random.seed(42)
n_samples = 2000

# Variables explicatives avec corrÃ©lations rÃ©alistes
surface = np.random.normal(120, 40, n_samples)
surface = np.clip(surface, 30, 300)  # Limiter les valeurs aberrantes

chambres = np.random.poisson(3, n_samples)
chambres = np.clip(chambres, 1, 8)

# Ã‚ge avec distribution plus rÃ©aliste
age = np.random.exponential(15, n_samples)
age = np.clip(age, 0, 100)

# Localisation (score de 1 Ã  10)
localisation = np.random.choice([1,2,3,4,5,6,7,8,9,10], n_samples, 
                               p=[0.05,0.08,0.12,0.15,0.2,0.15,0.12,0.08,0.03,0.02])

# Ã‰tage (0 = RDC, puis Ã©tages)
etage = np.random.choice(range(0, 20), n_samples, 
                        p=[0.3] + [0.7/19]*19)

# Garage (binaire)
garage = np.random.choice([0, 1], n_samples, p=[0.4, 0.6])

# Formule rÃ©aliste pour le prix avec interactions
prix_base = (
    surface * 2500 +                    # 2500â‚¬/mÂ²
    chambres * 12000 +                  # Bonus par chambre
    localisation * 8000 +               # Impact localisation
    garage * 15000 +                    # Bonus garage
    (etage > 0) * 5000 +               # Bonus Ã©tage
    -age * 800                          # DÃ©prÃ©ciation
)

# Ajout d'interactions et de non-linÃ©aritÃ©s
prix_interactions = (
    surface * localisation * 30 +       # Interaction surface-localisation
    (surface > 150) * 20000 +           # Bonus grandes surfaces
    (age > 50) * -15000 +               # Malus vieilles constructions
    np.random.normal(0, 25000, n_samples)  # Bruit rÃ©aliste
)

prix = prix_base + prix_interactions
prix = np.clip(prix, 50000, 800000)  # Prix rÃ©alistes

# CrÃ©ation du DataFrame
df = pd.DataFrame({
    'surface': surface,
    'chambres': chambres,
    'age': age,
    'localisation': localisation,
    'etage': etage,
    'garage': garage,
    'prix': prix
})

print(f"âœ… Dataset crÃ©Ã©: {df.shape[0]} biens immobiliers")
print(f"ðŸ’° Prix moyen: {df['prix'].mean():,.0f}â‚¬")
print(f"ðŸ“Š Prix mÃ©dian: {df['prix'].median():,.0f}â‚¬")

# 2. ðŸ” ANALYSE EXPLORATOIRE APPROFONDIE
print("\\nðŸ” Analyse exploratoire des donnÃ©es")
print("=" * 40)

# Statistiques descriptives
print("ðŸ“Š Statistiques descriptives:")
print(df.describe().round(2))

# VÃ©rification des valeurs manquantes
print(f"\\nâŒ Valeurs manquantes: {df.isnull().sum().sum()}")

# CrÃ©ation de nouvelles features
df['prix_par_m2'] = df['prix'] / df['surface']
df['surface_par_chambre'] = df['surface'] / df['chambres']

# Visualisations complÃ¨tes
fig, axes = plt.subplots(3, 3, figsize=(20, 18))
fig.suptitle('ðŸ  Analyse Exploratoire ComplÃ¨te du MarchÃ© Immobilier', fontsize=16, fontweight='bold')

# Distribution des prix
axes[0,0].hist(df['prix'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
axes[0,0].set_title('ðŸ“Š Distribution des Prix')
axes[0,0].set_xlabel('Prix (â‚¬)')
axes[0,0].set_ylabel('FrÃ©quence')

# Prix vs Surface avec rÃ©gression
axes[0,1].scatter(df['surface'], df['prix'], alpha=0.6, color='coral')
z = np.polyfit(df['surface'], df['prix'], 1)
p = np.poly1d(z)
axes[0,1].plot(df['surface'], p(df['surface']), "r--", alpha=0.8, linewidth=2)
axes[0,1].set_title('ðŸ  Prix vs Surface')
axes[0,1].set_xlabel('Surface (mÂ²)')
axes[0,1].set_ylabel('Prix (â‚¬)')

# Prix par nombre de chambres
df.boxplot(column='prix', by='chambres', ax=axes[0,2])
axes[0,2].set_title('ðŸ’° Prix par Nombre de Chambres')
axes[0,2].set_xlabel('Nombre de chambres')

# Impact de l'Ã¢ge
axes[1,0].scatter(df['age'], df['prix'], alpha=0.6, color='green')
axes[1,0].set_title('ðŸ“… Impact de l\\'Ã‚ge sur le Prix')
axes[1,0].set_xlabel('Ã‚ge (annÃ©es)')
axes[1,0].set_ylabel('Prix (â‚¬)')

# Impact localisation
prix_par_localisation = df.groupby('localisation')['prix'].mean()
axes[1,1].bar(prix_par_localisation.index, prix_par_localisation.values, color='purple', alpha=0.7)
axes[1,1].set_title('ðŸŒ Prix Moyen par Score de Localisation')
axes[1,1].set_xlabel('Score Localisation')
axes[1,1].set_ylabel('Prix Moyen (â‚¬)')

# Matrice de corrÃ©lation
correlation_matrix = df.corr()
im = axes[1,2].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
axes[1,2].set_xticks(range(len(correlation_matrix.columns)))
axes[1,2].set_yticks(range(len(correlation_matrix.columns)))
axes[1,2].set_xticklabels(correlation_matrix.columns, rotation=45)
axes[1,2].set_yticklabels(correlation_matrix.columns)
axes[1,2].set_title('ðŸ”— Matrice de CorrÃ©lation')

# Ajout des valeurs dans la matrice
for i in range(len(correlation_matrix.columns)):
    for j in range(len(correlation_matrix.columns)):
        axes[1,2].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                      ha='center', va='center', fontsize=8)

# Prix par mÂ² selon localisation
axes[2,0].scatter(df['localisation'], df['prix_par_m2'], alpha=0.6, color='orange')
axes[2,0].set_title('ðŸ’Ž Prix au mÂ² selon Localisation')
axes[2,0].set_xlabel('Score Localisation')
axes[2,0].set_ylabel('Prix/mÂ² (â‚¬)')

# Impact garage
prix_avec_sans_garage = df.groupby('garage')['prix'].mean()
axes[2,1].bar(['Sans garage', 'Avec garage'], prix_avec_sans_garage.values, 
             color=['lightcoral', 'lightgreen'], alpha=0.8)
axes[2,1].set_title('ðŸš— Impact du Garage')
axes[2,1].set_ylabel('Prix Moyen (â‚¬)')

# Relation surface/chambres
axes[2,2].scatter(df['surface_par_chambre'], df['prix'], alpha=0.6, color='teal')
axes[2,2].set_title('ðŸ“ Prix vs Surface par Chambre')
axes[2,2].set_xlabel('Surface par chambre (mÂ²)')
axes[2,2].set_ylabel('Prix (â‚¬)')

plt.tight_layout()
plt.show()

# 3. âš™ï¸ PRÃ‰PARATION AVANCÃ‰E DES DONNÃ‰ES
print("\\nâš™ï¸ PrÃ©paration avancÃ©e des donnÃ©es")
print("=" * 40)

# Features et target
features_base = ['surface', 'chambres', 'age', 'localisation', 'etage', 'garage']
X = df[features_base].copy()
y = df['prix'].copy()

# CrÃ©ation de features polynomiales pour capturer les non-linÃ©aritÃ©s
poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_poly = poly_features.fit_transform(X)

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train_poly, X_test_poly, _, _ = train_test_split(
    X_poly, y, test_size=0.2, random_state=42
)

print(f"ðŸ“ˆ Features originales: {X.shape[1]}")
print(f"ðŸ”„ Features polynomiales: {X_poly.shape[1]}")
print(f"ðŸ“Š Taille train: {X_train.shape[0]}")
print(f"ðŸŽ¯ Taille test: {X_test.shape[0]}")

# 4. ðŸ¤– COMPARAISON DE MODÃˆLES AVANCÃ‰S
print("\\nðŸ¤– EntraÃ®nement et comparaison des modÃ¨les")
print("=" * 50)

# DÃ©finition des modÃ¨les avec pipelines
models = {
    'Linear Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', LinearRegression())
    ]),
    'Ridge Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Ridge(alpha=1.0))
    ]),
    'Lasso Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Lasso(alpha=100))
    ]),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Polynomial Ridge': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Ridge(alpha=10))
    ])
}

results = {}

for name, model in models.items():
    print(f"\\nðŸ”§ EntraÃ®nement {name}...")
    
    # Utiliser features polynomiales pour le modÃ¨le polynomial
    if name == 'Polynomial Ridge':
        model.fit(X_train_poly, y_train)
        y_pred = model.predict(X_test_poly)
        cv_scores = cross_val_score(model, X_train_poly, y_train, cv=5, scoring='r2')
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # Calcul des mÃ©triques
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {
        'model': model,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"âœ… RÂ² Score: {r2:.3f}")
    print(f"ðŸ“Š RMSE: {rmse:,.0f}â‚¬")
    print(f"ðŸ“ˆ MAE: {mae:,.0f}â‚¬")
    print(f"ðŸŽ¯ CV RÂ²: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# 5. ðŸ† ANALYSE DU MEILLEUR MODÃˆLE
best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])
best_model = results[best_model_name]['model']
best_pred = results[best_model_name]['predictions']

print(f"\\nðŸ† MEILLEUR MODÃˆLE: {best_model_name}")
print("=" * 50)
print(f"ðŸŽ¯ RÂ² Score: {results[best_model_name]['r2']:.3f}")
print(f"ðŸ“Š RMSE: {results[best_model_name]['rmse']:,.0f}â‚¬")
print(f"ðŸ“ˆ MAE: {results[best_model_name]['mae']:,.0f}â‚¬")

# 6. ðŸ“Š VISUALISATIONS AVANCÃ‰ES
print("\\nðŸ“Š CrÃ©ation des visualisations avancÃ©es...")

fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle(f'ðŸ“ˆ Analyse ComplÃ¨te du ModÃ¨le {best_model_name}', fontsize=16, fontweight='bold')

# PrÃ©dictions vs RÃ©alitÃ©
axes[0,0].scatter(y_test, best_pred, alpha=0.6, color='blue')
min_val, max_val = min(y_test.min(), best_pred.min()), max(y_test.max(), best_pred.max())
axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
axes[0,0].set_xlabel('Prix RÃ©els (â‚¬)')
axes[0,0].set_ylabel('Prix PrÃ©dits (â‚¬)')
axes[0,0].set_title('ðŸŽ¯ PrÃ©dictions vs RÃ©alitÃ©')

# RÃ©sidus
residus = y_test - best_pred
axes[0,1].scatter(best_pred, residus, alpha=0.6, color='green')
axes[0,1].axhline(y=0, color='r', linestyle='--')
axes[0,1].set_xlabel('Prix PrÃ©dits (â‚¬)')
axes[0,1].set_ylabel('RÃ©sidus (â‚¬)')
axes[0,1].set_title('ðŸ“‰ Analyse des RÃ©sidus')

# Distribution des rÃ©sidus
axes[0,2].hist(residus, bins=30, alpha=0.7, color='orange', edgecolor='black')
axes[0,2].set_xlabel('RÃ©sidus (â‚¬)')
axes[0,2].set_ylabel('FrÃ©quence')
axes[0,2].set_title('ðŸ“Š Distribution des RÃ©sidus')

# Comparaison des modÃ¨les
model_names = list(results.keys())
r2_scores = [results[name]['r2'] for name in model_names]
rmse_scores = [results[name]['rmse'] for name in model_names]

axes[1,0].barh(model_names, r2_scores, color='lightblue', alpha=0.8)
axes[1,0].set_xlabel('RÂ² Score')
axes[1,0].set_title('ðŸ† Comparaison RÂ² des ModÃ¨les')

axes[1,1].barh(model_names, rmse_scores, color='lightcoral', alpha=0.8)
axes[1,1].set_xlabel('RMSE (â‚¬)')
axes[1,1].set_title('ðŸ“Š Comparaison RMSE des ModÃ¨les')

# Erreur absolue par gamme de prix
price_ranges = pd.cut(y_test, bins=5, labels=['TrÃ¨s bas', 'Bas', 'Moyen', 'Haut', 'TrÃ¨s haut'])
error_by_range = pd.DataFrame({'range': price_ranges, 'error': np.abs(residus)}).groupby('range')['error'].mean()

axes[1,2].bar(error_by_range.index, error_by_range.values, color='purple', alpha=0.7)
axes[1,2].set_xlabel('Gamme de Prix')
axes[1,2].set_ylabel('Erreur Absolue Moyenne (â‚¬)')
axes[1,2].set_title('ðŸ“ˆ Erreur par Gamme de Prix')
axes[1,2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 7. ðŸ”® PRÃ‰DICTIONS SUR DE NOUVEAUX BIENS
print("\\nðŸ”® Test sur de nouveaux biens immobiliers")
print("=" * 50)

nouveaux_biens = [
    {'surface': 80, 'chambres': 3, 'age': 5, 'localisation': 8, 'etage': 2, 'garage': 1},
    {'surface': 120, 'chambres': 4, 'age': 15, 'localisation': 6, 'etage': 0, 'garage': 1},
    {'surface': 200, 'chambres': 6, 'age': 30, 'localisation': 9, 'etage': 5, 'garage': 0}
]

for i, bien in enumerate(nouveaux_biens):
    bien_df = pd.DataFrame([bien])
    
    if best_model_name == 'Polynomial Ridge':
        bien_poly = poly_features.transform(bien_df)
        prix_predit = best_model.predict(bien_poly)[0]
    else:
        prix_predit = best_model.predict(bien_df)[0]
    
    print(f"\\nðŸ  Bien {i+1}:")
    print(f"   ðŸ“ Surface: {bien['surface']}mÂ²")
    print(f"   ðŸ›ï¸  Chambres: {bien['chambres']}")
    print(f"   ðŸ“… Ã‚ge: {bien['age']} ans")
    print(f"   ðŸŒ Localisation: {bien['localisation']}/10")
    print(f"   ðŸ¢ Ã‰tage: {bien['etage']}")
    print(f"   ðŸš— Garage: {'Oui' if bien['garage'] else 'Non'}")
    print(f"   ðŸ’° Prix prÃ©dit: {prix_predit:,.0f}â‚¬")
    print(f"   ðŸ’Ž Prix/mÂ²: {prix_predit/bien['surface']:,.0f}â‚¬/mÂ²")

print("\\nâœ… Analyse complÃ¨te terminÃ©e !")
print("ðŸŽ¯ Votre modÃ¨le peut maintenant estimer des prix immobiliers avec prÃ©cision!")`,language:"python",outputs:["RÂ² Score: 0.892","RMSE: 42,150â‚¬","Meilleur modÃ¨le: Random Forest"]},{title:"Clustering K-means avancÃ©",description:"Segmentation client sophistiquÃ©e avec optimisation et analyse mÃ©tier",difficulty:"AvancÃ©",estimatedTime:"45 min",code:`# Clustering K-means avancÃ© pour segmentation client - Analyse mÃ©tier complÃ¨te
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configuration pour de beaux graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("Set2")

print("ðŸ›ï¸ SEGMENTATION CLIENT AVANCÃ‰E")
print("=" * 50)

# 1. ðŸŽ² GÃ‰NÃ‰RATION DE DONNÃ‰ES CLIENT RÃ‰ALISTES
print("\\nðŸ“Š GÃ©nÃ©ration du dataset client...")
np.random.seed(42)
n_clients = 2000

# DÃ©finition de 4 segments clients rÃ©alistes
segments = {
    'Ã‰conomes': {'size': 400, 'revenus': (25, 8), 'fidelite': (30, 12), 'freq_achat': (2, 1), 'panier_moyen': (25, 8)},
    'Moyens': {'size': 800, 'revenus': (45, 12), 'fidelite': (55, 15), 'freq_achat': (6, 2), 'panier_moyen': (65, 20)},
    'FidÃ¨les': {'size': 600, 'revenus': (55, 15), 'fidelite': (80, 10), 'freq_achat': (12, 3), 'panier_moyen': (85, 25)},
    'VIP': {'size': 200, 'revenus': (90, 20), 'fidelite': (85, 8), 'freq_achat': (20, 5), 'panier_moyen': (150, 40)}
}

# GÃ©nÃ©ration des donnÃ©es pour chaque segment
clients_data = []
true_labels = []

for segment_name, params in segments.items():
    size = params['size']
    
    # Revenus (en kâ‚¬)
    revenus = np.random.normal(params['revenus'][0], params['revenus'][1], size)
    revenus = np.clip(revenus, 15, 150)
    
    # Score de fidÃ©litÃ© (0-100)
    fidelite = np.random.normal(params['fidelite'][0], params['fidelite'][1], size)
    fidelite = np.clip(fidelite, 0, 100)
    
    # FrÃ©quence d'achat mensuelle
    freq_achat = np.random.normal(params['freq_achat'][0], params['freq_achat'][1], size)
    freq_achat = np.clip(freq_achat, 0.5, 30)
    
    # Panier moyen (â‚¬)
    panier_moyen = np.random.normal(params['panier_moyen'][0], params['panier_moyen'][1], size)
    panier_moyen = np.clip(panier_moyen, 10, 300)
    
    # Ajout de corrÃ©lations rÃ©alistes
    # Plus de revenus = panier plus Ã©levÃ©
    panier_moyen = panier_moyen + revenus * 0.3 + np.random.normal(0, 5, size)
    panier_moyen = np.clip(panier_moyen, 10, 300)
    
    # Clients fidÃ¨les achÃ¨tent plus souvent
    freq_achat = freq_achat + fidelite * 0.05 + np.random.normal(0, 1, size)
    freq_achat = np.clip(freq_achat, 0.5, 30)
    
    for i in range(size):
        clients_data.append([revenus[i], fidelite[i], freq_achat[i], panier_moyen[i]])
        true_labels.append(segment_name)

# CrÃ©ation du DataFrame
df = pd.DataFrame(clients_data, columns=['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen'])
df['segment_reel'] = true_labels

# Ajout de features dÃ©rivÃ©es mÃ©tier
df['ca_mensuel'] = df['freq_achat_mensuelle'] * df['panier_moyen']
df['ca_annuel'] = df['ca_mensuel'] * 12
df['ratio_fidelite_revenus'] = df['score_fidelite'] / df['revenus_annuels']

print(f"âœ… Dataset crÃ©Ã©: {len(df)} clients")
print(f"ðŸ’° CA annuel moyen: {df['ca_annuel'].mean():,.0f}â‚¬")
print(f"ðŸ“Š RÃ©partition des vrais segments:")
print(df['segment_reel'].value_counts())

# 2. ðŸ” ANALYSE EXPLORATOIRE MÃ‰TIER APPROFONDIE
print("\\nðŸ” Analyse exploratoire mÃ©tier")
print("=" * 35)

# Statistiques par segment rÃ©el
print("ðŸ“Š Statistiques par segment rÃ©el:")
stats_by_segment = df.groupby('segment_reel').agg({
    'revenus_annuels': ['mean', 'std'],
    'score_fidelite': ['mean', 'std'],
    'freq_achat_mensuelle': ['mean', 'std'],
    'panier_moyen': ['mean', 'std'],
    'ca_annuel': ['mean', 'std']
}).round(2)

print(stats_by_segment)

# Visualisations mÃ©tier complÃ¨tes
fig, axes = plt.subplots(3, 4, figsize=(24, 18))
fig.suptitle('ðŸ›ï¸ Analyse Exploratoire ComplÃ¨te - Segmentation Client', fontsize=16, fontweight='bold')

# Distribution des variables principales
variables = ['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen']
for i, var in enumerate(variables):
    axes[0, i].hist(df[var], bins=30, alpha=0.7, edgecolor='black')
    axes[0, i].set_title(f'ðŸ“Š Distribution {var}')
    axes[0, i].set_xlabel(var)
    axes[0, i].set_ylabel('FrÃ©quence')

# Box plots par segment rÃ©el
for i, var in enumerate(variables):
    df.boxplot(column=var, by='segment_reel', ax=axes[1, i])
    axes[1, i].set_title(f'ðŸ“¦ {var} par segment')
    axes[1, i].set_xlabel('Segment')

# Matrices de corrÃ©lation et scatter plots
correlation_matrix = df[variables + ['ca_annuel']].corr()
im = axes[2, 0].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
axes[2, 0].set_xticks(range(len(correlation_matrix.columns)))
axes[2, 0].set_yticks(range(len(correlation_matrix.columns)))
axes[2, 0].set_xticklabels(correlation_matrix.columns, rotation=45)
axes[2, 0].set_yticklabels(correlation_matrix.columns)
axes[2, 0].set_title('ðŸ”— Matrice de CorrÃ©lation')

# Scatter plots mÃ©tier
scatter_plots = [
    ('revenus_annuels', 'panier_moyen', 'ðŸ’° Revenus vs Panier'),
    ('score_fidelite', 'freq_achat_mensuelle', 'â¤ï¸ FidÃ©litÃ© vs FrÃ©quence'),
    ('ca_annuel', 'score_fidelite', 'ðŸ“ˆ CA vs FidÃ©litÃ©')
]

for i, (x, y, title) in enumerate(scatter_plots):
    scatter = axes[2, i+1].scatter(df[x], df[y], c=df['segment_reel'].astype('category').cat.codes, 
                                  cmap='Set1', alpha=0.6)
    axes[2, i+1].set_xlabel(x)
    axes[2, i+1].set_ylabel(y)
    axes[2, i+1].set_title(title)

plt.tight_layout()
plt.show()

# 3. âš™ï¸ PRÃ‰PARATION AVANCÃ‰E DES DONNÃ‰ES
print("\\nâš™ï¸ PrÃ©paration avancÃ©e des donnÃ©es")
print("=" * 40)

# SÃ©lection des features pour le clustering
features_clustering = ['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen', 'ca_annuel']
X = df[features_clustering].copy()

print(f"ðŸ“Š Features sÃ©lectionnÃ©es: {features_clustering}")
print(f"ðŸ“ˆ Forme des donnÃ©es: {X.shape}")

# DÃ©tection et traitement des outliers avec mÃ©thode IQR
def detect_outliers_iqr(data, factor=1.5):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers

outliers_mask = pd.DataFrame(index=X.index)
for col in X.columns:
    outliers_mask[col] = detect_outliers_iqr(X[col])

total_outliers = outliers_mask.any(axis=1).sum()
print(f"ðŸš¨ Outliers dÃ©tectÃ©s: {total_outliers} ({total_outliers/len(X)*100:.1f}%)")

# Option: supprimer les outliers extrÃªmes ou les conserver pour l'analyse
# X_clean = X[~outliers_mask.any(axis=1)]
X_clean = X.copy()  # Conserver tous les points pour cet exemple

# Standardisation avec RobustScaler (moins sensible aux outliers)
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X_clean)
X_scaled_df = pd.DataFrame(X_scaled, columns=features_clustering, index=X_clean.index)

print(f"âœ… DonnÃ©es standardisÃ©es: {X_scaled.shape}")

# 4. ðŸ” DÃ‰TERMINATION OPTIMALE DU NOMBRE DE CLUSTERS
print("\\nðŸ” Optimisation du nombre de clusters")
print("=" * 45)

# MÃ©thodes multiples pour dÃ©terminer k optimal
k_range = range(2, 12)
metrics = {
    'inertia': [],
    'silhouette': [],
    'calinski_harabasz': [],
    'davies_bouldin': []
}

print("ðŸ”„ Test de diffÃ©rentes valeurs de k...")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    metrics['inertia'].append(kmeans.inertia_)
    metrics['silhouette'].append(silhouette_score(X_scaled, cluster_labels))
    metrics['calinski_harabasz'].append(calinski_harabasz_score(X_scaled, cluster_labels))
    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, cluster_labels))

# Visualisation des mÃ©triques
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('ðŸ“Š Optimisation du Nombre de Clusters', fontsize=14, fontweight='bold')

# MÃ©thode du coude (Elbow)
axes[0,0].plot(k_range, metrics['inertia'], 'bo-', linewidth=2, markersize=8)
axes[0,0].set_xlabel('Nombre de clusters (k)')
axes[0,0].set_ylabel('Inertie')
axes[0,0].set_title('ðŸ“ MÃ©thode du Coude')
axes[0,0].grid(True, alpha=0.3)

# Score de silhouette (plus Ã©levÃ© = mieux)
axes[0,1].plot(k_range, metrics['silhouette'], 'ro-', linewidth=2, markersize=8)
axes[0,1].set_xlabel('Nombre de clusters (k)')
axes[0,1].set_ylabel('Score de Silhouette')
axes[0,1].set_title('ðŸŽ¯ Score de Silhouette')
axes[0,1].grid(True, alpha=0.3)

# Calinski-Harabasz (plus Ã©levÃ© = mieux)
axes[1,0].plot(k_range, metrics['calinski_harabasz'], 'go-', linewidth=2, markersize=8)
axes[1,0].set_xlabel('Nombre de clusters (k)')
axes[1,0].set_ylabel('Score Calinski-Harabasz')
axes[1,0].set_title('ðŸ“ˆ Score Calinski-Harabasz')
axes[1,0].grid(True, alpha=0.3)

# Davies-Bouldin (plus faible = mieux)
axes[1,1].plot(k_range, metrics['davies_bouldin'], 'mo-', linewidth=2, markersize=8)
axes[1,1].set_xlabel('Nombre de clusters (k)')
axes[1,1].set_ylabel('Score Davies-Bouldin')
axes[1,1].set_title('ðŸ“‰ Score Davies-Bouldin')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# DÃ©termination automatique du k optimal
k_optimal_silhouette = k_range[np.argmax(metrics['silhouette'])]
k_optimal_ch = k_range[np.argmax(metrics['calinski_harabasz'])]
k_optimal_db = k_range[np.argmin(metrics['davies_bouldin'])]

print(f"ðŸŽ¯ K optimal selon Silhouette: {k_optimal_silhouette}")
print(f"ðŸ“Š K optimal selon Calinski-Harabasz: {k_optimal_ch}")
print(f"ðŸ“‰ K optimal selon Davies-Bouldin: {k_optimal_db}")

# Utilisation de la moyenne des recommandations
k_optimal = int(np.mean([k_optimal_silhouette, k_optimal_ch, k_optimal_db]))
print(f"\\nðŸ† K optimal choisi: {k_optimal}")

# 5. ðŸ¤– COMPARAISON D'ALGORITHMES DE CLUSTERING
print("\\nðŸ¤– Comparaison des algorithmes de clustering")
print("=" * 50)

algorithms = {
    'K-Means': KMeans(n_clusters=k_optimal, random_state=42, n_init=10),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
    'Agglomerative': AgglomerativeClustering(n_clusters=k_optimal)
}

clustering_results = {}

for name, algorithm in algorithms.items():
    print(f"\\nðŸ”§ Test {name}...")
    
    if name == 'DBSCAN':
        # Optimisation automatique d'epsilon pour DBSCAN
        from sklearn.neighbors import NearestNeighbors
        neighbors = NearestNeighbors(n_neighbors=4)
        neighbors_fit = neighbors.fit(X_scaled)
        distances, indices = neighbors_fit.kneighbors(X_scaled)
        distances = np.sort(distances[:, 3], axis=0)
        
        # Heuristique pour epsilon (peut nÃ©cessiter ajustement)
        eps_optimal = distances[int(0.95 * len(distances))]
        algorithm.set_params(eps=eps_optimal)
    
    cluster_labels = algorithm.fit_predict(X_scaled)
    
    # Calcul des mÃ©triques (si plus d'un cluster)
    n_clusters = len(np.unique(cluster_labels))
    if n_clusters > 1 and -1 not in cluster_labels:  # Pas de points de bruit
        silhouette = silhouette_score(X_scaled, cluster_labels)
        ch_score = calinski_harabasz_score(X_scaled, cluster_labels)
        db_score = davies_bouldin_score(X_scaled, cluster_labels)
    else:
        silhouette = ch_score = db_score = np.nan
    
    clustering_results[name] = {
        'algorithm': algorithm,
        'labels': cluster_labels,
        'n_clusters': n_clusters,
        'silhouette': silhouette,
        'calinski_harabasz': ch_score,
        'davies_bouldin': db_score
    }
    
    print(f"   ðŸŽ¯ Nombre de clusters: {n_clusters}")
    if not np.isnan(silhouette):
        print(f"   ðŸ“Š Silhouette: {silhouette:.3f}")
        print(f"   ðŸ“ˆ Calinski-Harabasz: {ch_score:.2f}")
        print(f"   ðŸ“‰ Davies-Bouldin: {db_score:.3f}")

# SÃ©lection du meilleur algorithme
valid_algorithms = {k: v for k, v in clustering_results.items() if not np.isnan(v['silhouette'])}
best_algorithm_name = max(valid_algorithms.keys(), key=lambda x: valid_algorithms[x]['silhouette'])
best_clusters = clustering_results[best_algorithm_name]['labels']

print(f"\\nðŸ† Meilleur algorithme: {best_algorithm_name}")
print(f"ðŸŽ¯ Score silhouette: {clustering_results[best_algorithm_name]['silhouette']:.3f}")

# 6. ðŸ“Š VISUALISATION AVANCÃ‰E DES CLUSTERS
print("\\nðŸ“Š CrÃ©ation des visualisations avancÃ©es...")

# RÃ©duction de dimensionnalitÃ© pour visualisation
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_scaled)

# Graphiques de visualisation
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle(f'ðŸŽ¯ Visualisation AvancÃ©e - {best_algorithm_name}', fontsize=16, fontweight='bold')

# PCA
scatter = axes[0,0].scatter(X_pca[:, 0], X_pca[:, 1], c=best_clusters, cmap='Set1', alpha=0.7)
axes[0,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
axes[0,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
axes[0,0].set_title('ðŸ“Š Visualisation PCA')

# t-SNE
axes[0,1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_clusters, cmap='Set1', alpha=0.7)
axes[0,1].set_xlabel('t-SNE 1')
axes[0,1].set_ylabel('t-SNE 2')
axes[0,1].set_title('ðŸ” Visualisation t-SNE')

# Comparaison avec vrais segments
true_labels_encoded = pd.Categorical(df['segment_reel']).codes
axes[0,2].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels_encoded, cmap='Set2', alpha=0.7)
axes[0,2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
axes[0,2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%% variance)')
axes[0,2].set_title('ðŸŽ¯ Vrais Segments (RÃ©fÃ©rence)')

# Analyse par paires de variables
axes[1,0].scatter(df['revenus_annuels'], df['score_fidelite'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,0].set_xlabel('Revenus Annuels (kâ‚¬)')
axes[1,0].set_ylabel('Score FidÃ©litÃ©')
axes[1,0].set_title('ðŸ’° Revenus vs FidÃ©litÃ©')

axes[1,1].scatter(df['freq_achat_mensuelle'], df['panier_moyen'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,1].set_xlabel('FrÃ©quence Achat Mensuelle')
axes[1,1].set_ylabel('Panier Moyen (â‚¬)')
axes[1,1].set_title('ðŸ›’ FrÃ©quence vs Panier')

axes[1,2].scatter(df['ca_annuel'], df['score_fidelite'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,2].set_xlabel('CA Annuel (â‚¬)')
axes[1,2].set_ylabel('Score FidÃ©litÃ©')
axes[1,2].set_title('ðŸ“ˆ CA vs FidÃ©litÃ©')

plt.tight_layout()
plt.show()

# 7. ðŸ§¾ ANALYSE MÃ‰TIER APPROFONDIE DES SEGMENTS
print("\\nðŸ§¾ ANALYSE MÃ‰TIER DES SEGMENTS DÃ‰COUVERTS")
print("=" * 55)

# Ajout des clusters au DataFrame
df['cluster'] = best_clusters

# Analyse dÃ©taillÃ©e par cluster
cluster_analysis = df.groupby('cluster').agg({
    'revenus_annuels': ['count', 'mean', 'std', 'min', 'max'],
    'score_fidelite': ['mean', 'std'],
    'freq_achat_mensuelle': ['mean', 'std'],
    'panier_moyen': ['mean', 'std'],
    'ca_annuel': ['mean', 'std', 'sum']
}).round(2)

print("ðŸ“Š Analyse dÃ©taillÃ©e par cluster:")
print(cluster_analysis)

# CaractÃ©risation mÃ©tier de chaque segment
print("\\nðŸŽ¯ CARACTÃ‰RISATION MÃ‰TIER DES SEGMENTS:")
print("=" * 50)

for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:  # Points de bruit DBSCAN
        continue
        
    cluster_data = df[df['cluster'] == cluster_id]
    size = len(cluster_data)
    
    print(f"\\nðŸ·ï¸  CLUSTER {cluster_id} ({size} clients - {size/len(df)*100:.1f}%)")
    print("-" * 40)
    
    # Statistiques principales
    stats = cluster_data[features_clustering].mean()
    print(f"ðŸ’° Revenus moyens: {stats['revenus_annuels']:.0f}kâ‚¬")
    print(f"â¤ï¸  FidÃ©litÃ© moyenne: {stats['score_fidelite']:.0f}/100")
    print(f"ðŸ›’ FrÃ©quence d'achat: {stats['freq_achat_mensuelle']:.1f}/mois")
    print(f"ðŸ›ï¸  Panier moyen: {stats['panier_moyen']:.0f}â‚¬")
    print(f"ðŸ“ˆ CA annuel moyen: {stats['ca_annuel']:,.0f}â‚¬")
    print(f"ðŸ’Ž CA total du segment: {cluster_data['ca_annuel'].sum():,.0f}â‚¬")
    
    # Recommandations stratÃ©giques
    ca_moyen = stats['ca_annuel']
    fidelite_moyenne = stats['score_fidelite']
    revenus_moyens = stats['revenus_annuels']
    
    print(f"\\nðŸŽ¯ STRATÃ‰GIE RECOMMANDÃ‰E:")
    if ca_moyen > 15000 and fidelite_moyenne > 75:
        print("   ðŸŒŸ SEGMENT VIP - Programme privilÃ¨ge, services exclusifs")
    elif ca_moyen > 8000 and fidelite_moyenne > 60:
        print("   ðŸ’Ž SEGMENT FIDÃˆLE - RÃ©compenses fidÃ©litÃ©, offres personnalisÃ©es")
    elif revenus_moyens > 50 and fidelite_moyenne < 50:
        print("   ðŸŽ¯ SEGMENT POTENTIEL - Campagnes d'engagement, amÃ©liorer expÃ©rience")
    else:
        print("   ðŸ“¢ SEGMENT BASIQUE - Promotions attractives, acquisition")

# 8. ðŸ“Š COMPARAISON AVEC LES VRAIS SEGMENTS
print("\\nðŸ“Š COMPARAISON AVEC LES SEGMENTS RÃ‰ELS")
print("=" * 45)

# Matrice de contingence
contingency_matrix = pd.crosstab(df['segment_reel'], df['cluster'], margins=True)
print("ðŸ“‹ Matrice de contingence (RÃ©el vs PrÃ©dit):")
print(contingency_matrix)

# Calcul de la puretÃ© des clusters
purity_scores = []
for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:
        continue
    cluster_data = df[df['cluster'] == cluster_id]
    most_common_segment = cluster_data['segment_reel'].mode()[0]
    purity = (cluster_data['segment_reel'] == most_common_segment).mean()
    purity_scores.append(purity)
    print(f"ðŸŽ¯ PuretÃ© Cluster {cluster_id}: {purity:.2f} (majoritairement {most_common_segment})")

average_purity = np.mean(purity_scores)
print(f"\\nðŸ† PuretÃ© moyenne: {average_purity:.2f}")

print("\\nâœ… ANALYSE COMPLÃˆTE TERMINÃ‰E!")
print("ðŸŽ¯ Vos segments clients sont prÃªts pour les actions marketing!")`,language:"python",outputs:["4 clusters dÃ©tectÃ©s","Score silhouette: 0.687","PuretÃ© moyenne: 0.83"]}],Y=(p,m)=>{navigator.clipboard.writeText(p),N(m),setTimeout(()=>N(null),2e3)};return e.jsxs("section",{id:"practical-exercises",className:"space-y-16",children:[e.jsxs("div",{className:"text-center mb-12",children:[e.jsx("h2",{className:"text-4xl font-bold mb-6 bg-gradient-to-r from-purple-600 to-blue-600 bg-clip-text text-transparent",children:"Exercices Pratiques et Exemples de Code"}),e.jsx("p",{className:"text-xl text-gray-600 max-w-4xl mx-auto",children:"MaÃ®trisez le Machine Learning par la pratique avec des projets concrets, des dÃ©fis progressifs et des exemples dÃ©taillÃ©s"}),e.jsxs("div",{className:"flex justify-center gap-4 mt-6",children:[e.jsx(d,{className:"bg-purple-100 text-purple-800 px-4 py-2",children:"ðŸ’» Code Complet"}),e.jsx(d,{className:"bg-blue-100 text-blue-800 px-4 py-2",children:"ðŸ“Š Analyses DÃ©taillÃ©es"}),e.jsx(d,{className:"bg-green-100 text-green-800 px-4 py-2",children:"ðŸŽ¯ Applications MÃ©tier"})]})]}),e.jsx(u,{title:"ðŸ’» Laboratoire de Code Interactif",type:"exemple",children:e.jsxs("div",{className:"space-y-8",children:[e.jsx("div",{className:"text-center mb-6",children:e.jsx("p",{className:"text-lg text-gray-700 mb-4",children:"Explorez des implÃ©mentations complÃ¨tes avec analyses dÃ©taillÃ©es et recommandations mÃ©tier"})}),e.jsx("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-4 mb-6",children:c.map((p,m)=>e.jsxs("button",{onClick:()=>a(m),className:`p-6 rounded-xl text-left transition-all duration-500 border-2 hover:scale-105 ${s===m?"bg-gradient-to-r from-blue-600 to-purple-600 text-white shadow-2xl border-blue-500":"bg-white text-gray-700 hover:bg-gray-50 border-gray-200 hover:border-gray-300 hover:shadow-lg"}`,children:[e.jsxs("div",{className:"flex items-center justify-between mb-3",children:[e.jsx("h4",{className:"font-bold text-lg",children:p.title}),e.jsx(d,{className:`${s===m?"bg-white text-blue-600":"bg-gray-100 text-gray-700"}`,children:p.difficulty})]}),e.jsx("p",{className:`text-sm mb-3 ${s===m?"text-blue-100":"text-gray-600"}`,children:p.description}),e.jsx("div",{className:"flex items-center gap-2",children:e.jsxs("span",{className:`text-xs ${s===m?"text-blue-200":"text-gray-500"}`,children:["â±ï¸ ",p.estimatedTime]})})]},m))}),e.jsxs(t,{className:"border-2 hover:shadow-2xl transition-all duration-500",children:[e.jsx(n,{className:"bg-gradient-to-r from-gray-50 to-gray-100",children:e.jsxs("div",{className:"flex flex-col lg:flex-row lg:items-center lg:justify-between gap-4",children:[e.jsxs("div",{className:"flex items-start gap-4",children:[e.jsx("div",{className:"bg-blue-600 p-3 rounded-lg",children:e.jsx(S,{className:"h-6 w-6 text-white"})}),e.jsxs("div",{children:[e.jsx(l,{className:"text-xl",children:c[s].title}),e.jsx(f,{className:"text-lg mt-1",children:c[s].description}),e.jsxs("div",{className:"flex items-center gap-4 mt-3",children:[e.jsxs(d,{className:"bg-blue-100 text-blue-800",children:["ðŸŽ¯ ",c[s].difficulty]}),e.jsxs(d,{className:"bg-green-100 text-green-800",children:["â±ï¸ ",c[s].estimatedTime]})]})]})]}),e.jsxs("div",{className:"flex gap-3",children:[e.jsx("button",{onClick:()=>Y(c[s].code,s),className:"flex items-center gap-2 px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors shadow-lg",children:b===s?e.jsxs(e.Fragment,{children:[e.jsx("span",{className:"text-green-300",children:"âœ“"}),"CopiÃ© !"]}):e.jsxs(e.Fragment,{children:[e.jsx(ne,{className:"h-4 w-4"}),"Copier le code"]})}),e.jsxs("button",{className:"flex items-center gap-2 px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition-colors shadow-lg",children:[e.jsx(le,{className:"h-4 w-4"}),"TÃ©lÃ©charger"]})]})]})}),e.jsxs(o,{className:"p-0",children:[e.jsx("div",{className:"bg-gray-950 text-gray-100 p-8 overflow-x-auto",children:e.jsx("pre",{className:"text-sm leading-relaxed",children:e.jsx("code",{children:c[s].code})})}),e.jsxs("div",{className:"bg-green-50 p-6 border-t-4 border-green-500",children:[e.jsxs("h4",{className:"font-bold text-green-800 mb-3 flex items-center gap-2",children:[e.jsx(C,{className:"h-5 w-5"}),"RÃ©sultats attendus :"]}),e.jsx("div",{className:"space-y-2",children:c[s].outputs.map((p,m)=>e.jsx("div",{className:"bg-white p-3 rounded-lg border border-green-200",children:e.jsx("code",{className:"text-sm text-green-700",children:p})},m))})]})]})]})]})}),e.jsxs("div",{className:"grid grid-cols-1 lg:grid-cols-2 gap-8",children:[e.jsx(h,{title:"ðŸŽ¬ SystÃ¨me de Recommandation de Films",difficulty:"intermÃ©diaire",estimatedTime:"25 min",problem:"Vous travaillez pour une plateforme de streaming qui souhaite amÃ©liorer son systÃ¨me de recommandation. CrÃ©ez un systÃ¨me qui recommande des films basÃ© sur les ratings des utilisateurs et les mÃ©tadonnÃ©es des films (genre, acteurs, rÃ©alisateur). Le dataset contient 10,000 utilisateurs, 5,000 films et 1 million de ratings.",hints:["Commencez par une approche de filtrage collaboratif avec la similaritÃ© cosinus","Utilisez la factorisation matricielle (SVD) pour capturer les patterns latents","IntÃ©grez les mÃ©tadonnÃ©es avec un modÃ¨le hybride (collaboratif + contenu)","Ã‰valuez avec RMSE, prÃ©cision@K et diversitÃ© des recommandations","GÃ©rez le problÃ¨me de dÃ©marrage Ã  froid pour les nouveaux utilisateurs"],solution:`# SystÃ¨me de Recommandation de Films - Solution ComplÃ¨te
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt

# 1. PRÃ‰PARATION DES DONNÃ‰ES
print("ðŸŽ¬ SystÃ¨me de Recommandation de Films")
print("=" * 40)

# Simulation de donnÃ©es (en pratique, vous chargeriez vos datasets)
np.random.seed(42)

# GÃ©nÃ©ration de donnÃ©es rÃ©alistes
n_users, n_movies = 1000, 500  # RÃ©duit pour l'exemple
n_ratings = 50000

users = np.random.randint(0, n_users, n_ratings)
movies = np.random.randint(0, n_movies, n_ratings)
ratings = np.random.choice([1,2,3,4,5], n_ratings, p=[0.1,0.1,0.2,0.4,0.2])

# DataFrame des ratings
ratings_df = pd.DataFrame({
    'user_id': users,
    'movie_id': movies,
    'rating': ratings
})

# Suppression des doublons (un user peut noter un film qu'une fois)
ratings_df = ratings_df.drop_duplicates(['user_id', 'movie_id'])

print(f"ðŸ“Š Dataset: {len(ratings_df)} ratings")
print(f"ðŸ‘¥ Utilisateurs: {ratings_df['user_id'].nunique()}")
print(f"ðŸŽ¬ Films: {ratings_df['movie_id'].nunique()}")

# 2. CRÃ‰ATION DE LA MATRICE USER-ITEM
print("\\nðŸ”§ CrÃ©ation de la matrice utilisateur-film...")

# Matrice pivot
user_item_matrix = ratings_df.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)
print(f"ðŸ“ Forme de la matrice: {user_item_matrix.shape}")

# Conversion en matrice sparse pour l'efficacitÃ©
user_item_sparse = csr_matrix(user_item_matrix.values)

# 3. FILTRAGE COLLABORATIF - SIMILARITÃ‰ UTILISATEURS
print("\\nðŸ‘¥ Filtrage collaboratif basÃ© utilisateurs...")

def collaborative_filtering_users(user_id, user_item_matrix, n_recommendations=10):
    # Calcul de similaritÃ© entre utilisateurs
    user_similarity = cosine_similarity(user_item_matrix)
    
    # Obtenir l'index de l'utilisateur
    user_idx = list(user_item_matrix.index).index(user_id)
    
    # Trouver les utilisateurs similaires
    similar_users = user_similarity[user_idx]
    similar_users_idx = np.argsort(similar_users)[::-1][1:11]  # Top 10 (excluant lui-mÃªme)
    
    # Films dÃ©jÃ  vus par l'utilisateur
    user_ratings = user_item_matrix.iloc[user_idx]
    seen_movies = user_ratings[user_ratings > 0].index
    
    # Calculer les scores de recommandation
    recommendations = {}
    for movie in user_item_matrix.columns:
        if movie not in seen_movies:
            score = 0
            similarity_sum = 0
            
            for similar_user_idx in similar_users_idx:
                similar_user_id = user_item_matrix.index[similar_user_idx]
                similarity = similar_users[similar_user_idx]
                rating = user_item_matrix.loc[similar_user_id, movie]
                
                if rating > 0:  # Si l'utilisateur similaire a notÃ© ce film
                    score += similarity * rating
                    similarity_sum += similarity
            
            if similarity_sum > 0:
                recommendations[movie] = score / similarity_sum
    
    # Trier et retourner top N
    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
    return sorted_recommendations[:n_recommendations]

# 4. FACTORISATION MATRICIELLE (SVD)
print("\\nðŸ§® Factorisation matricielle avec SVD...")

# SVD pour capturer les facteurs latents
svd = TruncatedSVD(n_components=50, random_state=42)
user_factors = svd.fit_transform(user_item_sparse)
movie_factors = svd.components_.T

print(f"âœ… SVD terminÃ©e: {user_factors.shape[1]} facteurs latents")

def svd_recommendations(user_id, user_item_matrix, user_factors, movie_factors, n_recommendations=10):
    user_idx = list(user_item_matrix.index).index(user_id)
    
    # Films dÃ©jÃ  vus
    user_ratings = user_item_matrix.iloc[user_idx]
    seen_movies = user_ratings[user_ratings > 0].index
    
    # PrÃ©diction pour tous les films
    user_vector = user_factors[user_idx]
    predicted_ratings = np.dot(user_vector, movie_factors.T)
    
    # CrÃ©er dataframe des prÃ©dictions
    predictions = pd.DataFrame({
        'movie_id': user_item_matrix.columns,
        'predicted_rating': predicted_ratings
    })
    
    # Filtrer les films dÃ©jÃ  vus
    unseen_predictions = predictions[~predictions['movie_id'].isin(seen_movies)]
    
    # Trier et retourner top N
    top_recommendations = unseen_predictions.nlargest(n_recommendations, 'predicted_rating')
    
    return list(zip(top_recommendations['movie_id'], top_recommendations['predicted_rating']))

# 5. Ã‰VALUATION DU SYSTÃˆME
print("\\nðŸ“Š Ã‰valuation du systÃ¨me...")

# Division train/test
def train_test_split_ratings(ratings_df, test_ratio=0.2):
    # Pour chaque utilisateur, garder quelques ratings pour le test
    train_list, test_list = [], []
    
    for user_id in ratings_df['user_id'].unique():
        user_ratings = ratings_df[ratings_df['user_id'] == user_id]
        
        if len(user_ratings) >= 5:  # Au moins 5 ratings
            n_test = max(1, int(len(user_ratings) * test_ratio))
            test_indices = np.random.choice(user_ratings.index, n_test, replace=False)
            
            test_list.append(user_ratings.loc[test_indices])
            train_list.append(user_ratings.drop(test_indices))
        else:
            train_list.append(user_ratings)
    
    train_df = pd.concat(train_list, ignore_index=True)
    test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()
    
    return train_df, test_df

train_df, test_df = train_test_split_ratings(ratings_df)
print(f"ðŸ“ˆ Train: {len(train_df)} ratings")
print(f"ðŸŽ¯ Test: {len(test_df)} ratings")

# RecrÃ©er la matrice avec donnÃ©es d'entraÃ®nement
train_matrix = train_df.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)

# 6. SYSTÃˆME HYBRIDE (Exemple simplifiÃ©)
print("\\nðŸ”„ SystÃ¨me hybride...")

def hybrid_recommendations(user_id, train_matrix, user_factors, movie_factors, 
                          alpha=0.7, n_recommendations=10):
    try:
        # Recommandations SVD
        svd_recs = dict(svd_recommendations(user_id, train_matrix, user_factors, movie_factors, 20))
        
        # Recommandations collaboratives
        collab_recs = dict(collaborative_filtering_users(user_id, train_matrix, 20))
        
        # Combinaison hybride
        all_movies = set(svd_recs.keys()) | set(collab_recs.keys())
        hybrid_scores = {}
        
        for movie in all_movies:
            svd_score = svd_recs.get(movie, 0)
            collab_score = collab_recs.get(movie, 0)
            
            # Normalisation simple (en pratique, plus sophistiquÃ©e)
            hybrid_scores[movie] = alpha * svd_score + (1 - alpha) * collab_score
        
        # Retourner top N
        sorted_hybrid = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_hybrid[:n_recommendations]
    
    except:
        # Fallback si erreur
        return []

# 7. TEST ET DÃ‰MONSTRATION
print("\\nðŸŽ¬ Test du systÃ¨me de recommandation...")

# Choisir un utilisateur test
test_users = [u for u in test_df['user_id'].unique() if u in train_matrix.index]
if test_users:
    test_user = test_users[0]
    
    print(f"\\nðŸŽ¯ Recommandations pour l'utilisateur {test_user}:")
    
    # SVD recommandations
    svd_recs = svd_recommendations(test_user, train_matrix, user_factors, movie_factors, 5)
    print("\\nðŸ“Š Top 5 - SVD:")
    for i, (movie, score) in enumerate(svd_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")
    
    # Collaboratif
    collab_recs = collaborative_filtering_users(test_user, train_matrix, 5)
    print("\\nðŸ‘¥ Top 5 - Collaboratif:")
    for i, (movie, score) in enumerate(collab_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")
    
    # Hybride
    hybrid_recs = hybrid_recommendations(test_user, train_matrix, user_factors, movie_factors, 0.7, 5)
    print("\\nðŸ”„ Top 5 - Hybride:")
    for i, (movie, score) in enumerate(hybrid_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")

# 8. MÃ‰TRIQUES D'Ã‰VALUATION
print("\\nðŸ“ˆ Calcul des mÃ©triques d'Ã©valuation...")

def calculate_precision_at_k(recommended_items, relevant_items, k):
    recommended_at_k = recommended_items[:k]
    relevant_and_recommended = set(recommended_at_k) & set(relevant_items)
    
    if len(recommended_at_k) == 0:
        return 0
    
    return len(relevant_and_recommended) / len(recommended_at_k)

# Exemple de calcul de prÃ©cision@5
if test_users and len(test_df) > 0:
    test_user = test_users[0]
    user_test_movies = test_df[test_df['user_id'] == test_user]
    relevant_movies = user_test_movies[user_test_movies['rating'] >= 4]['movie_id'].tolist()
    
    if hybrid_recs:
        recommended_movies = [movie for movie, score in hybrid_recs]
        precision_5 = calculate_precision_at_k(recommended_movies, relevant_movies, 5)
        print(f"ðŸŽ¯ PrÃ©cision@5: {precision_5:.2f}")

print("\\nâœ… SystÃ¨me de recommandation opÃ©rationnel!")
print("ðŸš€ PrÃªt pour la production avec monitoring et A/B testing!")`}),e.jsx(h,{title:"ðŸ” DÃ©tection d'Anomalies dans les Transactions",difficulty:"avancÃ©",estimatedTime:"35 min",problem:"Une banque vous demande de crÃ©er un systÃ¨me de dÃ©tection de fraude en temps rÃ©el. Analysez 100,000 transactions avec 30 features (montant, localisation, heure, type de marchande, historique client...). Seules 0.1% des transactions sont frauduleuses. CrÃ©ez un modÃ¨le capable de dÃ©tecter 95% des fraudes tout en minimisant les faux positifs.",hints:["Le dataset est trÃ¨s dÃ©sÃ©quilibrÃ© - utilisez des techniques de rÃ©Ã©quilibrage (SMOTE, undersampling)","Testez plusieurs algorithmes : Isolation Forest, One-Class SVM, Random Forest","CrÃ©ez des features d'ingÃ©nierie : frÃ©quence, patterns temporels, gÃ©olocalisation","Optimisez pour le rappel (catch frauds) tout en contrÃ´lant la prÃ©cision","Utilisez la validation temporelle (pas de data leakage temporel)"],solution:`# DÃ©tection d'Anomalies - Fraude Bancaire - Solution ComplÃ¨te
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.metrics import average_precision_score, roc_curve
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import warnings
warnings.filterwarnings('ignore')

print("ðŸ” DÃ‰TECTION DE FRAUDE BANCAIRE")
print("=" * 40)

# 1. GÃ‰NÃ‰RATION DE DONNÃ‰ES RÃ‰ALISTES
print("\\nðŸ“Š GÃ©nÃ©ration du dataset de transactions...")
np.random.seed(42)

n_transactions = 50000  # RÃ©duit pour l'exemple
fraud_rate = 0.002      # 0.2% de fraudes (trÃ¨s dÃ©sÃ©quilibrÃ©)

# GÃ©nÃ©ration des transactions normales (99.8%)
n_normal = int(n_transactions * (1 - fraud_rate))
n_fraud = n_transactions - n_normal

print(f"ðŸ’³ Transactions normales: {n_normal}")
print(f"ðŸš¨ Transactions frauduleuses: {n_fraud}")

# Features pour transactions normales
normal_data = {
    'montant': np.random.lognormal(3, 1, n_normal),  # Distribution log-normale rÃ©aliste
    'heure': np.random.normal(14, 4, n_normal) % 24, # Pic Ã  14h
    'jour_semaine': np.random.choice(range(7), n_normal, p=[0.12,0.14,0.14,0.14,0.14,0.16,0.16]),
    'nb_trans_jour': np.random.poisson(3, n_normal),
    'nb_trans_semaine': np.random.poisson(20, n_normal),
    'solde_compte': np.random.normal(5000, 3000, n_normal),
    'age_compte_jours': np.random.exponential(500, n_normal),
    '
_merchant_category': np.random.choice(range(20), n_normal, p=[0.15,0.12,0.1,0.08,0.06,0.05,0.05,0.04,0.04,0.03,0.03,0.03,0.03,0.03,0.02,0.02,0.02,0.02,0.01,0.08]),
    'pays_merchant': np.random.choice(range(5), n_normal, p=[0.7,0.15,0.08,0.04,0.03]),
    'montant_moy_30j': np.random.lognormal(3, 0.8, n_normal)
}

# Features pour transactions frauduleuses (patterns diffÃ©rents)
fraud_data = {
    'montant': np.random.lognormal(4.5, 1.5, n_fraud),  # Montants plus Ã©levÃ©s
    'heure': np.random.choice(range(24), n_fraud, p=[0.08,0.09,0.1,0.12,0.08,0.06,0.04,0.03,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.04,0.05,0.04,0.03,0.03,0.04,0.06,0.07]),  # Plus la nuit
    'jour_semaine': np.random.choice(range(7), n_fraud),  # RÃ©partition uniforme
    'nb_trans_jour': np.random.poisson(8, n_fraud),  # Plus de transactions
    'nb_trans_semaine': np.random.poisson(45, n_fraud),
    'solde_compte': np.random.normal(3000, 4000, n_fraud),  # Soldes plus variables
    'age_compte_jours': np.random.exponential(200, n_fraud),  # Comptes plus rÃ©cents
    'merchant_category': np.random.choice(range(20), n_fraud),  # RÃ©partition diffÃ©rente
    'pays_merchant': np.random.choice(range(5), n_fraud, p=[0.4,0.2,0.2,0.1,0.1]),  # Plus d'international
    'montant_moy_30j': np.random.lognormal(2.8, 1.2, n_fraud)
}

# CrÃ©ation du DataFrame combinÃ©
transactions_data = []
labels = []

for key in normal_data.keys():
    if key not in transactions_data:
        transactions_data.append([])
    transactions_data[-1] = list(normal_data[key]) + list(fraud_data[key])

# Transposer pour avoir la bonne structure
df_data = {}
for i, key in enumerate(normal_data.keys()):
    df_data[key] = list(normal_data[key]) + list(fraud_data[key])

df = pd.DataFrame(df_data)
df['is_fraud'] = [0] * n_normal + [1] * n_fraud

# Ajout de features d'ingÃ©nierie
df['montant_log'] = np.log1p(df['montant'])
df['ratio_montant_solde'] = df['montant'] / (df['solde_compte'] + 1)
df['ratio_montant_moyenne'] = df['montant'] / (df['montant_moy_30j'] + 1)
df['is_weekend'] = (df['jour_semaine'] >= 5).astype(int)
df['is_night'] = ((df['heure'] >= 22) | (df['heure'] <= 6)).astype(int)
df['freq_trans_day_high'] = (df['nb_trans_jour'] > 10).astype(int)

print(f"âœ… Dataset crÃ©Ã©: {len(df)} transactions")
print(f"ðŸ“Š Taux de fraude: {df['is_fraud'].mean():.3%}")

# 2. ANALYSE EXPLORATOIRE SPÃ‰CIALISÃ‰E
print("\\nðŸ” Analyse exploratoire des patterns de fraude...")

# Statistiques par classe
fraud_stats = df.groupby('is_fraud').agg({
    'montant': ['mean', 'std', 'median'],
    'heure': ['mean', 'std'],
    'nb_trans_jour': ['mean', 'std'],
    'solde_compte': ['mean', 'std'],
    'is_night': 'mean',
    'is_weekend': 'mean'
}).round(3)

print("ðŸ“Š Statistiques par classe:")
print(fraud_stats)

# Visualisations spÃ©cialisÃ©es
fig, axes = plt.subplots(3, 3, figsize=(20, 15))
fig.suptitle('ðŸ” Analyse des Patterns de Fraude', fontsize=16, fontweight='bold')

# Distribution des montants
axes[0,0].hist(df[df['is_fraud']==0]['montant_log'], bins=50, alpha=0.7, label='Normal', density=True)
axes[0,0].hist(df[df['is_fraud']==1]['montant_log'], bins=50, alpha=0.7, label='Fraude', density=True)
axes[0,0].set_title('ðŸ’° Distribution Montants (log)')
axes[0,0].legend()

# RÃ©partition par heure
hour_fraud = df.groupby(['heure', 'is_fraud']).size().unstack().fillna(0)
hour_fraud_rate = hour_fraud[1] / (hour_fraud[0] + hour_fraud[1])
axes[0,1].plot(hour_fraud_rate.index, hour_fraud_rate.values * 100, 'r-o')
axes[0,1].set_title('ðŸ• Taux de Fraude par Heure')
axes[0,1].set_ylabel('Taux de fraude (%)')

# RÃ©partition par jour de la semaine
day_fraud = df.groupby(['jour_semaine', 'is_fraud']).size().unstack().fillna(0)
day_fraud_rate = day_fraud[1] / (day_fraud[0] + day_fraud[1])
axes[0,2].bar(range(7), day_fraud_rate.values * 100, color='orange', alpha=0.7)
axes[0,2].set_title('ðŸ“… Taux de Fraude par Jour')
axes[0,2].set_xticks(range(7))
axes[0,2].set_xticklabels(['L','M','M','J','V','S','D'])

# CorrÃ©lation avec la fraude
correlation_with_fraud = df.corr()['is_fraud'].sort_values(key=abs, ascending=False)[1:]
axes[1,0].barh(range(len(correlation_with_fraud)), correlation_with_fraud.values)
axes[1,0].set_yticks(range(len(correlation_with_fraud)))
axes[1,0].set_yticklabels(correlation_with_fraud.index, rotation=0)
axes[1,0].set_title('ðŸ”— CorrÃ©lation avec Fraude')

# Box plot montants
df.boxplot(column='montant_log', by='is_fraud', ax=axes[1,1])
axes[1,1].set_title('ðŸ“¦ Montants par Classe')

# Scatter plot multidimensionnel
scatter = axes[1,2].scatter(df['ratio_montant_solde'], df['nb_trans_jour'], 
                           c=df['is_fraud'], cmap='RdYlBu', alpha=0.6)
axes[1,2].set_title('ðŸŽ¯ Ratio Montant/Solde vs Nb Trans')
axes[1,2].set_xlabel('Ratio Montant/Solde')
axes[1,2].set_ylabel('Nb Trans Jour')

# Matrice de corrÃ©lation
features_corr = ['montant_log', 'nb_trans_jour', 'solde_compte', 'age_compte_jours', 'ratio_montant_solde']
corr_matrix = df[features_corr + ['is_fraud']].corr()
im = axes[2,0].imshow(corr_matrix, cmap='coolwarm', aspect='auto')
axes[2,0].set_xticks(range(len(corr_matrix.columns)))
axes[2,0].set_yticks(range(len(corr_matrix.columns)))
axes[2,0].set_xticklabels(corr_matrix.columns, rotation=45)
axes[2,0].set_yticklabels(corr_matrix.columns)
axes[2,0].set_title('ðŸ”— Matrice de CorrÃ©lation')

# Distribution des ratios
axes[2,1].hist(df[df['is_fraud']==0]['ratio_montant_moyenne'], bins=50, alpha=0.7, label='Normal', density=True)
axes[2,1].hist(df[df['is_fraud']==1]['ratio_montant_moyenne'], bins=50, alpha=0.7, label='Fraude', density=True)
axes[2,1].set_title('ðŸ“Š Ratio Montant/Moyenne')
axes[2,1].legend()

# Analyse temporelle
night_fraud = df.groupby('is_night')['is_fraud'].mean()
axes[2,2].bar(['Jour', 'Nuit'], [night_fraud[0]*100, night_fraud[1]*100], 
             color=['lightblue', 'darkblue'], alpha=0.7)
axes[2,2].set_title('ðŸŒ™ Fraude Jour vs Nuit')
axes[2,2].set_ylabel('Taux de fraude (%)')

plt.tight_layout()
plt.show()

# 3. PRÃ‰PARATION AVANCÃ‰E DES DONNÃ‰ES
print("\\nâš™ï¸ PrÃ©paration des donnÃ©es pour la modÃ©lisation...")

# SÃ©lection des features
features = ['montant_log', 'heure', 'jour_semaine', 'nb_trans_jour', 'nb_trans_semaine',
           'solde_compte', 'age_compte_jours', 'merchant_category', 'pays_merchant',
           'ratio_montant_solde', 'ratio_montant_moyenne', 'is_weekend', 'is_night',
           'freq_trans_day_high']

X = df[features].copy()
y = df['is_fraud'].copy()

print(f"ðŸ“Š Features sÃ©lectionnÃ©es: {len(features)}")
print(f"ðŸŽ¯ Distribution des classes: {y.value_counts().to_dict()}")

# Division temporelle (important pour Ã©viter le data leakage)
# Simuler une division temporelle
split_index = int(0.7 * len(df))
X_temp_train, X_temp_test = X[:split_index], X[split_index:]
y_temp_train, y_temp_test = y[:split_index], y[split_index:]

# Division train/validation sur la partie temporelle d'entraÃ®nement
X_train, X_val, y_train, y_val = train_test_split(
    X_temp_train, y_temp_train, test_size=0.2, random_state=42, stratify=y_temp_train
)

print(f"ðŸ“ˆ Train: {len(X_train)} (fraude: {y_train.sum()}/{len(y_train)})")
print(f"ðŸ“Š Validation: {len(X_val)} (fraude: {y_val.sum()}/{len(y_val)})")
print(f"ðŸŽ¯ Test temporel: {len(X_temp_test)} (fraude: {y_temp_test.sum()}/{len(y_temp_test)})")

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_temp_test_scaled = scaler.transform(X_temp_test)

# 4. GESTION DU DÃ‰SÃ‰QUILIBRE
print("\\nâš–ï¸ Gestion du dÃ©sÃ©quilibre des classes...")

# SMOTE pour l'over-sampling
smote = SMOTE(random_state=42, k_neighbors=3)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"âœ… AprÃ¨s SMOTE: {len(X_train_smote)} Ã©chantillons")
print(f"ðŸ“Š Distribution: {pd.Series(y_train_smote).value_counts().to_dict()}")

# 5. MODÃ‰LISATION AVEC MULTIPLES ALGORITHMES
print("\\nðŸ¤– Test de multiple algorithmes de dÃ©tection...")

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, 
                                          class_weight='balanced'),
    'Random Forest SMOTE': RandomForestClassifier(n_estimators=100, random_state=42),
    'Isolation Forest': IsolationForest(contamination=0.002, random_state=42),
    'One-Class SVM': OneClassSVM(nu=0.002, kernel='rbf')
}

results = {}

for name, model in models.items():
    print(f"\\nðŸ”§ EntraÃ®nement {name}...")
    
    if name == 'Random Forest SMOTE':
        # Utiliser les donnÃ©es SMOTE
        model.fit(X_train_smote, y_train_smote)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_proba_val = model.predict_proba(X_val_scaled)[:, 1]
        
    elif name in ['Isolation Forest', 'One-Class SVM']:
        # Algorithmes non-supervisÃ©s - entraÃ®ner sur donnÃ©es normales uniquement
        X_train_normal = X_train_scaled[y_train == 0]
        model.fit(X_train_normal)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_val = [1 if x == -1 else 0 for x in y_pred_val]  # Conversion -1/1 vers 0/1
        y_pred_proba_val = model.decision_function(X_val_scaled)
        # Normaliser les scores de dÃ©cision pour avoir des pseudo-probabilitÃ©s
        y_pred_proba_val = (y_pred_proba_val - y_pred_proba_val.min()) / (y_pred_proba_val.max() - y_pred_proba_val.min())
        
    else:
        # Random Forest standard
        model.fit(X_train_scaled, y_train)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_proba_val = model.predict_proba(X_val_scaled)[:, 1]
    
    # MÃ©triques
    if name not in ['Isolation Forest', 'One-Class SVM']:
        auc_score = roc_auc_score(y_val, y_pred_proba_val)
        avg_precision = average_precision_score(y_val, y_pred_proba_val)
    else:
        auc_score = roc_auc_score(y_val, y_pred_proba_val)
        avg_precision = average_precision_score(y_val, y_pred_proba_val)
    
    results[name] = {
        'model': model,
        'predictions': y_pred_val,
        'probabilities': y_pred_proba_val,
        'auc': auc_score,
        'avg_precision': avg_precision
    }
    
    print(f"   ðŸŽ¯ AUC: {auc_score:.3f}")
    print(f"   ðŸ“Š Average Precision: {avg_precision:.3f}")
    
    # Rapport de classification
    print(f"   ðŸ“‹ Classification Report:")
    print(classification_report(y_val, y_pred_val, target_names=['Normal', 'Fraude']))

# 6. SÃ‰LECTION DU MEILLEUR MODÃˆLE
best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])
best_model = results[best_model_name]['model']

print(f"\\nðŸ† MEILLEUR MODÃˆLE: {best_model_name}")
print(f"ðŸŽ¯ AUC: {results[best_model_name]['auc']:.3f}")
print(f"ðŸ“Š Average Precision: {results[best_model_name]['avg_precision']:.3f}")

# 7. ANALYSE APPROFONDIE DU MEILLEUR MODÃˆLE
print("\\nðŸ“Š Analyse approfondie du meilleur modÃ¨le...")

best_pred = results[best_model_name]['predictions']
best_proba = results[best_model_name]['probabilities']

# Matrice de confusion
cm = confusion_matrix(y_val, best_pred)
print("\\nðŸŽ¯ Matrice de confusion:")
print(cm)

# Visualisations avancÃ©es
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle(f'ðŸ“ˆ Analyse du ModÃ¨le {best_model_name}', fontsize=14, fontweight='bold')

# ROC Curve
fpr, tpr, _ = roc_curve(y_val, best_proba)
axes[0,0].plot(fpr, tpr, linewidth=2, label=f'AUC = {results[best_model_name]["auc"]:.3f}')
axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
axes[0,0].set_xlabel('Taux de Faux Positifs')
axes[0,0].set_ylabel('Taux de Vrais Positifs')
axes[0,0].set_title('ðŸ“ˆ Courbe ROC')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_val, best_proba)
axes[0,1].plot(recall, precision, linewidth=2, label=f'AP = {results[best_model_name]["avg_precision"]:.3f}')
axes[0,1].set_xlabel('Rappel')
axes[0,1].set_ylabel('PrÃ©cision')
axes[0,1].set_title('ðŸŽ¯ Courbe PrÃ©cision-Rappel')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Distribution des scores
axes[1,0].hist(best_proba[y_val == 0], bins=50, alpha=0.7, label='Normal', density=True)
axes[1,0].hist(best_proba[y_val == 1], bins=50, alpha=0.7, label='Fraude', density=True)
axes[1,0].set_xlabel('Score de Fraude')
axes[1,0].set_ylabel('DensitÃ©')
axes[1,0].set_title('ðŸ“Š Distribution des Scores')
axes[1,0].legend()

# Matrice de confusion heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])
axes[1,1].set_title('ðŸŽ¯ Matrice de Confusion')
axes[1,1].set_xlabel('PrÃ©diction')
axes[1,1].set_ylabel('RÃ©alitÃ©')

plt.tight_layout()
plt.show()

# 8. OPTIMISATION DU SEUIL
print("\\nðŸŽ¯ Optimisation du seuil de dÃ©cision...")

# Calcul des mÃ©triques pour diffÃ©rents seuils
thresholds = np.arange(0.1, 0.9, 0.05)
metrics_by_threshold = []

for threshold in thresholds:
    y_pred_thresh = (best_proba >= threshold).astype(int)
    
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred_thresh).ravel()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    metrics_by_threshold.append({
        'threshold': threshold,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'fpr': false_positive_rate
    })

metrics_df = pd.DataFrame(metrics_by_threshold)

# Trouver le seuil optimal (maximiser F1 ou selon critÃ¨re mÃ©tier)
optimal_threshold = metrics_df.loc[metrics_df['f1'].idxmax(), 'threshold']
print(f"ðŸŽ¯ Seuil optimal (F1): {optimal_threshold:.2f}")

# Appliquer le seuil optimal
y_pred_optimal = (best_proba >= optimal_threshold).astype(int)
print("\\nðŸ“‹ Performance avec seuil optimal:")
print(classification_report(y_val, y_pred_optimal, target_names=['Normal', 'Fraude']))

# 9. TEST SUR DONNÃ‰ES TEMPORELLES
print("\\nðŸ• Test sur donnÃ©es temporelles (simulation production)...")

if best_model_name == 'Random Forest SMOTE':
    y_pred_temporal = best_model.predict(X_temp_test_scaled)
    y_proba_temporal = best_model.predict_proba(X_temp_test_scaled)[:, 1]
elif best_model_name in ['Isolation Forest', 'One-Class SVM']:
    y_pred_temporal_raw = best_model.predict(X_temp_test_scaled)
    y_pred_temporal = [1 if x == -1 else 0 for x in y_pred_temporal_raw]
    y_proba_temporal = best_model.decision_function(X_temp_test_scaled)
    y_proba_temporal = (y_proba_temporal - y_proba_temporal.min()) / (y_proba_temporal.max() - y_proba_temporal.min())
else:
    y_pred_temporal = best_model.predict(X_temp_test_scaled)
    y_proba_temporal = best_model.predict_proba(X_temp_test_scaled)[:, 1]

# Appliquer le seuil optimal
y_pred_temporal_optimal = (y_proba_temporal >= optimal_threshold).astype(int)

print("ðŸ“Š Performance sur donnÃ©es temporelles:")
print(classification_report(y_temp_test, y_pred_temporal_optimal, target_names=['Normal', 'Fraude']))

temporal_auc = roc_auc_score(y_temp_test, y_proba_temporal)
print(f"ðŸŽ¯ AUC temporel: {temporal_auc:.3f}")

print("\\nâœ… SYSTÃˆME DE DÃ‰TECTION DE FRAUDE OPÃ‰RATIONNEL!")
print("ðŸš€ PrÃªt pour le dÃ©ploiement en temps rÃ©el avec monitoring!")`})]}),e.jsxs("div",{className:"grid grid-cols-1 lg:grid-cols-2 gap-8",children:[e.jsx(M,{question:"Dans un problÃ¨me de classification avec des classes trÃ¨s dÃ©sÃ©quilibrÃ©es (99% classe A, 1% classe B), quelle stratÃ©gie d'Ã©valuation est la plus appropriÃ©e ?",options:["Utiliser uniquement l'accuracy globale","Se concentrer sur la prÃ©cision de la classe majoritaire","Utiliser l'AUC-ROC et l'Average Precision avec validation stratifiÃ©e","Ã‰quilibrer artificiellement le dataset de test"],correctAnswer:2,explanation:"Pour les classes dÃ©sÃ©quilibrÃ©es, l'AUC-ROC et l'Average Precision sont les mÃ©triques les plus robustes car elles Ã©valuent la capacitÃ© du modÃ¨le Ã  distinguer les classes indÃ©pendamment du seuil de dÃ©cision. La validation stratifiÃ©e preserve la proportion des classes dans chaque fold.",difficulty:"difficile"}),e.jsx(M,{question:"Qu'est-ce qui caractÃ©rise le mieux le surapprentissage (overfitting) dans un modÃ¨le de Machine Learning ?",options:["Le modÃ¨le a une faible prÃ©cision sur les donnÃ©es d'entraÃ®nement et de test","Le modÃ¨le a une haute prÃ©cision sur l'entraÃ®nement mais faible sur le test","Le modÃ¨le prend trop de temps Ã  s'entraÃ®ner","Le modÃ¨le a besoin de plus de donnÃ©es d'entraÃ®nement"],correctAnswer:1,explanation:"Le surapprentissage se caractÃ©rise par un modÃ¨le qui 'mÃ©morise' les donnÃ©es d'entraÃ®nement au lieu d'apprendre des patterns gÃ©nÃ©ralisables. Il obtient donc d'excellents rÃ©sultats sur les donnÃ©es qu'il a vues (entraÃ®nement) mais Ã©choue sur de nouvelles donnÃ©es (test). C'est le signe d'un modÃ¨le trop complexe pour la quantitÃ© de donnÃ©es disponible.",difficulty:"moyen"})]}),e.jsx(u,{title:"ðŸŽ¯ Projet GuidÃ© : SystÃ¨me de Recommandation E-commerce",type:"exercice",children:e.jsxs("div",{className:"space-y-8",children:[e.jsxs("div",{className:"text-center",children:[e.jsx("p",{className:"text-xl font-medium mb-4",children:"ðŸš€ CrÃ©ons ensemble un systÃ¨me de recommandation complet pour une plateforme e-commerce !"}),e.jsx("p",{className:"text-gray-600",children:"Projet fil rouge avec donnÃ©es rÃ©elles, dÃ©fis techniques et impact business"})]}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-blue-50 to-blue-100 p-6 rounded-xl border-2 border-blue-200 hover:shadow-lg transition-all duration-300",children:[e.jsx("div",{className:"text-2xl mb-3",children:"ðŸ“Š"}),e.jsx("h4",{className:"font-bold text-blue-800 mb-2",children:"Ã‰tape 1 : Analyse des DonnÃ©es"}),e.jsxs("div",{className:"text-sm space-y-2",children:[e.jsx("p",{children:e.jsx("strong",{children:"DonnÃ©es :"})}),e.jsxs("ul",{className:"list-disc pl-4 space-y-1",children:[e.jsx("li",{children:"1M+ interactions utilisateur-produit"}),e.jsx("li",{children:"100k utilisateurs, 50k produits"}),e.jsx("li",{children:"MÃ©tadonnÃ©es : catÃ©gories, prix, marques"}),e.jsx("li",{children:"Historique temporel sur 2 ans"})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-br from-green-50 to-green-100 p-6 rounded-xl border-2 border-green-200 hover:shadow-lg transition-all duration-300",children:[e.jsx("div",{className:"text-2xl mb-3",children:"ðŸ”§"}),e.jsx("h4",{className:"font-bold text-green-800 mb-2",children:"Ã‰tape 2 : Feature Engineering"}),e.jsxs("div",{className:"text-sm space-y-2",children:[e.jsx("p",{children:e.jsx("strong",{children:"Features avancÃ©es :"})}),e.jsxs("ul",{className:"list-disc pl-4 space-y-1",children:[e.jsx("li",{children:"Embeddings produits (Word2Vec)"}),e.jsx("li",{children:"Patterns temporels saisonniers"}),e.jsx("li",{children:"SimilaritÃ© collaborative"}),e.jsx("li",{children:"Profils comportementaux"})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-br from-purple-50 to-purple-100 p-6 rounded-xl border-2 border-purple-200 hover:shadow-lg transition-all duration-300",children:[e.jsx("div",{className:"text-2xl mb-3",children:"ðŸ¤–"}),e.jsx("h4",{className:"font-bold text-purple-800 mb-2",children:"Ã‰tape 3 : ModÃ©lisation"}),e.jsxs("div",{className:"text-sm space-y-2",children:[e.jsx("p",{children:e.jsx("strong",{children:"Approches :"})}),e.jsxs("ul",{className:"list-disc pl-4 space-y-1",children:[e.jsx("li",{children:"Matrix Factorization (SVD++)"}),e.jsx("li",{children:"Deep Learning (Neural CF)"}),e.jsx("li",{children:"Ensemble hybride"}),e.jsx("li",{children:"Real-time serving"})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-br from-red-50 to-red-100 p-6 rounded-xl border-2 border-red-200 hover:shadow-lg transition-all duration-300",children:[e.jsx("div",{className:"text-2xl mb-3",children:"ðŸ“ˆ"}),e.jsx("h4",{className:"font-bold text-red-800 mb-2",children:"Ã‰tape 4 : Ã‰valuation Business"}),e.jsxs("div",{className:"text-sm space-y-2",children:[e.jsx("p",{children:e.jsx("strong",{children:"MÃ©triques :"})}),e.jsxs("ul",{className:"list-disc pl-4 space-y-1",children:[e.jsx("li",{children:"CTR (Click-Through Rate)"}),e.jsx("li",{children:"Conversion rate"}),e.jsx("li",{children:"Revenue lift"}),e.jsx("li",{children:"A/B testing"})]})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-r from-indigo-100 to-purple-100 p-8 rounded-xl border-2 border-indigo-200",children:[e.jsx("h4",{className:"font-bold text-indigo-800 mb-6 text-xl text-center",children:"ðŸŽ¯ DÃ©fis Techniques Ã  RÃ©soudre"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-6",children:[e.jsxs("div",{className:"bg-white p-6 rounded-lg shadow-lg",children:[e.jsx("h5",{className:"font-bold text-red-600 mb-3",children:"ðŸš¨ Cold Start Problem"}),e.jsx("p",{className:"text-sm mb-3",children:"Comment recommander Ã  un nouvel utilisateur sans historique ?"}),e.jsx("div",{className:"bg-red-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Solution :"})," Utiliser les mÃ©tadonnÃ©es, donnÃ©es dÃ©mographiques et popularity-based recommendations"]})})]}),e.jsxs("div",{className:"bg-white p-6 rounded-lg shadow-lg",children:[e.jsx("h5",{className:"font-bold text-blue-600 mb-3",children:"âš¡ ScalabilitÃ©"}),e.jsx("p",{className:"text-sm mb-3",children:"Servir 10k+ requÃªtes/seconde en temps rÃ©el ?"}),e.jsx("div",{className:"bg-blue-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Solution :"})," Pre-computing, caching, approximate algorithms (LSH)"]})})]}),e.jsxs("div",{className:"bg-white p-6 rounded-lg shadow-lg",children:[e.jsx("h5",{className:"font-bold text-green-600 mb-3",children:"ðŸŽ­ DiversitÃ© vs PrÃ©cision"}),e.jsx("p",{className:"text-sm mb-3",children:"Ã‰quilibrer recommendations prÃ©cises et dÃ©couverte ?"}),e.jsx("div",{className:"bg-green-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Solution :"})," Multi-objective optimization, exploration vs exploitation"]})})]}),e.jsxs("div",{className:"bg-white p-6 rounded-lg shadow-lg",children:[e.jsx("h5",{className:"font-bold text-purple-600 mb-3",children:"ðŸ“Š DonnÃ©es Sparse"}),e.jsx("p",{className:"text-sm mb-3",children:"99%+ de la matrice user-item est vide ?"}),e.jsx("div",{className:"bg-purple-50 p-3 rounded",children:e.jsxs("p",{className:"text-xs",children:[e.jsx("strong",{children:"Solution :"})," Matrix factorization, implicit feedback, side information"]})})]})]})]}),e.jsxs("div",{className:"bg-gradient-to-r from-yellow-100 to-orange-100 p-8 rounded-xl border-2 border-yellow-200",children:[e.jsx("h4",{className:"font-bold text-orange-800 mb-4 text-center",children:"ðŸŽŠ Challenge Bonus : Impact Business"}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-4 text-center",children:[e.jsxs("div",{className:"bg-white p-6 rounded-lg",children:[e.jsx("div",{className:"text-3xl font-bold text-green-600",children:"+23%"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Augmentation des ventes"})]}),e.jsxs("div",{className:"bg-white p-6 rounded-lg",children:[e.jsx("div",{className:"text-3xl font-bold text-blue-600",children:"+35%"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Engagement utilisateur"})]}),e.jsxs("div",{className:"bg-white p-6 rounded-lg",children:[e.jsx("div",{className:"text-3xl font-bold text-purple-600",children:"â‚¬2.1M"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Revenue supplÃ©mentaire annuel"})]})]}),e.jsxs("p",{className:"text-center mt-4 text-sm text-gray-700",children:[e.jsx("strong",{children:"Objectif :"})," ImplÃ©menter et dÃ©ployer un systÃ¨me qui gÃ©nÃ¨re un ROI mesurable ! ðŸš€"]})]})]})}),e.jsx(u,{title:"ðŸ› ï¸ Ressources et Outils Professionnels",type:"rappel",children:e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6",children:[e.jsxs("div",{className:"bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300",children:[e.jsxs("h4",{className:"font-bold text-blue-600 mb-3 flex items-center gap-2",children:[e.jsx(S,{className:"h-5 w-5"}),"Notebooks Jupyter"]}),e.jsx("p",{className:"text-sm text-gray-600 mb-3",children:"Environnement interactif pour l'expÃ©rimentation"}),e.jsxs("div",{className:"flex items-center gap-2",children:[e.jsx(q,{className:"h-4 w-4 text-blue-500"}),e.jsx("a",{href:"https://jupyter.org",className:"text-blue-500 text-sm hover:underline",children:"jupyter.org"})]})]}),e.jsxs("div",{className:"bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300",children:[e.jsxs("h4",{className:"font-bold text-green-600 mb-3 flex items-center gap-2",children:[e.jsx(A,{className:"h-5 w-5"}),"MLflow"]}),e.jsx("p",{className:"text-sm text-gray-600 mb-3",children:"Suivi et gestion des expÃ©riences ML"}),e.jsxs("div",{className:"flex items-center gap-2",children:[e.jsx(q,{className:"h-4 w-4 text-green-500"}),e.jsx("a",{href:"https://mlflow.org",className:"text-green-500 text-sm hover:underline",children:"mlflow.org"})]})]}),e.jsxs("div",{className:"bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300",children:[e.jsxs("h4",{className:"font-bold text-purple-600 mb-3 flex items-center gap-2",children:[e.jsx(v,{className:"h-5 w-5"}),"TensorBoard"]}),e.jsx("p",{className:"text-sm text-gray-600 mb-3",children:"Visualisation des mÃ©triques d'entraÃ®nement"}),e.jsxs("div",{className:"flex items-center gap-2",children:[e.jsx(q,{className:"h-4 w-4 text-purple-500"}),e.jsx("a",{href:"https://tensorboard.dev",className:"text-purple-500 text-sm hover:underline",children:"tensorboard.dev"})]})]})]})})]})},Se=()=>{const{activeSection:s}=he();return e.jsxs("div",{className:"space-y-16",children:[e.jsx("div",{className:"block",children:e.jsx(_e,{})}),e.jsx("div",{className:"block",children:e.jsx(ge,{})}),e.jsx("div",{className:"block",children:e.jsx(fe,{})}),e.jsx("div",{className:"block",children:e.jsx(be,{})}),e.jsx("div",{className:"block",children:e.jsx(je,{})}),e.jsx("div",{className:"block",children:e.jsx(Ne,{})}),e.jsx("div",{className:"block",children:e.jsx(ye,{})})]})},cs=()=>{const s=["introduction","advanced-courses","supervised","unsupervised","evaluation","deep-learning","exercises"],{currentSection:a}=de(s);ce();const b=[{title:"Introduction au ML",href:"#introduction",isActive:a==="introduction",icon:e.jsx(X,{className:"h-4 w-4"})},{title:"Cours Approfondis",href:"#advanced-courses",isActive:a==="advanced-courses",icon:e.jsx(me,{className:"h-4 w-4"})},{title:"Apprentissage supervisÃ©",href:"#supervised",isActive:a==="supervised",icon:e.jsx(K,{className:"h-4 w-4"})},{title:"Apprentissage non supervisÃ©",href:"#unsupervised",isActive:a==="unsupervised",icon:e.jsx(H,{className:"h-4 w-4"})},{title:"Ã‰valuation des modÃ¨les",href:"#evaluation",isActive:a==="evaluation",icon:e.jsx(ue,{className:"h-4 w-4"})},{title:"Deep Learning",href:"#deep-learning",isActive:a==="deep-learning",icon:e.jsx(pe,{className:"h-4 w-4"})},{title:"Exercices pratiques",href:"#practical-exercises",isActive:a==="exercises",icon:e.jsx(S,{className:"h-4 w-4"})}];return e.jsx(xe,{children:e.jsx(Z,{title:"Machine Learning",backLink:{href:"/tools",label:"Retour aux outils"},sidebar:{items:b},children:e.jsxs("section",{className:"py-8",children:[e.jsx(oe,{variant:"page",title:"Machine Learning",description:"MaÃ®trisez l'art de l'apprentissage automatique avec des cours interactifs, des exercices pratiques et des projets concrets.",icon:X}),e.jsx(Se,{})]})})})};export{cs as default};
