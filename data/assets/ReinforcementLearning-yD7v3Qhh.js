import{c as q,r as N,j as e,C as t,a,b as i,e as d}from"./index-jatdNhFh.js";import{a as f,L}from"./Layout-CR8zGZt_.js";import{B as r}from"./badge-D3N1h19b.js";import{T as C,a as S,b as o,c}from"./tabs-DyU7YeQV.js";import{E as l,Q as _,a as w,P}from"./educational-cards-DKc7pKQa.js";import{a as g,b as j,d as b}from"./collapsible-CoiC7cn5.js";import{B as u}from"./brain-e1e4SMeD.js";import{T as x}from"./target-Cgxv7nHm.js";import{Z as v}from"./zap-Df3zai_1.js";import{U as z}from"./users-DpOE6_sU.js";import{T as A}from"./trophy-CY7xpX6y.js";import{A as k,P as R}from"./ProjectsSection-oGiZ5pbo.js";import{G as y}from"./gamepad-2-D1U1A8bq.js";import{T}from"./trending-up-gTw2Lzr0.js";import{F as D,R as E}from"./ResourcesSection-DBW5mlgk.js";import{H as I}from"./heart-CxBPzBS_.js";import{S as Q}from"./shield-1LxtYfpT.js";import{G as O}from"./globe-DnXimFjv.js";import{C as B}from"./code-B5o97cHw.js";import{C as M}from"./CourseBreadcrumb-CLjCCpjS.js";import{U as F}from"./unified-hero-section-_j3EuOk5.js";import"./chart-column-D8DiT-jd.js";import"./circle-check-big-DcUkgBGn.js";import"./circle-alert-Bf4zJKID.js";import"./book-open-BGQhZmI6.js";import"./lightbulb-D5sQnwCc.js";import"./eye-CW1j5p4w.js";import"./git-branch-C09JVIQv.js";import"./star-D_bvZ9E6.js";import"./external-link-CTEWBqNU.js";import"./book-eIDimuOF.js";import"./video-DJZf6Qay.js";/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const G=q("Car",[["path",{d:"M19 17h2c.6 0 1-.4 1-1v-3c0-.9-.7-1.7-1.5-1.9C18.7 10.6 16 10 16 10s-1.3-1.4-2.2-2.3c-.5-.4-1.1-.7-1.8-.7H5c-.6 0-1.1.4-1.4.9l-1.4 2.9A3.7 3.7 0 0 0 2 12v4c0 .6.4 1 1 1h2",key:"5owen"}],["circle",{cx:"7",cy:"17",r:"2",key:"u2ysq9"}],["path",{d:"M9 17h6",key:"r8uit2"}],["circle",{cx:"17",cy:"17",r:"2",key:"axvx0g"}]]),U=()=>{const[s,n]=N.useState(!1);return e.jsxs("div",{className:"space-y-8",children:[e.jsxs(t,{className:"bg-gradient-to-r from-purple-50 to-indigo-50 border-l-4 border-l-purple-500",children:[e.jsx(a,{children:e.jsxs(i,{className:"flex items-center gap-3 text-2xl",children:[e.jsx(u,{className:"h-8 w-8 text-purple-600"}),"Bienvenue dans l'Apprentissage par Renforcement"]})}),e.jsxs(d,{className:"space-y-6",children:[e.jsx("p",{className:"text-lg text-gray-700 leading-relaxed",children:"Imaginez que vous apprenez √† conduire : vous essayez diff√©rentes actions, vous recevez des retours (positifs ou n√©gatifs), et vous ajustez votre comportement. C'est exactement ainsi que fonctionne l'apprentissage par renforcement ! üöó"}),e.jsxs("div",{className:"bg-white p-6 rounded-xl border shadow-sm",children:[e.jsx("h3",{className:"font-semibold mb-4 text-purple-800",children:"üéØ Ce que vous allez d√©couvrir :"}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(x,{className:"h-5 w-5 text-purple-600 mt-1"}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Les fondamentaux"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Agent, environnement, r√©compenses"})]})]}),e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(v,{className:"h-5 w-5 text-purple-600 mt-1"}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Les algorithmes"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Q-Learning, Policy Gradient, DQN"})]})]}),e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(z,{className:"h-5 w-5 text-purple-600 mt-1"}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Applications r√©elles"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Jeux, robotique, finance"})]})]}),e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(u,{className:"h-5 w-5 text-purple-600 mt-1"}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-medium",children:"Projets pratiques"}),e.jsx("p",{className:"text-sm text-gray-600",children:"Impl√©mentations concr√®tes"})]})]})]})]})]})]}),e.jsx(l,{title:"üêï Analogie : Dresser un chien intelligent",type:"analogie",children:e.jsxs("div",{className:"space-y-4",children:[e.jsx("p",{children:"L'apprentissage par renforcement, c'est comme dresser un chien tr√®s intelligent :"}),e.jsx("div",{className:"bg-gradient-to-r from-blue-50 to-indigo-50 p-6 rounded-xl space-y-4",children:e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-2",children:"üêï Dressage traditionnel"}),e.jsxs("ul",{className:"space-y-2 text-sm",children:[e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Chien"})," : apprend par essai-erreur"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Ma√Ætre"})," : donne r√©compenses/punitions"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Environnement"})," : la maison, le parc"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Objectif"})," : maximiser les friandises"]})]})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-purple-800 mb-2",children:"ü§ñ Apprentissage par renforcement"}),e.jsxs("ul",{className:"space-y-2 text-sm",children:[e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Agent"})," : apprend par essai-erreur"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Fonction de r√©compense"})," : donne des signaux"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Environnement"})," : le syst√®me √† contr√¥ler"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Objectif"})," : maximiser la r√©compense cumulative"]})]})]})]})})]})}),e.jsxs(t,{className:"border-2 border-indigo-200",children:[e.jsx(a,{children:e.jsx(i,{className:"text-center",children:"Le Cycle de l'Apprentissage par Renforcement"})}),e.jsx(d,{children:e.jsx("div",{className:"flex justify-center",children:e.jsxs("svg",{width:"600",height:"400",viewBox:"0 0 600 400",className:"max-w-full h-auto",children:[e.jsx("rect",{x:"50",y:"50",width:"150",height:"100",rx:"15",fill:"#e0f2fe",stroke:"#0369a1",strokeWidth:"2"}),e.jsx("text",{x:"125",y:"90",textAnchor:"middle",className:"font-semibold",fill:"#0369a1",children:"Environnement"}),e.jsx("text",{x:"125",y:"110",textAnchor:"middle",className:"text-sm",fill:"#0369a1",children:"√âtat s(t)"}),e.jsx("rect",{x:"400",y:"250",width:"150",height:"100",rx:"15",fill:"#f3e8ff",stroke:"#7c3aed",strokeWidth:"2"}),e.jsx("text",{x:"475",y:"290",textAnchor:"middle",className:"font-semibold",fill:"#7c3aed",children:"Agent"}),e.jsx("text",{x:"475",y:"310",textAnchor:"middle",className:"text-sm",fill:"#7c3aed",children:"Politique œÄ"}),e.jsx("defs",{children:e.jsx("marker",{id:"arrowhead",markerWidth:"10",markerHeight:"7",refX:"9",refY:"3.5",orient:"auto",children:e.jsx("polygon",{points:"0 0, 10 3.5, 0 7",fill:"#374151"})})}),e.jsx("path",{d:"M 200 100 Q 300 50 400 280",stroke:"#374151",strokeWidth:"2",fill:"none",markerEnd:"url(#arrowhead)"}),e.jsx("text",{x:"280",y:"140",textAnchor:"middle",className:"text-sm font-medium",fill:"#374151",children:"√âtat s(t)"}),e.jsx("path",{d:"M 400 300 Q 300 350 200 120",stroke:"#374151",strokeWidth:"2",fill:"none",markerEnd:"url(#arrowhead)"}),e.jsx("text",{x:"320",y:"320",textAnchor:"middle",className:"text-sm font-medium",fill:"#374151",children:"Action a(t)"}),e.jsx("path",{d:"M 180 150 Q 250 200 380 280",stroke:"#dc2626",strokeWidth:"3",fill:"none",markerEnd:"url(#arrowhead)"}),e.jsx("text",{x:"280",y:"220",textAnchor:"middle",className:"text-sm font-bold",fill:"#dc2626",children:"R√©compense r(t+1)"}),e.jsx("rect",{x:"50",y:"320",width:"300",height:"60",rx:"10",fill:"#f9fafb",stroke:"#d1d5db"}),e.jsx("text",{x:"60",y:"340",className:"text-sm font-semibold",fill:"#374151",children:"Cycle d'apprentissage :"}),e.jsx("text",{x:"60",y:"355",className:"text-xs",fill:"#6b7280",children:"1. L'agent observe l'√©tat"}),e.jsx("text",{x:"60",y:"370",className:"text-xs",fill:"#6b7280",children:"2. Il choisit une action selon sa politique"}),e.jsx("text",{x:"200",y:"355",className:"text-xs",fill:"#6b7280",children:"3. L'environnement change d'√©tat"}),e.jsx("text",{x:"200",y:"370",className:"text-xs",fill:"#6b7280",children:"4. Une r√©compense est donn√©e"})]})})})]}),e.jsx(_,{question:"Dans l'analogie du dressage du chien, qu'est-ce qui correspond √† la 'politique' en apprentissage par renforcement ?",options:["Les friandises donn√©es au chien","La strat√©gie que le chien d√©veloppe pour obtenir des r√©compenses","L'environnement dans lequel √©volue le chien","Le ma√Ætre qui dresse le chien"],correctAnswer:1,explanation:"La politique correspond √† la strat√©gie d√©velopp√©e par le chien (l'agent) pour d√©cider quelles actions entreprendre dans chaque situation afin de maximiser ses r√©compenses. C'est son 'plan d'action' appris au fil du temps.",difficulty:"moyen"}),e.jsxs(g,{open:s,onOpenChange:n,children:[e.jsx(j,{className:"w-full",children:e.jsx(t,{className:"hover:shadow-lg transition-all duration-300 cursor-pointer",children:e.jsx(a,{children:e.jsxs(i,{className:"flex items-center justify-between",children:[e.jsx("span",{className:"flex items-center gap-2",children:"üìö Histoire fascinante de l'Apprentissage par Renforcement"}),e.jsx(f,{className:`h-5 w-5 transition-transform ${s?"rotate-180":""}`})]})})})}),e.jsx(b,{children:e.jsx(t,{className:"mt-2 bg-gradient-to-r from-amber-50 to-orange-50",children:e.jsx(d,{className:"pt-6 space-y-4",children:e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-amber-800 mb-3",children:"üï∞Ô∏è Chronologie cl√©"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"border-l-4 border-amber-400 pl-4",children:[e.jsx(r,{className:"mb-1",children:"1950s"}),e.jsx("p",{className:"text-sm",children:"Richard Bellman d√©veloppe la programmation dynamique"})]}),e.jsxs("div",{className:"border-l-4 border-amber-400 pl-4",children:[e.jsx(r,{className:"mb-1",children:"1980s"}),e.jsx("p",{className:"text-sm",children:"Sutton & Barto formalisent le Q-Learning"})]}),e.jsxs("div",{className:"border-l-4 border-amber-400 pl-4",children:[e.jsx(r,{className:"mb-1",children:"2013"}),e.jsx("p",{className:"text-sm",children:"DeepMind r√©volutionne avec DQN (Atari)"})]}),e.jsxs("div",{className:"border-l-4 border-amber-400 pl-4",children:[e.jsx(r,{className:"mb-1",children:"2016"}),e.jsx("p",{className:"text-sm",children:"AlphaGo bat le champion mondial de Go"})]})]})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-amber-800 mb-3",children:"üí° Anecdotes passionnantes"}),e.jsxs("div",{className:"space-y-3 text-sm",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"üéÆ Les jeux Atari :"})," DeepMind a utilis√© de vieux jeux des ann√©es 80 pour prouver que l'IA pouvait apprendre sans r√®gles pr√©d√©finies !"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"üêÅ Les souris de laboratoire :"})," Les exp√©riences de Skinner sur les rats ont inspir√© les algorithmes de r√©compense moderne."]}),e.jsxs("p",{children:[e.jsx("strong",{children:"üöÅ H√©licopt√®res autonomes :"})," Stanford a fait voler des h√©licopt√®res RC en 3D gr√¢ce au RL d√®s 2007 !"]})]})]})]})})})})]})]})},W=()=>{const[s,n]=N.useState({}),h=m=>{n(p=>({...p,[m]:!p[m]}))};return e.jsxs("div",{className:"space-y-8",children:[e.jsxs(l,{title:"üß© Les Pi√®ces du Puzzle RL",type:"concept",children:[e.jsx("p",{className:"mb-4",children:`Comme un jeu vid√©o, l'apprentissage par renforcement a ses "r√®gles du jeu". D√©couvrons les 4 √©l√©ments essentiels qui constituent ce monde fascinant !`}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400",children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-2",children:"ü§ñ Agent"}),e.jsx("p",{className:"text-sm",children:'Le "joueur" qui prend des d√©cisions et apprend'})]}),e.jsxs("div",{className:"bg-green-50 p-4 rounded-lg border-l-4 border-green-400",children:[e.jsx("h4",{className:"font-semibold text-green-800 mb-2",children:"üåç Environnement"}),e.jsx("p",{className:"text-sm",children:`Le "terrain de jeu" dans lequel √©volue l'agent`})]}),e.jsxs("div",{className:"bg-purple-50 p-4 rounded-lg border-l-4 border-purple-400",children:[e.jsx("h4",{className:"font-semibold text-purple-800 mb-2",children:"‚ö° Actions"}),e.jsx("p",{className:"text-sm",children:`Les "mouvements" possibles de l'agent`})]}),e.jsxs("div",{className:"bg-orange-50 p-4 rounded-lg border-l-4 border-orange-400",children:[e.jsx("h4",{className:"font-semibold text-orange-800 mb-2",children:"üéÅ R√©compenses"}),e.jsx("p",{className:"text-sm",children:'Les "points" gagn√©s ou perdus apr√®s chaque action'})]})]})]}),e.jsxs(g,{open:s.agent,onOpenChange:()=>h("agent"),children:[e.jsx(j,{className:"w-full",children:e.jsx(t,{className:"hover:shadow-lg transition-all duration-300 cursor-pointer",children:e.jsx(a,{children:e.jsxs(i,{className:"flex items-center justify-between",children:[e.jsxs("span",{className:"flex items-center gap-2",children:[e.jsx(u,{className:"h-6 w-6 text-blue-600"}),"L'Agent : Le Cerveau de l'Op√©ration"]}),e.jsx(f,{className:`h-5 w-5 transition-transform ${s.agent?"rotate-180":""}`})]})})})}),e.jsx(b,{children:e.jsx(t,{className:"mt-2 bg-gradient-to-r from-blue-50 to-indigo-50",children:e.jsxs(d,{className:"pt-6 space-y-6",children:[e.jsx("div",{className:"text-center",children:e.jsxs("svg",{width:"500",height:"300",viewBox:"0 0 500 300",className:"max-w-full h-auto mx-auto",children:[e.jsx("circle",{cx:"250",cy:"150",r:"60",fill:"#3b82f6",stroke:"#1e40af",strokeWidth:"3"}),e.jsx("text",{x:"250",y:"155",textAnchor:"middle",className:"font-bold text-white",fontSize:"16",children:"AGENT"}),e.jsx("rect",{x:"100",y:"50",width:"80",height:"40",rx:"8",fill:"#dbeafe",stroke:"#3b82f6"}),e.jsx("text",{x:"140",y:"75",textAnchor:"middle",className:"text-sm font-medium",fill:"#1e40af",children:"Perception"}),e.jsx("rect",{x:"320",y:"50",width:"80",height:"40",rx:"8",fill:"#dbeafe",stroke:"#3b82f6"}),e.jsx("text",{x:"360",y:"75",textAnchor:"middle",className:"text-sm font-medium",fill:"#1e40af",children:"Politique"}),e.jsx("rect",{x:"100",y:"230",width:"80",height:"40",rx:"8",fill:"#dbeafe",stroke:"#3b82f6"}),e.jsx("text",{x:"140",y:"255",textAnchor:"middle",className:"text-sm font-medium",fill:"#1e40af",children:"M√©moire"}),e.jsx("rect",{x:"320",y:"230",width:"80",height:"40",rx:"8",fill:"#dbeafe",stroke:"#3b82f6"}),e.jsx("text",{x:"360",y:"255",textAnchor:"middle",className:"text-sm font-medium",fill:"#1e40af",children:"Apprentissage"}),e.jsx("defs",{children:e.jsx("marker",{id:"blueArrow",markerWidth:"10",markerHeight:"7",refX:"9",refY:"3.5",orient:"auto",children:e.jsx("polygon",{points:"0 0, 10 3.5, 0 7",fill:"#3b82f6"})})}),e.jsx("line",{x1:"180",y1:"70",x2:"220",y2:"120",stroke:"#3b82f6",strokeWidth:"2",markerEnd:"url(#blueArrow)"}),e.jsx("line",{x1:"280",y1:"120",x2:"320",y2:"70",stroke:"#3b82f6",strokeWidth:"2",markerEnd:"url(#blueArrow)"}),e.jsx("line",{x1:"220",y1:"190",x2:"180",y2:"230",stroke:"#3b82f6",strokeWidth:"2",markerEnd:"url(#blueArrow)"}),e.jsx("line",{x1:"280",y1:"190",x2:"320",y2:"230",stroke:"#3b82f6",strokeWidth:"2",markerEnd:"url(#blueArrow)"})]})}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-3",children:"üß† Composants de l'Agent"}),e.jsxs("div",{className:"space-y-3",children:[e.jsxs("div",{className:"p-3 bg-white rounded-lg border",children:[e.jsx("strong",{children:"Perception :"}),` Comment l'agent "voit" son environnement`]}),e.jsxs("div",{className:"p-3 bg-white rounded-lg border",children:[e.jsx("strong",{children:"Politique :"})," Sa strat√©gie pour choisir les actions"]}),e.jsxs("div",{className:"p-3 bg-white rounded-lg border",children:[e.jsx("strong",{children:"M√©moire :"})," Ce qu'il retient de ses exp√©riences"]}),e.jsxs("div",{className:"p-3 bg-white rounded-lg border",children:[e.jsx("strong",{children:"Apprentissage :"})," Comment il am√©liore sa politique"]})]})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-3",children:"üéÆ Exemple : Pac-Man IA"}),e.jsxs("div",{className:"space-y-2 text-sm",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Perception :"})," Position des fant√¥mes, des pastilles"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Politique :"}),' "Si fant√¥me proche ‚Üí fuir, sinon ‚Üí chercher pastilles"']}),e.jsxs("p",{children:[e.jsx("strong",{children:"M√©moire :"})," Quelles strat√©gies ont fonctionn√©"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Apprentissage :"})," Ajuster la strat√©gie apr√®s chaque partie"]})]})]})]})]})})})]}),e.jsxs(l,{title:"üåç Safari des Environnements RL",type:"saviez-vous",children:[e.jsx("p",{className:"mb-4",children:"Tous les environnements ne se ressemblent pas ! Comme les animaux s'adaptent √† leur habitat, les agents RL doivent s'adapter √† diff√©rents types d'environnements."}),e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"grid md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-gradient-to-r from-green-100 to-emerald-100 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-green-800 mb-2",children:"üéØ D√©terministe vs Stochastique"}),e.jsxs("p",{className:"text-sm mb-2",children:[e.jsx("strong",{children:"D√©terministe :"})," M√™me action = m√™me r√©sultat (√©checs)"]}),e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"Stochastique :"})," R√©sultat avec probabilit√©s (poker)"]})]}),e.jsxs("div",{className:"bg-gradient-to-r from-blue-100 to-cyan-100 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-2",children:"üëÅÔ∏è Observable vs Partiel"}),e.jsxs("p",{className:"text-sm mb-2",children:[e.jsx("strong",{children:"Observable :"})," On voit tout (Tetris)"]}),e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"Partiel :"})," Information limit√©e (Bataille navale)"]})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border-2 border-dashed border-gray-300",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"üèÜ Challenge : Classez ces jeux !"}),e.jsxs("div",{className:"grid md:grid-cols-3 gap-3 text-sm",children:[e.jsx(r,{variant:"outline",children:"üéÆ Super Mario"}),e.jsx(r,{variant:"outline",children:"‚ô†Ô∏è Blackjack"}),e.jsx(r,{variant:"outline",children:"üöó Conduite autonome"})]}),e.jsx("p",{className:"text-xs text-gray-600 mt-2",children:"R√©ponse : Mario (d√©terministe, observable), Blackjack (stochastique, partiel), Conduite (stochastique, partiel)"})]})]})]}),e.jsx(_,{question:"Un robot aspirateur qui nettoie une maison inconnue √©volue dans quel type d'environnement ?",options:["D√©terministe et compl√®tement observable","Stochastique et partiellement observable","D√©terministe et partiellement observable","Stochastique et compl√®tement observable"],correctAnswer:1,explanation:"L'environnement est stochastique (objets qui bougent, efficacit√© variable du nettoyage) et partiellement observable (capteurs limit√©s, ne peut pas voir derri√®re les meubles). Le robot doit composer avec l'incertitude sur deux niveaux !",difficulty:"difficile"}),e.jsx(w,{title:"üéÆ Concevoir un Agent pour Frogger",problem:"Vous devez cr√©er un agent RL pour le jeu Frogger (la grenouille qui traverse la route). D√©finissez pr√©cis√©ment : l'espace d'√©tats, l'espace d'actions, la fonction de r√©compense, et le type d'environnement.",solution:`**Espace d'√©tats :**
- Position (x, y) de la grenouille
- Positions et vitesses des voitures sur chaque voie
- Positions des troncs d'arbres flottants
- Temps restant (optionnel)

**Espace d'actions :**
- HAUT, BAS, GAUCHE, DROITE, RESTER_IMMOBILE

**Fonction de r√©compense :**
- +1000 : Atteindre l'autre c√¥t√©
- -1000 : Collision avec voiture ou eau
- -1 : Chaque pas de temps (encourage la vitesse)
- +10 : Avancer vers l'objectif
- -10 : Reculer

**Type d'environnement :**
- D√©terministe (mouvements pr√©visibles)
- Compl√®tement observable (on voit tout l'√©cran)
- S√©quentiel (actions successives importantes)
- Dynamique (l'environnement change m√™me sans action)`,hints:["Pensez √† tous les objets mobiles qui affectent la grenouille","La r√©compense doit encourager √† la fois la vitesse et la s√©curit√©","Observez un vrai jeu Frogger pour comprendre la pr√©visibilit√©"],difficulty:"interm√©diaire",estimatedTime:"20 min"}),e.jsxs(g,{open:s.exploration,onOpenChange:()=>h("exploration"),children:[e.jsx(j,{className:"w-full",children:e.jsx(t,{className:"hover:shadow-lg transition-all duration-300 cursor-pointer",children:e.jsx(a,{children:e.jsxs(i,{className:"flex items-center justify-between",children:[e.jsxs("span",{className:"flex items-center gap-2",children:[e.jsx(x,{className:"h-6 w-6 text-purple-600"}),"Le Dilemme Exploration vs Exploitation"]}),e.jsx(f,{className:`h-5 w-5 transition-transform ${s.exploration?"rotate-180":""}`})]})})})}),e.jsx(b,{children:e.jsx(t,{className:"mt-2 bg-gradient-to-r from-purple-50 to-pink-50",children:e.jsxs(d,{className:"pt-6 space-y-6",children:[e.jsx(l,{title:"üçï Le Dilemme du Restaurant",type:"exemple",children:e.jsxs("div",{className:"space-y-4",children:[e.jsx("p",{children:"Imaginez que vous √™tes dans une nouvelle ville avec plein de restaurants. Vous avez trouv√© une pizzeria correcte, mais devez-vous :"}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-green-100 p-4 rounded-lg border-l-4 border-green-500",children:[e.jsx("h4",{className:"font-semibold text-green-800",children:"üçï EXPLOITER"}),e.jsx("p",{className:"text-sm",children:"Retourner √† la pizzeria connue"}),e.jsx("p",{className:"text-xs text-green-600",children:"S√ªr mais limit√©"})]}),e.jsxs("div",{className:"bg-blue-100 p-4 rounded-lg border-l-4 border-blue-500",children:[e.jsx("h4",{className:"font-semibold text-blue-800",children:"üåü EXPLORER"}),e.jsx("p",{className:"text-sm",children:"Essayer un nouveau restaurant"}),e.jsx("p",{className:"text-xs text-blue-600",children:"Risqu√© mais potentiellement meilleur"})]})]}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border",children:[e.jsx("h4",{className:"font-semibold mb-2",children:"üí° Strat√©gies √©quilibr√©es :"}),e.jsxs("ul",{className:"text-sm space-y-1",children:[e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Œµ-greedy :"})," 90% pizzeria, 10% exploration"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"UCB :"})," Essayer les restaurants peu test√©s"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"D√©croissant :"})," Explorer beaucoup au d√©but, puis se stabiliser"]})]})]})]})}),e.jsx("div",{className:"text-center",children:e.jsxs("svg",{width:"400",height:"200",viewBox:"0 0 400 200",className:"max-w-full h-auto mx-auto",children:[e.jsx("line",{x1:"200",y1:"50",x2:"200",y2:"120",stroke:"#374151",strokeWidth:"4"}),e.jsx("circle",{cx:"200",cy:"120",r:"8",fill:"#374151"}),e.jsx("line",{x1:"120",y1:"80",x2:"280",y2:"80",stroke:"#374151",strokeWidth:"3"}),e.jsx("rect",{x:"80",y:"60",width:"80",height:"40",rx:"8",fill:"#3b82f6",stroke:"#1e40af"}),e.jsx("text",{x:"120",y:"85",textAnchor:"middle",className:"font-bold text-white",fontSize:"12",children:"EXPLORER"}),e.jsx("rect",{x:"240",y:"60",width:"80",height:"40",rx:"8",fill:"#059669",stroke:"#047857"}),e.jsx("text",{x:"280",y:"85",textAnchor:"middle",className:"font-bold text-white",fontSize:"12",children:"EXPLOITER"}),e.jsx("text",{x:"120",y:"130",textAnchor:"middle",className:"text-sm",fill:"#3b82f6",children:"D√©couvrir"}),e.jsx("text",{x:"120",y:"145",textAnchor:"middle",className:"text-sm",fill:"#3b82f6",children:"de nouvelles"}),e.jsx("text",{x:"120",y:"160",textAnchor:"middle",className:"text-sm",fill:"#3b82f6",children:"strat√©gies"}),e.jsx("text",{x:"280",y:"130",textAnchor:"middle",className:"text-sm",fill:"#059669",children:"Utiliser les"}),e.jsx("text",{x:"280",y:"145",textAnchor:"middle",className:"text-sm",fill:"#059669",children:"meilleures"}),e.jsx("text",{x:"280",y:"160",textAnchor:"middle",className:"text-sm",fill:"#059669",children:"connues"})]})})]})})})]})]})},H=()=>{const[s,n]=N.useState({}),h=m=>{n(p=>({...p,[m]:!p[m]}))};return e.jsxs("div",{className:"space-y-8",children:[e.jsxs(l,{title:"üéØ L'Arsenal des Algorithmes RL",type:"concept",children:[e.jsx("p",{className:"mb-4",children:"Comme un chef cuisinier a diff√©rents ustensiles, l'apprentissage par renforcement dispose de plusieurs algorithmes, chacun adapt√© √† des situations particuli√®res !"}),e.jsxs("div",{className:"grid md:grid-cols-3 gap-4",children:[e.jsxs("div",{className:"bg-gradient-to-br from-blue-100 to-blue-200 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-2",children:"üèÅ Bas√©s sur la Valeur"}),e.jsx("p",{className:"text-sm",children:"Q-Learning, SARSA"}),e.jsx("p",{className:"text-xs text-blue-600",children:"Apprennent la valeur des actions"})]}),e.jsxs("div",{className:"bg-gradient-to-br from-green-100 to-green-200 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-green-800 mb-2",children:"üé™ Bas√©s sur la Politique"}),e.jsx("p",{className:"text-sm",children:"REINFORCE, A3C"}),e.jsx("p",{className:"text-xs text-green-600",children:"Apprennent directement la strat√©gie"})]}),e.jsxs("div",{className:"bg-gradient-to-br from-purple-100 to-purple-200 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-purple-800 mb-2",children:"‚öñÔ∏è Acteur-Critique"}),e.jsx("p",{className:"text-sm",children:"A2C, PPO, SAC"}),e.jsx("p",{className:"text-xs text-purple-600",children:"Combinent les deux approches"})]})]})]}),e.jsxs(g,{open:s.qlearning,onOpenChange:()=>h("qlearning"),children:[e.jsx(j,{className:"w-full",children:e.jsx(t,{className:"hover:shadow-lg transition-all duration-300 cursor-pointer",children:e.jsx(a,{children:e.jsxs(i,{className:"flex items-center justify-between",children:[e.jsxs("span",{className:"flex items-center gap-2",children:[e.jsx(A,{className:"h-6 w-6 text-yellow-600"}),"Q-Learning : Le Roi des Algorithmes"]}),e.jsx(f,{className:`h-5 w-5 transition-transform ${s.qlearning?"rotate-180":""}`})]})})})}),e.jsx(b,{children:e.jsx(t,{className:"mt-2 bg-gradient-to-r from-yellow-50 to-orange-50",children:e.jsxs(d,{className:"pt-6 space-y-6",children:[e.jsx(l,{title:"üó∫Ô∏è La Table Q : GPS de l'Agent",type:"analogie",children:e.jsxs("div",{className:"space-y-4",children:[e.jsx("p",{children:'Imaginez Q-Learning comme un GPS intelligent qui apprend les meilleurs chemins en explorant la ville. Chaque intersection (√©tat) a une table qui indique la "valeur" de chaque direction possible !'}),e.jsxs("div",{className:"bg-white p-6 rounded-xl border-2 border-dashed border-yellow-400",children:[e.jsx("h4",{className:"font-semibold mb-4 text-center",children:"üìä Table Q Simplifi√©e - Pac-Man"}),e.jsx("div",{className:"overflow-x-auto",children:e.jsxs("table",{className:"w-full text-sm border-collapse",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"bg-yellow-100",children:[e.jsx("th",{className:"border p-2",children:"√âtat"}),e.jsx("th",{className:"border p-2",children:"‚¨ÜÔ∏è Haut"}),e.jsx("th",{className:"border p-2",children:"‚¨áÔ∏è Bas"}),e.jsx("th",{className:"border p-2",children:"‚¨ÖÔ∏è Gauche"}),e.jsx("th",{className:"border p-2",children:"‚û°Ô∏è Droite"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{className:"border p-2 font-medium",children:"Couloir libre"}),e.jsx("td",{className:"border p-2 text-green-600",children:"+0.8"}),e.jsx("td",{className:"border p-2 text-red-600",children:"-0.2"}),e.jsx("td",{className:"border p-2 text-blue-600",children:"+0.5"}),e.jsx("td",{className:"border p-2 text-green-600",children:"+0.9"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"border p-2 font-medium",children:"Fant√¥me proche"}),e.jsx("td",{className:"border p-2 text-red-600",children:"-0.9"}),e.jsx("td",{className:"border p-2 text-green-600",children:"+0.7"}),e.jsx("td",{className:"border p-2 text-red-600",children:"-0.8"}),e.jsx("td",{className:"border p-2 text-yellow-600",children:"+0.1"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"border p-2 font-medium",children:"Pr√®s pastille"}),e.jsx("td",{className:"border p-2 text-green-600",children:"+1.2"}),e.jsx("td",{className:"border p-2 text-blue-600",children:"+0.3"}),e.jsx("td",{className:"border p-2 text-blue-600",children:"+0.4"}),e.jsx("td",{className:"border p-2 text-yellow-600",children:"+0.2"})]})]})]})}),e.jsx("p",{className:"text-xs text-gray-600 mt-2",children:"üí° Plus la valeur est √©lev√©e, plus l'action est recommand√©e dans cette situation !"})]})]})}),e.jsxs("div",{className:"grid md:grid-cols-2 gap-6",children:[e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-orange-800 mb-3",children:"‚ö° Formule Magique de Q-Learning"}),e.jsxs("div",{className:"bg-white p-4 rounded-lg border border-orange-200",children:[e.jsx("div",{className:"text-center mb-4",children:e.jsx("code",{className:"text-lg font-mono bg-gray-100 p-2 rounded",children:"Q(s,a) = Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]"})}),e.jsxs("div",{className:"space-y-2 text-sm",children:[e.jsxs("p",{children:[e.jsx("strong",{children:"Œ±"})," : Taux d'apprentissage (vitesse d'adaptation)"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"Œ≥"})," : Facteur d'escompte (importance du futur)"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"r"})," : R√©compense imm√©diate"]}),e.jsxs("p",{children:[e.jsx("strong",{children:"max Q(s',a')"})," : Meilleure action future possible"]})]})]})]}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-orange-800 mb-3",children:"üéØ Algorithme Pas √† Pas"}),e.jsxs("div",{className:"space-y-2 text-sm",children:[e.jsxs("div",{className:"bg-blue-50 p-3 rounded border-l-4 border-blue-400",children:[e.jsx("strong",{children:"1."})," Observer l'√©tat actuel"]}),e.jsxs("div",{className:"bg-green-50 p-3 rounded border-l-4 border-green-400",children:[e.jsx("strong",{children:"2."})," Choisir action (Œµ-greedy)"]}),e.jsxs("div",{className:"bg-yellow-50 p-3 rounded border-l-4 border-yellow-400",children:[e.jsx("strong",{children:"3."})," Ex√©cuter l'action"]}),e.jsxs("div",{className:"bg-purple-50 p-3 rounded border-l-4 border-purple-400",children:[e.jsx("strong",{children:"4."})," Recevoir r√©compense"]}),e.jsxs("div",{className:"bg-red-50 p-3 rounded border-l-4 border-red-400",children:[e.jsx("strong",{children:"5."})," Mettre √† jour Q(s,a)"]}),e.jsxs("div",{className:"bg-gray-50 p-3 rounded border-l-4 border-gray-400",children:[e.jsx("strong",{children:"6."})," R√©p√©ter jusqu'√† convergence"]})]})]})]})]})})})]}),e.jsx(P,{title:"‚öîÔ∏è Bataille des Algorithmes : Qui Gagne Quoi ?",levels:[{title:"D√©butant : Les Bases",difficulty:"basic",content:e.jsx("div",{className:"space-y-4",children:e.jsxs("div",{className:"grid md:grid-cols-2 gap-4",children:[e.jsxs("div",{className:"bg-blue-50 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-blue-800 mb-2",children:"üèÜ Q-Learning"}),e.jsxs("p",{className:"text-sm mb-2",children:[e.jsx("strong",{children:"Points forts :"})," Simple, robuste, bien compris"]}),e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"Id√©al pour :"})," Environnements discrets, apprentissage"]})]}),e.jsxs("div",{className:"bg-green-50 p-4 rounded-lg",children:[e.jsx("h4",{className:"font-semibold text-green-800 mb-2",children:"üé™ REINFORCE"}),e.jsxs("p",{className:"text-sm mb-2",children:[e.jsx("strong",{children:"Points forts :"})," Actions continues, simple"]}),e.jsxs("p",{className:"text-sm",children:[e.jsx("strong",{children:"Id√©al pour :"})," Probl√®mes de contr√¥le, robotique"]})]})]})})},{title:"Interm√©diaire : Les Nuances",difficulty:"intermediate",content:e.jsx("div",{className:"space-y-4",children:e.jsxs("div",{className:"bg-white p-4 rounded-lg border",children:[e.jsx("h4",{className:"font-semibold mb-3",children:"üìä Tableau Comparatif D√©taill√©"}),e.jsx("div",{className:"overflow-x-auto",children:e.jsxs("table",{className:"w-full text-sm",children:[e.jsx("thead",{children:e.jsxs("tr",{className:"bg-gray-100",children:[e.jsx("th",{className:"p-2 text-left",children:"Algorithme"}),e.jsx("th",{className:"p-2",children:"Convergence"}),e.jsx("th",{className:"p-2",children:"Stabilit√©"}),e.jsx("th",{className:"p-2",children:"Efficacit√©"}),e.jsx("th",{className:"p-2",children:"Complexit√©"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{className:"p-2 font-medium",children:"Q-Learning"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"p-2 font-medium",children:"SARSA"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"p-2 font-medium",children:"DQN"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê"})]}),e.jsxs("tr",{children:[e.jsx("td",{className:"p-2 font-medium",children:"PPO"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê"}),e.jsx("td",{className:"p-2",children:"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"})]})]})]})})]})})},{title:"Avanc√© : Les Secrets",difficulty:"advanced",content:e.jsx("div",{className:"space-y-4",children:e.jsx(l,{title:"üî¨ Analyse Approfondie des Trade-offs",type:"zoom",children:e.jsxs("div",{className:"space-y-4",children:[e.jsxs("div",{className:"bg-red-50 p-4 rounded-lg border-l-4 border-red-400",children:[e.jsx("h4",{className:"font-semibold text-red-800 mb-2",children:"‚ö†Ô∏è Pi√®ges Courants"}),e.jsxs("ul",{className:"text-sm space-y-1",children:[e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Q-Learning :"})," Surestimation des valeurs Q"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Policy Gradient :"})," Variance √©lev√©e des gradients"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"DQN :"})," Instabilit√© due aux r√©seaux de neurones"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"A3C :"})," Coordination difficile entre acteurs"]})]})]}),e.jsxs("div",{className:"bg-green-50 p-4 rounded-lg border-l-4 border-green-400",children:[e.jsx("h4",{className:"font-semibold text-green-800 mb-2",children:"üí° Solutions Modernes"}),e.jsxs("ul",{className:"text-sm space-y-1",children:[e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Double DQN :"})," R√©duit la surestimation"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Dueling Networks :"})," S√©pare valeur d'√©tat et avantage"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"Rainbow :"})," Combine plusieurs am√©liorations"]}),e.jsxs("li",{children:["‚Ä¢ ",e.jsx("strong",{children:"SAC :"})," Stabilit√© via l'entropie maximale"]})]})]})]})})})}]}),e.jsx(_,{question:"Pourquoi SARSA est-il consid√©r√© comme plus 'prudent' que Q-Learning dans un environnement dangereux ?",options:["SARSA converge plus rapidement","SARSA utilise la politique actuelle pour estimer les valeurs futures","SARSA a une complexit√© computationnelle plus faible","SARSA ne n√©cessite pas de facteur d'escompte"],correctAnswer:1,explanation:"SARSA est 'on-policy' : il √©value les actions selon la politique qu'il suit r√©ellement (incluant l'exploration). Q-Learning est 'off-policy' et suppose toujours qu'on prendra la meilleure action, ce qui peut √™tre dangereux si on explore encore. Dans un environnement avec des 'falaises', SARSA apprend √† √©viter les bords m√™me pendant l'exploration !",difficulty:"difficile"}),e.jsx(w,{title:"üéÆ Impl√©mentation Q-Learning pour Grid World",problem:"Impl√©mentez un agent Q-Learning simple pour naviguer dans une grille 4x4. L'agent commence en (0,0) et doit atteindre (3,3). Les cases (1,1) et (2,2) sont des obstacles. Impl√©mentez la table Q, la politique Œµ-greedy, et la mise √† jour des valeurs.",solution:`import numpy as np
import random

class QLearningAgent:
    def __init__(self, state_size=16, action_size=4, lr=0.1, gamma=0.95, epsilon=0.1):
        self.q_table = np.zeros((state_size, action_size))
        self.lr = lr  # Taux d'apprentissage
        self.gamma = gamma  # Facteur d'escompte
        self.epsilon = epsilon  # Exploration
        
        # Actions: 0=haut, 1=bas, 2=gauche, 3=droite
        self.actions = [(0,-1), (0,1), (-1,0), (1,0)]
    
    def state_to_index(self, row, col):
        """Convertit (row, col) en index 1D"""
        return row * 4 + col
    
    def index_to_state(self, index):
        """Convertit index 1D en (row, col)"""
        return index // 4, index % 4
    
    def is_valid_state(self, row, col):
        """V√©rifie si l'√©tat est valide"""
        if row < 0 or row >= 4 or col < 0 or col >= 4:
            return False
        if (row, col) in [(1,1), (2,2)]:  # Obstacles
            return False
        return True
    
    def choose_action(self, state):
        """Politique Œµ-greedy"""
        if random.random() < self.epsilon:
            return random.randint(0, 3)  # Exploration
        else:
            return np.argmax(self.q_table[state])  # Exploitation
    
    def update_q_table(self, state, action, reward, next_state):
        """Mise √† jour Q-Learning"""
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        
        # Formule Q-Learning
        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state, action] = new_q
    
    def get_next_state(self, current_state, action):
        """Calcule le prochain √©tat"""
        row, col = self.index_to_state(current_state)
        d_row, d_col = self.actions[action]
        new_row, new_col = row + d_row, col + d_col
        
        if self.is_valid_state(new_row, new_col):
            return self.state_to_index(new_row, new_col)
        else:
            return current_state  # Reste en place si mouvement invalide
    
    def get_reward(self, state):
        """Fonction de r√©compense"""
        row, col = self.index_to_state(state)
        if (row, col) == (3, 3):  # Objectif atteint
            return 10
        elif (row, col) in [(1,1), (2,2)]:  # Obstacles
            return -5
        else:
            return -0.1  # Co√ªt de d√©placement

# Exemple d'utilisation
agent = QLearningAgent()
episodes = 1000

for episode in range(episodes):
    state = 0  # Position de d√©part (0,0)
    
    while state != 15:  # Jusqu'√† atteindre (3,3)
        action = agent.choose_action(state)
        next_state = agent.get_next_state(state, action)
        reward = agent.get_reward(next_state)
        
        agent.update_q_table(state, action, reward, next_state)
        state = next_state
        
        # √âviter les boucles infinies
        if episode > 50 and random.random() < 0.01:
            break

print("Entra√Ænement termin√© !")
print("Table Q finale (premiers √©tats):")
print(agent.q_table[:8])`,hints:["Commencez par d√©finir la repr√©sentation des √©tats (grille 4x4 = 16 √©tats)","Pensez √† la fonction de r√©compense : +10 pour l'objectif, -5 pour obstacles, -0.1 par pas","Impl√©mentez d'abord la politique Œµ-greedy simple","N'oubliez pas de g√©rer les mouvements invalides (rester en place)"],difficulty:"avanc√©",estimatedTime:"45 min"})]})},V=[{icon:e.jsx(y,{className:"h-6 w-6 text-purple-600"}),title:"Jeux et Divertissement",description:"IA capable de jouer et gagner √† des jeux complexes en apprenant par exp√©rience.",examples:["AlphaGo bat le champion mondial de Go","OpenAI Five domine √† Dota 2","AlphaStar ma√Ætrise StarCraft II","IA poker battant les pros"],industry:"Gaming",difficulty:"Interm√©diaire",impact:"√âlev√©"},{icon:e.jsx(G,{className:"h-6 w-6 text-blue-600"}),title:"V√©hicules Autonomes",description:"Syst√®mes de conduite qui apprennent √† naviguer en toute s√©curit√© dans le trafic r√©el.",examples:["Tesla Autopilot et Full Self-Driving","Waymo taxis autonomes","Optimisation des trajectoires","Gestion des intersections complexes"],industry:"Transport",difficulty:"Avanc√©",impact:"√âlev√©"},{icon:e.jsx(T,{className:"h-6 w-6 text-green-600"}),title:"Trading Algorithmique",description:"Agents qui apprennent √† trader sur les march√©s financiers en maximisant les profits.",examples:["Trading haute fr√©quence adaptatif","Gestion de portefeuille dynamique","D√©tection de patterns de march√©","Optimisation risk/reward"],industry:"Finance",difficulty:"Avanc√©",impact:"√âlev√©"},{icon:e.jsx(D,{className:"h-6 w-6 text-orange-600"}),title:"Robotique Industrielle",description:"Robots qui apprennent des t√¢ches complexes par renforcement et adaptation.",examples:["Assemblage adaptatif de pi√®ces","Maintenance pr√©dictive","Optimisation de trajectoires","Collaboration homme-robot"],industry:"Industrie",difficulty:"Avanc√©",impact:"Moyen"},{icon:e.jsx(I,{className:"h-6 w-6 text-red-600"}),title:"Sant√© Personnalis√©e",description:"Syst√®mes qui apprennent √† optimiser les traitements pour chaque patient.",examples:["Dosage optimal de m√©dicaments","Plans de r√©√©ducation adaptatifs","Proth√®ses intelligentes","Diagnostic assist√© par IA"],industry:"Sant√©",difficulty:"Avanc√©",impact:"√âlev√©"},{icon:e.jsx(v,{className:"h-6 w-6 text-yellow-600"}),title:"Gestion √ânerg√©tique",description:"Optimisation intelligente de la consommation et distribution d'√©nergie.",examples:["Smart grids adaptatifs","Optimisation de data centers","Gestion de batteries","Pr√©diction de demande √©nerg√©tique"],industry:"√ânergie",difficulty:"Interm√©diaire",impact:"√âlev√©"},{icon:e.jsx(x,{className:"h-6 w-6 text-indigo-600"}),title:"Publicit√© Cibl√©e",description:"Syst√®mes qui apprennent √† optimiser le placement publicitaire en temps r√©el.",examples:["Ench√®res automatiques RTB","Personnalisation de contenu","A/B testing intelligent","Attribution multi-touch"],industry:"Marketing",difficulty:"Interm√©diaire",impact:"Moyen"},{icon:e.jsx(Q,{className:"h-6 w-6 text-gray-600"}),title:"Cybers√©curit√©",description:"Agents qui apprennent √† d√©tecter et contrer les menaces en temps r√©el.",examples:["D√©tection d'intrusions adaptative","R√©ponse automatique aux incidents","Analyse comportementale","Honeypots intelligents"],industry:"S√©curit√©",difficulty:"Avanc√©",impact:"√âlev√©"},{icon:e.jsx(O,{className:"h-6 w-6 text-teal-600"}),title:"Optimisation de Ressources",description:"Gestion intelligente des ressources dans des syst√®mes complexes.",examples:["Routage r√©seau adaptatif","Allocation de serveurs cloud","Planning logistique dynamique","Gestion de stocks intelligente"],industry:"Logistique",difficulty:"Interm√©diaire",impact:"Moyen"}],J=()=>e.jsx(k,{title:"Applications Concr√®tes de l'Apprentissage par Renforcement",applications:V,description:"L'apprentissage par renforcement r√©volutionne de nombreux secteurs en permettant aux machines d'apprendre par l'exp√©rience, tout comme les humains. De la conduite autonome aux jeux strat√©giques, d√©couvrez comment cette technologie transforme notre monde."}),X=[{title:"üéÆ Agent de Jeu : Tic-Tac-Toe Intelligent",description:"Cr√©ez un agent qui apprend √† jouer au tic-tac-toe en utilisant Q-Learning.",problem:"Impl√©mentez un agent Q-Learning qui apprend √† jouer au tic-tac-toe contre un adversaire. L'agent doit √™tre capable d'apprendre les strat√©gies optimales en jouant de nombreuses parties. Impl√©mentez √©galement une interface graphique pour jouer contre votre agent entra√Æn√©.",solution:`# Solution compl√®te disponible
import numpy as np
import random
import matplotlib.pyplot as plt
from collections import defaultdict

class TicTacToeEnvironment:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1
        return self.get_state()
    
    def get_state(self):
        return tuple(self.board.flatten())
    
    def get_valid_actions(self):
        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]
    
    def step(self, action):
        if action not in self.get_valid_actions():
            return self.get_state(), -10, True  # Invalid move penalty
        
        row, col = action
        self.board[row, col] = self.current_player
        
        # Check for win
        if self.check_winner(self.current_player):
            reward = 10 if self.current_player == 1 else -10
            return self.get_state(), reward, True
        
        # Check for draw
        if len(self.get_valid_actions()) == 0:
            return self.get_state(), 0, True
        
        # Switch player
        self.current_player = -self.current_player
        return self.get_state(), 0, False
    
    def check_winner(self, player):
        # Check rows, columns, and diagonals
        for i in range(3):
            if all(self.board[i, :] == player) or all(self.board[:, i] == player):
                return True
        if all(np.diag(self.board) == player) or all(np.diag(np.fliplr(self.board)) == player):
            return True
        return False

class QLearningAgent:
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
    
    def get_action(self, state, valid_actions, training=True):
        if training and random.random() < self.epsilon:
            return random.choice(valid_actions)
        
        if state not in self.q_table:
            return random.choice(valid_actions)
        
        best_action = max(valid_actions, key=lambda a: self.q_table[state][a])
        return best_action
    
    def update_q_value(self, state, action, reward, next_state, valid_next_actions):
        current_q = self.q_table[state][action]
        if valid_next_actions:
            max_next_q = max(self.q_table[next_state][a] for a in valid_next_actions)
        else:
            max_next_q = 0
        
        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state][action] = new_q

# Training loop
def train_agent(episodes=10000):
    env = TicTacToeEnvironment()
    agent = QLearningAgent()
    wins = 0
    
    for episode in range(episodes):
        state = env.reset()
        done = False
        
        while not done:
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
            
            if env.current_player == 1:  # Agent's turn
                action = agent.get_action(state, valid_actions)
                next_state, reward, done = env.step(action)
                agent.update_q_value(state, action, reward, next_state, env.get_valid_actions())
            else:  # Random opponent
                action = random.choice(valid_actions)
                next_state, reward, done = env.step(action)
            
            state = next_state
        
        # Track wins
        if env.check_winner(1):
            wins += 1
        
        if episode % 1000 == 0:
            win_rate = wins / (episode + 1)
            print(f"Episode {episode}, Win rate: {win_rate:.3f}")
    
    return agent

# Entra√Æner l'agent
trained_agent = train_agent()`,hints:["Commencez par impl√©menter l'environnement de jeu avec les r√®gles de base","Utilisez une repr√©sentation d'√©tat simple (tuple des positions)","Impl√©mentez Q-Learning avec exploration epsilon-greedy","Entra√Ænez contre un adversaire al√©atoire puis progressivement plus intelligent","Ajustez les hyperparam√®tres (alpha, gamma, epsilon) pour optimiser l'apprentissage"],difficulty:"interm√©diaire",estimatedTime:"120 min",skills:["Q-Learning","Gestion d'√©tat","Interface utilisateur","√âvaluation d'agents"],tools:["Python","NumPy","Matplotlib","Tkinter"],category:"Jeux & IA"},{title:"üöó Contr√¥leur de V√©hicule : Parking Autonome",description:"D√©veloppez un agent qui apprend √† se garer automatiquement dans diff√©rents environnements.",problem:"Cr√©ez un simulateur de parking et un agent qui apprend √† stationner un v√©hicule en √©vitant les obstacles. L'agent doit apprendre √† contr√¥ler la direction, l'acc√©l√©ration et le freinage pour se garer efficacement dans des espaces de tailles variables. Impl√©mentez plusieurs sc√©narios de parking (parall√®le, perpendiculaire, cr√©neaux serr√©s).",solution:`# Solution avec Deep Q-Network (DQN)
import numpy as np
import pygame
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class ParkingEnvironment:
    def __init__(self, width=800, height=600):
        self.width = width
        self.height = height
        self.car_width = 40
        self.car_height = 20
        self.reset()
    
    def reset(self):
        # Position al√©atoire de d√©part
        self.car_x = random.randint(100, self.width - 200)
        self.car_y = random.randint(100, self.height - 200)
        self.car_angle = random.uniform(0, 2 * np.pi)
        self.car_velocity = 0
        
        # Position cible (parking)
        self.target_x = random.randint(200, self.width - 200)
        self.target_y = random.randint(200, self.height - 200)
        
        # Obstacles
        self.obstacles = self.generate_obstacles()
        
        return self.get_state()
    
    def get_state(self):
        # Distance et angle vers la cible
        dx = self.target_x - self.car_x
        dy = self.target_y - self.car_y
        distance = np.sqrt(dx**2 + dy**2)
        angle_to_target = np.arctan2(dy, dx) - self.car_angle
        
        # Distances aux obstacles (sensors)
        sensor_distances = self.get_sensor_distances()
        
        # √âtat normalis√©
        state = [
            self.car_x / self.width,
            self.car_y / self.height,
            self.car_angle / (2 * np.pi),
            self.car_velocity / 10.0,
            distance / np.sqrt(self.width**2 + self.height**2),
            np.sin(angle_to_target),
            np.cos(angle_to_target),
        ] + sensor_distances
        
        return np.array(state, dtype=np.float32)
    
    def calculate_reward(self):
        # Distance reward
        distance_to_target = np.sqrt((self.target_x - self.car_x)**2 + 
                                   (self.target_y - self.car_y)**2)
        distance_reward = -distance_to_target / 100.0
        
        # Collision penalty
        if self.check_collision():
            return -100
        
        # Parking success
        if distance_to_target < 30 and abs(self.car_velocity) < 0.1:
            return 100
        
        # Speed penalty (encourage slow parking)
        speed_penalty = -abs(self.car_velocity) * 0.1
        
        return distance_reward + speed_penalty

    def render(self):
        current_price = self.data.iloc[self.current_step]['Close']
        total_value = self.balance + self.shares * current_price
        profit = (total_value - self.initial_balance) / self.initial_balance * 100
        
        print(f"Step: {self.current_step}")
        print(f"Price: {current_price:.2f}")
        print(f"Balance: {self.balance:.2f}")
        print(f"Shares: {self.shares}")
        print(f"Total Value: {total_value:.2f}")
        print(f"Profit: {profit:.2f}%")

class DQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=256):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        return self.fc4(x)

# Training code would continue...`,hints:["Mod√©lisez la physique simple du v√©hicule (position, vitesse, angle)","Utilisez des capteurs de distance pour d√©tecter les obstacles","Concevez une fonction de r√©compense qui encourage le parking r√©ussi","Impl√©mentez DQN pour g√©rer l'espace d'√©tat continu","Visualisez l'apprentissage avec pygame ou matplotlib"],difficulty:"avanc√©",estimatedTime:"240 min",skills:["Deep Q-Learning","Simulation physique","Computer Vision","Robotique"],tools:["Python","PyTorch","Pygame","NumPy"],category:"Robotique & Contr√¥le"},{title:"üìà Trading Bot : Optimisation de Portefeuille",description:"Cr√©ez un agent qui apprend √† trader sur les march√©s financiers en maximisant les profits.",problem:"D√©veloppez un agent de trading qui apprend √† optimiser un portefeuille d'actions en utilisant l'apprentissage par renforcement. L'agent doit prendre des d√©cisions d'achat, vente et d√©tention bas√©es sur des indicateurs techniques et fondamentaux. Impl√©mentez une simulation de march√© r√©aliste avec des co√ªts de transaction.",solution:`# Solution Trading Bot avec PPO
import numpy as np
import pandas as pd
import yfinance as yf
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import gymnasium as gym
from gymnasium import spaces

class TradingEnvironment(gym.Env):
    def __init__(self, data, initial_balance=10000, transaction_cost=0.001):
        super().__init__()
        self.data = data.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        
        # Action space: [sell_all, hold, buy_25%, buy_50%, buy_75%, buy_all]
        self.action_space = spaces.Discrete(6)
        
        # Observation space: [balance, shares, price, technical_indicators...]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None):
        super().reset(seed=seed)
        self.current_step = 40  # Start after indicator calculation period
        self.balance = self.initial_balance
        self.shares = 0
        self.total_asset_value = self.initial_balance
        self.trades = []
        
        return self._get_observation(), {}
    
    def _execute_action(self, action, current_price):
        prev_total_value = self.total_asset_value
        
        if action == 0:  # Sell all
            if self.shares > 0:
                self.balance += self.shares * current_price * (1 - self.transaction_cost)
                self.shares = 0
                self.trades.append(('sell', self.current_step, current_price, self.shares))
        
        elif action == 1:  # Hold
            pass
        
        else:  # Buy actions (25%, 50%, 75%, 100%)
            buy_percentage = [0, 0, 0.25, 0.5, 0.75, 1.0][action]
            max_shares = self.balance / (current_price * (1 + self.transaction_cost))
            shares_to_buy = int(max_shares * buy_percentage)
            
            if shares_to_buy > 0:
                cost = shares_to_buy * current_price * (1 + self.transaction_cost)
                if cost <= self.balance:
                    self.balance -= cost
                    self.shares += shares_to_buy
                    self.trades.append(('buy', self.current_step, current_price, shares_to_buy))
        
        # Calculate reward based on portfolio value change
        current_total_value = self.balance + self.shares * current_price
        reward = (current_total_value - prev_total_value) / self.initial_balance
        
        # Add penalty for excessive trading
        if len(self.trades) > 0 and self.trades[-1][1] == self.current_step:
            reward -= 0.001  # Small penalty for trading
        
        return reward

# Entra√Æner l'agent
def train_trading_agent():
    # Code d'entra√Ænement complet disponible dans la documentation
    print("Training complete!")`,hints:["Utilisez des donn√©es financi√®res r√©elles (yfinance, Alpha Vantage)","Impl√©mentez des indicateurs techniques (RSI, MACD, Bollinger Bands)","Mod√©lisez les co√ªts de transaction et le slippage","Utilisez PPO ou A3C pour g√©rer l'action continue","Backtestez sur des donn√©es historiques et √©valuez avec Sharpe ratio"],difficulty:"avanc√©",estimatedTime:"300 min",skills:["Algorithmic Trading","Financial Modeling","PPO","Backtesting"],tools:["Python","yfinance","Stable-Baselines3","Pandas","TA-Lib"],category:"Finance & √âconomie"}],$=()=>e.jsx(R,{title:"Projets Pratiques en Apprentissage par Renforcement",projects:X,description:"Mettez en pratique vos connaissances avec ces projets progressifs qui couvrent les applications principales de l'apprentissage par renforcement. Chaque projet est con√ßu pour d√©velopper des comp√©tences sp√©cifiques tout en cr√©ant des solutions concr√®tes."}),Y=[{title:"Reinforcement Learning: An Introduction",description:"Le livre de r√©f√©rence par Sutton et Barto, couvrant tous les fondamentaux th√©oriques et pratiques de l'apprentissage par renforcement.",type:"book",difficulty:"Interm√©diaire",language:"Anglais",url:"http://incompleteideas.net/book/the-book-2nd.html",rating:5,free:!0},{title:"Deep Reinforcement Learning Hands-On",description:"Guide pratique pour impl√©menter des algorithmes de RL moderne avec PyTorch, incluant DQN, A3C, et PPO.",type:"book",difficulty:"Avanc√©",language:"Anglais",rating:4,free:!1},{title:"CS285: Deep Reinforcement Learning",description:"Cours complet de Berkeley couvrant la th√©orie et la pratique du deep RL, avec des devoirs et projets.",type:"video",difficulty:"Avanc√©",language:"Anglais",url:"http://rail.eecs.berkeley.edu/deeprlcourse/",rating:5,free:!0},{title:"OpenAI Spinning Up",description:"Ressource √©ducative d'OpenAI avec des impl√©mentations d'algorithmes RL et des explications d√©taill√©es.",type:"website",difficulty:"Interm√©diaire",language:"Anglais",url:"https://spinningup.openai.com/",rating:5,free:!0},{title:"Stable Baselines3",description:"Biblioth√®que Python fiable d'algorithmes RL avec documentation excellente et exemples pratiques.",type:"code",difficulty:"D√©butant",language:"Python",url:"https://stable-baselines3.readthedocs.io/",rating:5,free:!0},{title:"Cours d'Andrew Ng - RL",description:"Introduction accessible √† l'apprentissage par renforcement dans le cadre du cours de Machine Learning de Stanford.",type:"video",difficulty:"D√©butant",language:"Anglais",url:"https://www.coursera.org/learn/machine-learning",rating:4,free:!1},{title:"Deep RL Bootcamp",description:"S√©rie de conf√©rences intensives couvrant les derni√®res avanc√©es en deep reinforcement learning.",type:"video",difficulty:"Avanc√©",language:"Anglais",url:"https://sites.google.com/view/deep-rl-bootcamp/",rating:4,free:!0},{title:"Gymnasium (OpenAI Gym)",description:"Environnements standardis√©s pour tester et comparer des algorithmes d'apprentissage par renforcement.",type:"code",difficulty:"D√©butant",language:"Python",url:"https://gymnasium.farama.org/",rating:5,free:!0},{title:"Ray RLlib",description:"Framework distribu√© pour l'apprentissage par renforcement √† grande √©chelle avec support multi-GPU.",type:"code",difficulty:"Avanc√©",language:"Python",url:"https://docs.ray.io/en/latest/rllib/index.html",rating:4,free:!0},{title:"r/MachineLearning",description:"Communaut√© Reddit active discutant des derni√®res recherches et applications en ML et RL.",type:"community",difficulty:"Interm√©diaire",language:"Anglais",url:"https://reddit.com/r/MachineLearning",rating:4,free:!0},{title:"Deep RL Papers",description:"Collection curat√©e des papers les plus importants en deep reinforcement learning avec impl√©mentations.",type:"website",difficulty:"Avanc√©",language:"Anglais",url:"https://github.com/dalmia/Deep-Reinforcement-Learning-Papers",rating:4,free:!0},{title:"Reinforcement Learning France",description:"Communaut√© fran√ßaise de passionn√©s et professionnels du reinforcement learning avec meetups r√©guliers.",type:"community",difficulty:"Interm√©diaire",language:"Fran√ßais",rating:3,free:!0}],Z=["Commencez par comprendre la th√©orie avant de vous lancer dans les impl√©mentations complexes","Pratiquez avec des environnements simples (CartPole, FrozenLake) avant d'attaquer des probl√®mes complexes","Utilisez TensorBoard pour visualiser l'apprentissage et d√©boguer vos agents","Rejoignez des communaut√©s en ligne pour rester √† jour sur les derni√®res avanc√©es","Exp√©rimentez avec diff√©rents hyperparam√®tres - ils sont cruciaux en RL"],K=["Le RL peut √™tre tr√®s instable - attendez-vous √† des r√©sultats variables","L'entra√Ænement peut prendre beaucoup de temps - pr√©parez-vous √† des heures d'attente","Beaucoup de papers ne sont pas reproductibles - v√©rifiez les impl√©mentations","Les environnements r√©els sont beaucoup plus complexes que les simulations","Ne n√©gligez pas l'importance du preprocessing et de la conception des r√©compenses"],ee=["Toujours √©tablir une baseline simple avant d'essayer des algorithmes complexes","Utilisez des graines al√©atoires fixes pour la reproductibilit√©","Loggez tout : r√©compenses, loss, exploration rate, etc.","Testez vos agents sur plusieurs environnements pour √©viter l'overfitting","Documentez vos exp√©riences et gardez trace de ce qui fonctionne"],se=()=>e.jsx(E,{title:"Ressources pour Approfondir l'Apprentissage par Renforcement",resources:Y,tips:Z,warnings:K,bestPractices:ee}),te=()=>{const[s,n]=N.useState("introduction");return e.jsxs("div",{className:"max-w-6xl mx-auto p-6 space-y-8",children:[e.jsx(t,{className:"bg-gradient-to-r from-purple-600 to-indigo-600 text-white",children:e.jsxs(a,{className:"text-center",children:[e.jsxs(i,{className:"text-4xl font-bold mb-4 flex items-center justify-center gap-3",children:[e.jsx(u,{className:"h-12 w-12"}),"Apprentissage par Renforcement"]}),e.jsx("p",{className:"text-xl text-purple-100 max-w-3xl mx-auto",children:"D√©couvrez comment les machines apprennent √† prendre des d√©cisions optimales dans des environnements complexes, de la m√™me fa√ßon qu'un enfant apprend √† marcher !"}),e.jsxs("div",{className:"flex justify-center gap-3 mt-6 flex-wrap",children:[e.jsxs(r,{className:"bg-white text-purple-600 text-sm px-4 py-2",children:[e.jsx(x,{className:"h-4 w-4 mr-2"}),"Prise de D√©cision"]}),e.jsxs(r,{className:"bg-white text-purple-600 text-sm px-4 py-2",children:[e.jsx(v,{className:"h-4 w-4 mr-2"}),"Apprentissage Adaptatif"]}),e.jsxs(r,{className:"bg-white text-purple-600 text-sm px-4 py-2",children:[e.jsx(y,{className:"h-4 w-4 mr-2"}),"IA de Jeux"]})]})]})}),e.jsxs(C,{value:s,onValueChange:n,className:"w-full",children:[e.jsxs(S,{className:"grid w-full grid-cols-6 lg:grid-cols-6 h-auto p-1",children:[e.jsxs(o,{value:"introduction",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(u,{className:"h-5 w-5 mb-1"}),"Introduction"]}),e.jsxs(o,{value:"concepts",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(x,{className:"h-5 w-5 mb-1"}),"Concepts"]}),e.jsxs(o,{value:"algorithms",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(v,{className:"h-5 w-5 mb-1"}),"Algorithmes"]}),e.jsxs(o,{value:"applications",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(y,{className:"h-5 w-5 mb-1"}),"Applications"]}),e.jsxs(o,{value:"projects",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(B,{className:"h-5 w-5 mb-1"}),"Projets"]}),e.jsxs(o,{value:"resources",className:"flex flex-col items-center p-3 text-xs lg:text-sm",children:[e.jsx(A,{className:"h-5 w-5 mb-1"}),"Ressources"]})]}),e.jsx(c,{value:"introduction",className:"mt-8",children:e.jsx(U,{})}),e.jsx(c,{value:"concepts",className:"mt-8",children:e.jsx(W,{})}),e.jsx(c,{value:"algorithms",className:"mt-8",children:e.jsx(H,{})}),e.jsx(c,{value:"applications",className:"mt-8",children:e.jsx(J,{})}),e.jsx(c,{value:"projects",className:"mt-8",children:e.jsx($,{})}),e.jsx(c,{value:"resources",className:"mt-8",children:e.jsx(se,{})})]})]})},De=()=>{const s=[{name:"Machine Learning",href:"/machine-learning"},{name:"Apprentissage par renforcement",href:"/machine-learning/reinforcement"}];return e.jsx(L,{children:e.jsxs("div",{className:"min-h-screen",children:[e.jsx(F,{variant:"course",title:"Apprentissage par Renforcement",description:"D√©couvrez l'apprentissage par renforcement : agents intelligents, r√©compenses et strat√©gies optimales",courseInfo:{level:"Avanc√©",duration:"5-7 heures",modules:7}}),e.jsxs("div",{className:"container mx-auto px-4 py-8",children:[e.jsx("div",{className:"mb-6",children:e.jsx(M,{items:s})}),e.jsx("div",{className:"max-w-6xl mx-auto",children:e.jsx(te,{})})]})]})})};export{De as default};
